{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOlKRvREyFsuJ9trrs14c5Y"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision\nimport torchvision.transforms\n\n!pip install kornia\nfrom kornia import augmentation as K\nfrom kornia.augmentation import AugmentationSequential\nfrom torch.utils.data import random_split\n\nimport numpy as np\n\nimport os\nimport time\nfrom pathlib import Path\nimport pickle\n\nfrom torch.utils.data import Subset\nfrom sklearn.model_selection import train_test_split\n\nDEBUG = False\nMODEL_NAME = \"baseline_500\"\nTRAIN_SIZE = 5000\nVALIDATION_SIZE = 5000\nenv = 'kaggle' # 'kaggle' or 'colab'\n\nif env == 'colab':\n    from google.colab import drive\n    drive.mount('/content/drive')\n    base_dir = Path('/content/drive/MyDrive/dl_pj')    \nelif env == 'kaggle':\n    base_dir = Path('/kaggle/working/')\n\ncheckpoint_dir = base_dir / 'checkpoints'\ncheckpoint_dir.mkdir(parents=True, exist_ok=True)\ntraining_stats_dir = base_dir / 'stats'\ntraining_stats_dir.mkdir(parents=True, exist_ok=True)","metadata":{"id":"zNf1AQxVkZdi","executionInfo":{"status":"ok","timestamp":1766743875913,"user_tz":-120,"elapsed":5636,"user":{"displayName":"Gilad Navok","userId":"06092819627906668279"}},"outputId":"7fd9b195-abbc-4da1-f986-982016920821","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:03:59.027770Z","iopub.execute_input":"2025-12-26T19:03:59.028520Z","iopub.status.idle":"2025-12-26T19:04:02.205689Z","shell.execute_reply.started":"2025-12-26T19:03:59.028477Z","shell.execute_reply":"2025-12-26T19:04:02.204941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BasicBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes,kernel_size=1,stride=stride,bias=False),\n                nn.BatchNorm2d(planes)\n            )\n\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layer = []\n        for s in strides:\n            layer.append(block(self.in_planes, planes, s))\n            self.in_planes = planes\n        return nn.Sequential(*layer)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2, 2, 2, 2])\n","metadata":{"id":"ElgGUpWbrrci","executionInfo":{"status":"ok","timestamp":1766743054230,"user_tz":-120,"elapsed":10,"user":{"displayName":"Gilad Navok","userId":"06092819627906668279"}},"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:04:02.207258Z","iopub.execute_input":"2025-12-26T19:04:02.207573Z","iopub.status.idle":"2025-12-26T19:04:02.219457Z","shell.execute_reply.started":"2025-12-26T19:04:02.207544Z","shell.execute_reply":"2025-12-26T19:04:02.218596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model_summary(model):\n    num_params = sum(p.numel() for p in model.parameters())\n    total_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n    size_mb = total_bytes / (1024 ** 2)\n    return num_params, size_mb\n\ntotal_params, model_size_mb = get_model_summary(ResNet18())\nprint(f\"Total Parameters: {total_params:,}\")\nprint(f\"Model Size: {model_size_mb:.2f} MB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:04:02.220326Z","iopub.execute_input":"2025-12-26T19:04:02.220558Z","iopub.status.idle":"2025-12-26T19:04:02.319410Z","shell.execute_reply.started":"2025-12-26T19:04:02.220534Z","shell.execute_reply":"2025-12-26T19:04:02.318642Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def calculate_accuracy(model, dataloader, device):\n    model.eval() # put in evaluation mode,  turn off Dropout, BatchNorm uses learned statistics\n    total_correct = 0\n    total_images = 0\n    with torch.no_grad():\n        for data in dataloader:\n            images, labels = data\n            images = images.to(device)\n            labels = labels.to(device)\n            images = normalize(images)\n            outputs = model(images)\n            predictions = torch.argmax(outputs, dim=-1)\n            total_images += labels.size(0)\n            total_correct += (predictions == labels).sum().item()\n\n    model_accuracy = total_correct / total_images * 100\n    return model_accuracy\n","metadata":{"id":"k3Z55XXW407W","executionInfo":{"status":"ok","timestamp":1766743013043,"user_tz":-120,"elapsed":4,"user":{"displayName":"Gilad Navok","userId":"06092819627906668279"}},"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:04:02.320940Z","iopub.execute_input":"2025-12-26T19:04:02.321155Z","iopub.status.idle":"2025-12-26T19:04:02.326289Z","shell.execute_reply.started":"2025-12-26T19:04:02.321134Z","shell.execute_reply":"2025-12-26T19:04:02.325484Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Split data set into train-validation-test.\nWe are using 80% train, 20% validation split","metadata":{"id":"kJ7YrZBl6lho"}},{"cell_type":"code","source":"transform = torchvision.transforms.Compose(\n    [torchvision.transforms.ToTensor()]\n)\n\n# 80/20% split\ntrain_val_set = torchvision.datasets.CIFAR10(\n    root='./data',\n    train=True,\n    download=True,\n    transform=transform\n)\n\n\ntargets = np.array(train_val_set.targets)\n\nindices = np.arange(len(targets))\ntrain_indices, remaining_indices = train_test_split(\n    indices,\n    train_size=TRAIN_SIZE,\n    stratify=targets,\n    random_state=42\n)\n\nvalidation_indices, _ = train_test_split(\n    remaining_indices,\n    train_size=VALIDATION_SIZE,\n    stratify=targets[remaining_indices],\n    random_state=42\n)\n\ntrainset = Subset(train_val_set, train_indices)\nvalset = Subset(train_val_set, validation_indices)\nprint(f\"Original size: {len(train_val_set)}\")\nprint(f\"Train size: {len(trainset)}\")\nprint(f\"Val size: {len(valset)}\")\n\nfrom collections import Counter \nsubset_labels = [targets[i] for i in train_indices]\nprint(\"Samples per class\", Counter(subset_labels))\nsubset_labels = [targets[i] for i in validation_indices]\nprint(\"Samples per class\", Counter(subset_labels))\n\ntestset = torchvision.datasets.CIFAR10(\n    root='./data',\n    train=False,\n    download=True,\n    transform=transform\n)\n","metadata":{"id":"rkI3-MRt58hj","executionInfo":{"status":"ok","timestamp":1766743016313,"user_tz":-120,"elapsed":1992,"user":{"displayName":"Gilad Navok","userId":"06092819627906668279"}},"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:04:02.327080Z","iopub.execute_input":"2025-12-26T19:04:02.327276Z","iopub.status.idle":"2025-12-26T19:04:04.009029Z","shell.execute_reply.started":"2025-12-26T19:04:02.327257Z","shell.execute_reply":"2025-12-26T19:04:04.008433Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparamters\nbatch_size = 128\n\nlr = 0.1\nmomentum = 0.9\nweight_decay = 5e-4\n\nT_max = 200\n\nn_epochs = 1 if DEBUG else 200\n\nprint_progress_every = 1\nval_accuracy_storing_threshold = 50\n","metadata":{"id":"Zy87RRrf7Vh6","executionInfo":{"status":"ok","timestamp":1766743919319,"user_tz":-120,"elapsed":2,"user":{"displayName":"Gilad Navok","userId":"06092819627906668279"}},"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:04:04.009932Z","iopub.execute_input":"2025-12-26T19:04:04.010223Z","iopub.status.idle":"2025-12-26T19:04:04.014784Z","shell.execute_reply.started":"2025-12-26T19:04:04.010191Z","shell.execute_reply":"2025-12-26T19:04:04.013880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmean = torch.tensor([0.4914, 0.4822, 0.4465]).to(device)\nstd = torch.tensor([0.2023, 0.1994, 0.2010]).to(device)\nnormalize = K.Normalize(mean=mean, std=std)\n# define a sequence of augmentations\naug_list = AugmentationSequential(\n    K.RandomHorizontalFlip(p=0.5),\n    K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.2),\n    K.RandomResizedCrop(size=(32,32), scale=(0.7, 1.0), p=0.5),\n    normalize,\n    same_on_batch=False\n).to(device)\n\n\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\nvalloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\nmodel = ResNet18().to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n\nstats = {\n    'total_training_time': 0,\n    'loss': [],\n    'time_per_epoch': [],\n    'total_time_per_epoch': [],\n    'val_accuracy': [],\n    'max_val_accuracy': 0,\n    'allocated_memory': [], # Memory currently used by Tensors\n    'reserved_memory': [], # Memory held by the PyTorch caching allocator\n}\n\nstart_time = time.time()\nfor epoch in range(n_epochs):\n    model.train()\n    iteration_losses = []\n    epoch_start_time = time.time()\n    for inputs, targets in trainloader:\n        inputs = inputs.to(device)\n        targets = targets.to(device)\n\n        inputs = aug_list(inputs)\n\n        outputs = model(inputs)\n\n        optimizer.zero_grad()\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        iteration_losses.append(loss.item())\n\n    scheduler.step()\n    epoch_end_time = time.time()\n\n    model.eval()\n    val_accuracy = calculate_accuracy(model, valloader, device)\n\n    # Track stats\n    if (epoch % 1) == 0:\n        stats['loss'].append(\n            np.mean(iteration_losses)\n        )\n        stats['val_accuracy'].append(\n            val_accuracy\n        )\n        stats['allocated_memory'].append(torch.cuda.memory_allocated())\n        stats['reserved_memory'].append(torch.cuda.memory_reserved())\n        stats['time_per_epoch'].append(epoch_end_time - epoch_start_time)\n        stats['total_time_per_epoch'].append(time.time() - start_time)\n\n    # Store best model\n    if (val_accuracy > stats['max_val_accuracy']):\n        if (val_accuracy > val_accuracy_storing_threshold):\n            stats['max_val_accuracy'] = val_accuracy\n            print('==> Saving model ...')\n            state = {\n                'net': model.state_dict(),\n                'epoch': epoch,\n                'acc':val_accuracy\n            }\n            save_path = checkpoint_dir / f\"{MODEL_NAME}_max_acc.pth\"\n            torch.save(state, save_path)\n\n    if DEBUG:\n        print('==> Saving model ... DEBUG')\n        state = {\n            'net': model.state_dict(),\n            'epoch': epoch,\n            'acc':val_accuracy\n        }\n        save_path = checkpoint_dir / f\"{MODEL_NAME}_max_acc.pth\"\n        torch.save(state, save_path)\n        \n    # Print progress\n    if (epoch % print_progress_every) == 0:\n        print(f\"Epoch {epoch} Loss {stats['loss'][-1]:.3f} Val Acc {stats['val_accuracy'][-1]:.3f}\")\n\n","metadata":{"id":"W45Vot2zvrLE","trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:04:04.015600Z","iopub.execute_input":"2025-12-26T19:04:04.015907Z","iopub.status.idle":"2025-12-26T19:04:09.241892Z","shell.execute_reply.started":"2025-12-26T19:04:04.015873Z","shell.execute_reply":"2025-12-26T19:04:09.240958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel = ResNet18().to(device)\ncheckpoint = torch.load(checkpoint_dir / f\"{MODEL_NAME}_max_acc.pth\", map_location=device)\nmodel.load_state_dict(checkpoint['net'])\nmodel.eval()\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\nprint(f'Final test accuracy is: {calculate_accuracy(model, testloader, device):.3f}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:04:09.242840Z","iopub.execute_input":"2025-12-26T19:04:09.243052Z","iopub.status.idle":"2025-12-26T19:04:12.274933Z","shell.execute_reply.started":"2025-12-26T19:04:09.243032Z","shell.execute_reply":"2025-12-26T19:04:12.274261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(training_stats_dir / f'{MODEL_NAME}_stats.pkl', 'wb') as file:\n    pickle.dump(stats, file)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T19:04:12.275990Z","iopub.execute_input":"2025-12-26T19:04:12.276359Z","iopub.status.idle":"2025-12-26T19:04:12.280542Z","shell.execute_reply.started":"2025-12-26T19:04:12.276334Z","shell.execute_reply":"2025-12-26T19:04:12.279876Z"}},"outputs":[],"execution_count":null}]}