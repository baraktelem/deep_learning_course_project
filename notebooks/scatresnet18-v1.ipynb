{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f314c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:17:12.914546Z",
     "iopub.status.busy": "2025-12-30T20:17:12.914292Z",
     "iopub.status.idle": "2025-12-30T20:17:32.896087Z",
     "shell.execute_reply": "2025-12-30T20:17:32.895395Z"
    },
    "executionInfo": {
     "elapsed": 5636,
     "status": "ok",
     "timestamp": 1766743875913,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "zNf1AQxVkZdi",
    "outputId": "7fd9b195-abbc-4da1-f986-982016920821",
    "papermill": {
     "duration": 19.987356,
     "end_time": "2025-12-30T20:17:32.897817",
     "exception": false,
     "start_time": "2025-12-30T20:17:12.910461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kornia in /usr/local/lib/python3.12/dist-packages (0.8.2)\r\n",
      "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from kornia) (0.1.10)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kornia) (25.0)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from kornia) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.4.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->kornia) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->kornia) (3.0.3)\r\n",
      "Collecting kymatio\r\n",
      "  Downloading kymatio-0.3.0-py3-none-any.whl.metadata (9.6 kB)\r\n",
      "Collecting appdirs (from kymatio)\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Collecting configparser (from kymatio)\r\n",
      "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from kymatio) (2.0.2)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kymatio) (25.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from kymatio) (1.15.3)\r\n",
      "Downloading kymatio-0.3.0-py3-none-any.whl (87 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Downloading configparser-7.2.0-py3-none-any.whl (17 kB)\r\n",
      "Installing collected packages: appdirs, configparser, kymatio\r\n",
      "Successfully installed appdirs-1.4.4 configparser-7.2.0 kymatio-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "\n",
    "!pip install kornia\n",
    "!pip install kymatio\n",
    "from kornia import augmentation as K\n",
    "from kornia.augmentation import AugmentationSequential\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from kymatio.torch import Scattering2D\n",
    "\n",
    "DEBUG = False\n",
    "MODEL_NAME = \"ScatResNet18_v1\"\n",
    "\n",
    "env = 'kaggle' # 'kaggle' or 'colab'\n",
    "\n",
    "if env == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_dir = Path('/content/drive/MyDrive/dl_pj')    \n",
    "elif env == 'kaggle':\n",
    "    base_dir = Path('/kaggle/working/')\n",
    "\n",
    "checkpoint_dir = base_dir / 'checkpoints'\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "training_stats_dir = base_dir / 'stats'\n",
    "training_stats_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "Ls = [4, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b0d185b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:17:32.904808Z",
     "iopub.status.busy": "2025-12-30T20:17:32.904244Z",
     "iopub.status.idle": "2025-12-30T20:17:32.916256Z",
     "shell.execute_reply": "2025-12-30T20:17:32.915644Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1766743054230,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "ElgGUpWbrrci",
    "papermill": {
     "duration": 0.016959,
     "end_time": "2025-12-30T20:17:32.917591",
     "exception": false,
     "start_time": "2025-12-30T20:17:32.900632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,kernel_size=1,stride=stride,bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, L=8):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.L = L\n",
    "        self.scat_channels = (1 + L) * 3\n",
    "        self.conv1 = nn.Conv2d(3, 64 - self.scat_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.scat1 = Scattering2D(J=1, shape=(32, 32), L=L, max_order=2, backend='torch')\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=self.scat_channels, out_channels=self.scat_channels, groups=self.scat_channels, kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layer = []\n",
    "        for s in strides:\n",
    "            layer.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        scat = self.scat1(x)\n",
    "        scat = scat.view(scat.size(0), -1, 16,16)\n",
    "        scat = self.deconv1(scat)\n",
    "        out = torch.cat((out, scat), dim=1)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ScatResNet18(L):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], L=L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ea771d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:17:32.923662Z",
     "iopub.status.busy": "2025-12-30T20:17:32.923093Z",
     "iopub.status.idle": "2025-12-30T20:17:33.225670Z",
     "shell.execute_reply": "2025-12-30T20:17:33.224819Z"
    },
    "papermill": {
     "duration": 0.307146,
     "end_time": "2025-12-30T20:17:33.227114",
     "exception": false,
     "start_time": "2025-12-30T20:17:32.919968",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 11,173,632\n",
      "Model Size: 42.62 MB\n",
      "Total Parameters: 11,173,236\n",
      "Model Size: 42.62 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_summary(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    total_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    size_mb = total_bytes / (1024 ** 2)\n",
    "    return num_params, size_mb\n",
    "for L in Ls:\n",
    "    total_params, model_size_mb = get_model_summary(ScatResNet18(L))\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77b045c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:17:33.233488Z",
     "iopub.status.busy": "2025-12-30T20:17:33.233234Z",
     "iopub.status.idle": "2025-12-30T20:17:33.238027Z",
     "shell.execute_reply": "2025-12-30T20:17:33.237451Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766743013043,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "k3Z55XXW407W",
    "papermill": {
     "duration": 0.009479,
     "end_time": "2025-12-30T20:17:33.239280",
     "exception": false,
     "start_time": "2025-12-30T20:17:33.229801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval() # put in evaluation mode,  turn off Dropout, BatchNorm uses learned statistics\n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = normalize(images)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    return model_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed9ec08",
   "metadata": {
    "id": "kJ7YrZBl6lho",
    "papermill": {
     "duration": 0.002828,
     "end_time": "2025-12-30T20:17:33.244561",
     "exception": false,
     "start_time": "2025-12-30T20:17:33.241733",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Split data set into train-validation-test.\n",
    "We are using 80% train, 20% validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e3453de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:17:33.250536Z",
     "iopub.status.busy": "2025-12-30T20:17:33.250304Z",
     "iopub.status.idle": "2025-12-30T20:17:48.045316Z",
     "shell.execute_reply": "2025-12-30T20:17:48.044715Z"
    },
    "executionInfo": {
     "elapsed": 1992,
     "status": "ok",
     "timestamp": 1766743016313,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "rkI3-MRt58hj",
    "papermill": {
     "duration": 14.799932,
     "end_time": "2025-12-30T20:17:48.046972",
     "exception": false,
     "start_time": "2025-12-30T20:17:33.247040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:10<00:00, 15.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "# 80/20% split\n",
    "train_val_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_size = int(0.8 * len(train_val_set))\n",
    "val_size = len(train_val_set) - train_size\n",
    "trainset, valset = random_split(\n",
    "    train_val_set,\n",
    "    [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89108585",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:17:48.057707Z",
     "iopub.status.busy": "2025-12-30T20:17:48.057486Z",
     "iopub.status.idle": "2025-12-30T20:17:48.061157Z",
     "shell.execute_reply": "2025-12-30T20:17:48.060540Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766743919319,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "Zy87RRrf7Vh6",
    "papermill": {
     "duration": 0.010459,
     "end_time": "2025-12-30T20:17:48.062468",
     "exception": false,
     "start_time": "2025-12-30T20:17:48.052009",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "batch_size = 128\n",
    "\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "\n",
    "T_max = 200\n",
    "\n",
    "n_epochs = 1 if DEBUG else 200\n",
    "\n",
    "print_progress_every = 1\n",
    "val_accuracy_storing_threshold = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2716edae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:17:48.072776Z",
     "iopub.status.busy": "2025-12-30T20:17:48.072569Z",
     "iopub.status.idle": "2025-12-30T23:57:18.929012Z",
     "shell.execute_reply": "2025-12-30T23:57:18.928278Z"
    },
    "id": "W45Vot2zvrLE",
    "papermill": {
     "duration": 13170.882093,
     "end_time": "2025-12-30T23:57:18.949125",
     "exception": false,
     "start_time": "2025-12-30T20:17:48.067032",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving model ...\n",
      "L 4 Epoch 0 Loss 1.887 Val Acc 48.690\n",
      "==> Saving model ...\n",
      "L 4 Epoch 1 Loss 1.275 Val Acc 57.520\n",
      "==> Saving model ...\n",
      "L 4 Epoch 2 Loss 1.008 Val Acc 67.410\n",
      "==> Saving model ...\n",
      "L 4 Epoch 3 Loss 0.838 Val Acc 69.760\n",
      "==> Saving model ...\n",
      "L 4 Epoch 4 Loss 0.750 Val Acc 73.940\n",
      "==> Saving model ...\n",
      "L 4 Epoch 5 Loss 0.672 Val Acc 77.830\n",
      "==> Saving model ...\n",
      "L 4 Epoch 6 Loss 0.626 Val Acc 78.530\n",
      "L 4 Epoch 7 Loss 0.581 Val Acc 75.560\n",
      "L 4 Epoch 8 Loss 0.543 Val Acc 78.150\n",
      "==> Saving model ...\n",
      "L 4 Epoch 9 Loss 0.521 Val Acc 79.350\n",
      "L 4 Epoch 10 Loss 0.512 Val Acc 78.150\n",
      "==> Saving model ...\n",
      "L 4 Epoch 11 Loss 0.494 Val Acc 81.100\n",
      "L 4 Epoch 12 Loss 0.484 Val Acc 78.610\n",
      "==> Saving model ...\n",
      "L 4 Epoch 13 Loss 0.468 Val Acc 81.850\n",
      "L 4 Epoch 14 Loss 0.459 Val Acc 81.580\n",
      "==> Saving model ...\n",
      "L 4 Epoch 15 Loss 0.450 Val Acc 82.840\n",
      "L 4 Epoch 16 Loss 0.441 Val Acc 81.510\n",
      "L 4 Epoch 17 Loss 0.438 Val Acc 79.380\n",
      "==> Saving model ...\n",
      "L 4 Epoch 18 Loss 0.420 Val Acc 83.000\n",
      "L 4 Epoch 19 Loss 0.421 Val Acc 79.470\n",
      "==> Saving model ...\n",
      "L 4 Epoch 20 Loss 0.423 Val Acc 83.320\n",
      "L 4 Epoch 21 Loss 0.402 Val Acc 81.230\n",
      "L 4 Epoch 22 Loss 0.396 Val Acc 80.470\n",
      "L 4 Epoch 23 Loss 0.391 Val Acc 82.840\n",
      "==> Saving model ...\n",
      "L 4 Epoch 24 Loss 0.391 Val Acc 84.210\n",
      "L 4 Epoch 25 Loss 0.381 Val Acc 77.540\n",
      "L 4 Epoch 26 Loss 0.378 Val Acc 82.200\n",
      "L 4 Epoch 27 Loss 0.371 Val Acc 82.700\n",
      "L 4 Epoch 28 Loss 0.375 Val Acc 81.760\n",
      "L 4 Epoch 29 Loss 0.359 Val Acc 82.390\n",
      "L 4 Epoch 30 Loss 0.362 Val Acc 82.660\n",
      "L 4 Epoch 31 Loss 0.364 Val Acc 83.160\n",
      "L 4 Epoch 32 Loss 0.357 Val Acc 83.230\n",
      "L 4 Epoch 33 Loss 0.360 Val Acc 83.760\n",
      "L 4 Epoch 34 Loss 0.348 Val Acc 82.890\n",
      "L 4 Epoch 35 Loss 0.353 Val Acc 76.660\n",
      "L 4 Epoch 36 Loss 0.343 Val Acc 79.200\n",
      "L 4 Epoch 37 Loss 0.342 Val Acc 82.420\n",
      "L 4 Epoch 38 Loss 0.338 Val Acc 83.190\n",
      "L 4 Epoch 39 Loss 0.336 Val Acc 82.130\n",
      "==> Saving model ...\n",
      "L 4 Epoch 40 Loss 0.340 Val Acc 85.510\n",
      "L 4 Epoch 41 Loss 0.336 Val Acc 82.880\n",
      "L 4 Epoch 42 Loss 0.321 Val Acc 84.560\n",
      "L 4 Epoch 43 Loss 0.321 Val Acc 83.680\n",
      "L 4 Epoch 44 Loss 0.324 Val Acc 82.830\n",
      "L 4 Epoch 45 Loss 0.314 Val Acc 84.330\n",
      "L 4 Epoch 46 Loss 0.305 Val Acc 83.430\n",
      "L 4 Epoch 47 Loss 0.309 Val Acc 84.200\n",
      "L 4 Epoch 48 Loss 0.313 Val Acc 85.070\n",
      "L 4 Epoch 49 Loss 0.310 Val Acc 83.970\n",
      "==> Saving model ...\n",
      "L 4 Epoch 50 Loss 0.298 Val Acc 85.690\n",
      "L 4 Epoch 51 Loss 0.303 Val Acc 84.680\n",
      "==> Saving model ...\n",
      "L 4 Epoch 52 Loss 0.307 Val Acc 86.460\n",
      "L 4 Epoch 53 Loss 0.289 Val Acc 84.830\n",
      "L 4 Epoch 54 Loss 0.296 Val Acc 82.150\n",
      "L 4 Epoch 55 Loss 0.288 Val Acc 86.100\n",
      "L 4 Epoch 56 Loss 0.300 Val Acc 83.640\n",
      "L 4 Epoch 57 Loss 0.283 Val Acc 83.410\n",
      "L 4 Epoch 58 Loss 0.292 Val Acc 85.940\n",
      "L 4 Epoch 59 Loss 0.290 Val Acc 84.800\n",
      "L 4 Epoch 60 Loss 0.277 Val Acc 85.710\n",
      "L 4 Epoch 61 Loss 0.273 Val Acc 83.540\n",
      "L 4 Epoch 62 Loss 0.280 Val Acc 84.470\n",
      "L 4 Epoch 63 Loss 0.277 Val Acc 85.460\n",
      "L 4 Epoch 64 Loss 0.267 Val Acc 84.890\n",
      "==> Saving model ...\n",
      "L 4 Epoch 65 Loss 0.270 Val Acc 87.050\n",
      "L 4 Epoch 66 Loss 0.272 Val Acc 83.110\n",
      "L 4 Epoch 67 Loss 0.264 Val Acc 86.390\n",
      "==> Saving model ...\n",
      "L 4 Epoch 68 Loss 0.260 Val Acc 87.290\n",
      "L 4 Epoch 69 Loss 0.276 Val Acc 85.610\n",
      "L 4 Epoch 70 Loss 0.255 Val Acc 87.270\n",
      "L 4 Epoch 71 Loss 0.261 Val Acc 85.920\n",
      "L 4 Epoch 72 Loss 0.251 Val Acc 86.010\n",
      "L 4 Epoch 73 Loss 0.256 Val Acc 84.150\n",
      "L 4 Epoch 74 Loss 0.247 Val Acc 85.690\n",
      "L 4 Epoch 75 Loss 0.249 Val Acc 85.260\n",
      "==> Saving model ...\n",
      "L 4 Epoch 76 Loss 0.243 Val Acc 87.500\n",
      "L 4 Epoch 77 Loss 0.236 Val Acc 86.960\n",
      "L 4 Epoch 78 Loss 0.244 Val Acc 83.810\n",
      "L 4 Epoch 79 Loss 0.241 Val Acc 87.200\n",
      "L 4 Epoch 80 Loss 0.226 Val Acc 87.500\n",
      "L 4 Epoch 81 Loss 0.222 Val Acc 85.580\n",
      "==> Saving model ...\n",
      "L 4 Epoch 82 Loss 0.240 Val Acc 88.290\n",
      "L 4 Epoch 83 Loss 0.219 Val Acc 86.940\n",
      "L 4 Epoch 84 Loss 0.220 Val Acc 86.990\n",
      "L 4 Epoch 85 Loss 0.225 Val Acc 87.300\n",
      "L 4 Epoch 86 Loss 0.206 Val Acc 86.970\n",
      "L 4 Epoch 87 Loss 0.224 Val Acc 85.920\n",
      "L 4 Epoch 88 Loss 0.215 Val Acc 84.030\n",
      "L 4 Epoch 89 Loss 0.211 Val Acc 86.260\n",
      "L 4 Epoch 90 Loss 0.203 Val Acc 85.720\n",
      "L 4 Epoch 91 Loss 0.206 Val Acc 88.150\n",
      "L 4 Epoch 92 Loss 0.205 Val Acc 88.060\n",
      "L 4 Epoch 93 Loss 0.196 Val Acc 86.430\n",
      "L 4 Epoch 94 Loss 0.192 Val Acc 87.910\n",
      "L 4 Epoch 95 Loss 0.195 Val Acc 85.410\n",
      "L 4 Epoch 96 Loss 0.197 Val Acc 87.570\n",
      "L 4 Epoch 97 Loss 0.197 Val Acc 87.090\n",
      "L 4 Epoch 98 Loss 0.188 Val Acc 87.530\n",
      "L 4 Epoch 99 Loss 0.181 Val Acc 86.470\n",
      "L 4 Epoch 100 Loss 0.196 Val Acc 87.640\n",
      "L 4 Epoch 101 Loss 0.172 Val Acc 87.110\n",
      "==> Saving model ...\n",
      "L 4 Epoch 102 Loss 0.179 Val Acc 89.010\n",
      "L 4 Epoch 103 Loss 0.172 Val Acc 88.330\n",
      "L 4 Epoch 104 Loss 0.180 Val Acc 86.470\n",
      "L 4 Epoch 105 Loss 0.174 Val Acc 88.120\n",
      "L 4 Epoch 106 Loss 0.164 Val Acc 88.400\n",
      "L 4 Epoch 107 Loss 0.165 Val Acc 87.200\n",
      "L 4 Epoch 108 Loss 0.154 Val Acc 88.780\n",
      "L 4 Epoch 109 Loss 0.151 Val Acc 86.600\n",
      "L 4 Epoch 110 Loss 0.151 Val Acc 87.210\n",
      "L 4 Epoch 111 Loss 0.152 Val Acc 88.030\n",
      "L 4 Epoch 112 Loss 0.152 Val Acc 88.080\n",
      "L 4 Epoch 113 Loss 0.141 Val Acc 88.290\n",
      "==> Saving model ...\n",
      "L 4 Epoch 114 Loss 0.145 Val Acc 89.610\n",
      "L 4 Epoch 115 Loss 0.148 Val Acc 89.460\n",
      "L 4 Epoch 116 Loss 0.133 Val Acc 87.190\n",
      "L 4 Epoch 117 Loss 0.140 Val Acc 87.510\n",
      "==> Saving model ...\n",
      "L 4 Epoch 118 Loss 0.129 Val Acc 89.740\n",
      "==> Saving model ...\n",
      "L 4 Epoch 119 Loss 0.135 Val Acc 90.330\n",
      "L 4 Epoch 120 Loss 0.125 Val Acc 88.680\n",
      "L 4 Epoch 121 Loss 0.131 Val Acc 89.810\n",
      "L 4 Epoch 122 Loss 0.122 Val Acc 88.980\n",
      "L 4 Epoch 123 Loss 0.113 Val Acc 89.600\n",
      "L 4 Epoch 124 Loss 0.120 Val Acc 89.870\n",
      "L 4 Epoch 125 Loss 0.109 Val Acc 89.880\n",
      "L 4 Epoch 126 Loss 0.108 Val Acc 88.080\n",
      "L 4 Epoch 127 Loss 0.110 Val Acc 89.410\n",
      "L 4 Epoch 128 Loss 0.104 Val Acc 89.550\n",
      "L 4 Epoch 129 Loss 0.094 Val Acc 89.700\n",
      "L 4 Epoch 130 Loss 0.102 Val Acc 88.120\n",
      "L 4 Epoch 131 Loss 0.091 Val Acc 89.080\n",
      "L 4 Epoch 132 Loss 0.094 Val Acc 89.910\n",
      "==> Saving model ...\n",
      "L 4 Epoch 133 Loss 0.085 Val Acc 90.510\n",
      "L 4 Epoch 134 Loss 0.083 Val Acc 90.470\n",
      "==> Saving model ...\n",
      "L 4 Epoch 135 Loss 0.084 Val Acc 90.850\n",
      "L 4 Epoch 136 Loss 0.089 Val Acc 88.840\n",
      "L 4 Epoch 137 Loss 0.080 Val Acc 90.080\n",
      "L 4 Epoch 138 Loss 0.082 Val Acc 90.790\n",
      "L 4 Epoch 139 Loss 0.076 Val Acc 90.770\n",
      "L 4 Epoch 140 Loss 0.080 Val Acc 90.190\n",
      "==> Saving model ...\n",
      "L 4 Epoch 141 Loss 0.069 Val Acc 91.500\n",
      "L 4 Epoch 142 Loss 0.071 Val Acc 90.820\n",
      "L 4 Epoch 143 Loss 0.064 Val Acc 90.360\n",
      "==> Saving model ...\n",
      "L 4 Epoch 144 Loss 0.058 Val Acc 91.600\n",
      "L 4 Epoch 145 Loss 0.059 Val Acc 91.000\n",
      "L 4 Epoch 146 Loss 0.051 Val Acc 91.250\n",
      "L 4 Epoch 147 Loss 0.056 Val Acc 90.710\n",
      "L 4 Epoch 148 Loss 0.059 Val Acc 90.160\n",
      "==> Saving model ...\n",
      "L 4 Epoch 149 Loss 0.049 Val Acc 91.720\n",
      "L 4 Epoch 150 Loss 0.045 Val Acc 91.430\n",
      "==> Saving model ...\n",
      "L 4 Epoch 151 Loss 0.043 Val Acc 91.920\n",
      "L 4 Epoch 152 Loss 0.039 Val Acc 91.690\n",
      "L 4 Epoch 153 Loss 0.036 Val Acc 91.700\n",
      "L 4 Epoch 154 Loss 0.034 Val Acc 91.440\n",
      "==> Saving model ...\n",
      "L 4 Epoch 155 Loss 0.036 Val Acc 92.310\n",
      "L 4 Epoch 156 Loss 0.037 Val Acc 92.010\n",
      "L 4 Epoch 157 Loss 0.031 Val Acc 92.220\n",
      "==> Saving model ...\n",
      "L 4 Epoch 158 Loss 0.025 Val Acc 92.400\n",
      "L 4 Epoch 159 Loss 0.031 Val Acc 92.160\n",
      "==> Saving model ...\n",
      "L 4 Epoch 160 Loss 0.023 Val Acc 92.610\n",
      "L 4 Epoch 161 Loss 0.024 Val Acc 92.280\n",
      "==> Saving model ...\n",
      "L 4 Epoch 162 Loss 0.020 Val Acc 92.700\n",
      "L 4 Epoch 163 Loss 0.020 Val Acc 92.700\n",
      "L 4 Epoch 164 Loss 0.018 Val Acc 92.650\n",
      "L 4 Epoch 165 Loss 0.018 Val Acc 92.560\n",
      "==> Saving model ...\n",
      "L 4 Epoch 166 Loss 0.017 Val Acc 92.800\n",
      "==> Saving model ...\n",
      "L 4 Epoch 167 Loss 0.014 Val Acc 93.130\n",
      "==> Saving model ...\n",
      "L 4 Epoch 168 Loss 0.014 Val Acc 93.230\n",
      "L 4 Epoch 169 Loss 0.014 Val Acc 93.090\n",
      "==> Saving model ...\n",
      "L 4 Epoch 170 Loss 0.015 Val Acc 93.320\n",
      "==> Saving model ...\n",
      "L 4 Epoch 171 Loss 0.011 Val Acc 93.490\n",
      "==> Saving model ...\n",
      "L 4 Epoch 172 Loss 0.009 Val Acc 93.630\n",
      "L 4 Epoch 173 Loss 0.010 Val Acc 93.360\n",
      "L 4 Epoch 174 Loss 0.008 Val Acc 93.530\n",
      "==> Saving model ...\n",
      "L 4 Epoch 175 Loss 0.009 Val Acc 93.720\n",
      "L 4 Epoch 176 Loss 0.009 Val Acc 93.710\n",
      "L 4 Epoch 177 Loss 0.008 Val Acc 93.710\n",
      "L 4 Epoch 178 Loss 0.007 Val Acc 93.590\n",
      "L 4 Epoch 179 Loss 0.007 Val Acc 93.690\n",
      "==> Saving model ...\n",
      "L 4 Epoch 180 Loss 0.007 Val Acc 93.880\n",
      "L 4 Epoch 181 Loss 0.006 Val Acc 93.830\n",
      "L 4 Epoch 182 Loss 0.007 Val Acc 93.870\n",
      "L 4 Epoch 183 Loss 0.006 Val Acc 93.640\n",
      "L 4 Epoch 184 Loss 0.006 Val Acc 93.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 185 Loss 0.006 Val Acc 93.920\n",
      "==> Saving model ...\n",
      "L 4 Epoch 186 Loss 0.006 Val Acc 94.010\n",
      "L 4 Epoch 187 Loss 0.006 Val Acc 93.690\n",
      "==> Saving model ...\n",
      "L 4 Epoch 188 Loss 0.006 Val Acc 94.030\n",
      "L 4 Epoch 189 Loss 0.005 Val Acc 93.770\n",
      "L 4 Epoch 190 Loss 0.005 Val Acc 93.950\n",
      "L 4 Epoch 191 Loss 0.005 Val Acc 93.970\n",
      "L 4 Epoch 192 Loss 0.005 Val Acc 93.990\n",
      "==> Saving model ...\n",
      "L 4 Epoch 193 Loss 0.004 Val Acc 94.070\n",
      "L 4 Epoch 194 Loss 0.005 Val Acc 94.040\n",
      "==> Saving model ...\n",
      "L 4 Epoch 195 Loss 0.004 Val Acc 94.120\n",
      "L 4 Epoch 196 Loss 0.005 Val Acc 93.900\n",
      "L 4 Epoch 197 Loss 0.005 Val Acc 93.970\n",
      "L 4 Epoch 198 Loss 0.005 Val Acc 94.000\n",
      "==> Saving model ...\n",
      "L 4 Epoch 199 Loss 0.005 Val Acc 94.190\n",
      "==> Saving model ...\n",
      "L 10 Epoch 0 Loss 1.753 Val Acc 51.420\n",
      "==> Saving model ...\n",
      "L 10 Epoch 1 Loss 1.216 Val Acc 60.560\n",
      "==> Saving model ...\n",
      "L 10 Epoch 2 Loss 0.999 Val Acc 66.910\n",
      "==> Saving model ...\n",
      "L 10 Epoch 3 Loss 0.871 Val Acc 69.780\n",
      "==> Saving model ...\n",
      "L 10 Epoch 4 Loss 0.769 Val Acc 70.980\n",
      "==> Saving model ...\n",
      "L 10 Epoch 5 Loss 0.693 Val Acc 74.980\n",
      "==> Saving model ...\n",
      "L 10 Epoch 6 Loss 0.642 Val Acc 79.340\n",
      "L 10 Epoch 7 Loss 0.599 Val Acc 77.050\n",
      "L 10 Epoch 8 Loss 0.573 Val Acc 76.280\n",
      "L 10 Epoch 9 Loss 0.549 Val Acc 76.340\n",
      "L 10 Epoch 10 Loss 0.534 Val Acc 77.110\n",
      "L 10 Epoch 11 Loss 0.515 Val Acc 78.170\n",
      "L 10 Epoch 12 Loss 0.499 Val Acc 76.870\n",
      "==> Saving model ...\n",
      "L 10 Epoch 13 Loss 0.487 Val Acc 81.240\n",
      "==> Saving model ...\n",
      "L 10 Epoch 14 Loss 0.474 Val Acc 81.770\n",
      "L 10 Epoch 15 Loss 0.465 Val Acc 77.860\n",
      "L 10 Epoch 16 Loss 0.450 Val Acc 77.830\n",
      "L 10 Epoch 17 Loss 0.445 Val Acc 79.910\n",
      "L 10 Epoch 18 Loss 0.431 Val Acc 79.980\n",
      "L 10 Epoch 19 Loss 0.426 Val Acc 79.690\n",
      "==> Saving model ...\n",
      "L 10 Epoch 20 Loss 0.425 Val Acc 81.850\n",
      "==> Saving model ...\n",
      "L 10 Epoch 21 Loss 0.425 Val Acc 82.960\n",
      "==> Saving model ...\n",
      "L 10 Epoch 22 Loss 0.406 Val Acc 83.550\n",
      "L 10 Epoch 23 Loss 0.401 Val Acc 80.760\n",
      "L 10 Epoch 24 Loss 0.398 Val Acc 82.500\n",
      "L 10 Epoch 25 Loss 0.398 Val Acc 80.180\n",
      "L 10 Epoch 26 Loss 0.384 Val Acc 80.340\n",
      "L 10 Epoch 27 Loss 0.387 Val Acc 82.580\n",
      "==> Saving model ...\n",
      "L 10 Epoch 28 Loss 0.377 Val Acc 83.700\n",
      "L 10 Epoch 29 Loss 0.374 Val Acc 82.570\n",
      "L 10 Epoch 30 Loss 0.359 Val Acc 79.950\n",
      "L 10 Epoch 31 Loss 0.367 Val Acc 82.120\n",
      "L 10 Epoch 32 Loss 0.351 Val Acc 81.010\n",
      "==> Saving model ...\n",
      "L 10 Epoch 33 Loss 0.358 Val Acc 83.820\n",
      "L 10 Epoch 34 Loss 0.359 Val Acc 83.690\n",
      "L 10 Epoch 35 Loss 0.352 Val Acc 80.870\n",
      "L 10 Epoch 36 Loss 0.348 Val Acc 83.110\n",
      "L 10 Epoch 37 Loss 0.349 Val Acc 81.160\n",
      "==> Saving model ...\n",
      "L 10 Epoch 38 Loss 0.337 Val Acc 85.460\n",
      "L 10 Epoch 39 Loss 0.340 Val Acc 82.410\n",
      "L 10 Epoch 40 Loss 0.333 Val Acc 84.050\n",
      "L 10 Epoch 41 Loss 0.328 Val Acc 84.350\n",
      "L 10 Epoch 42 Loss 0.325 Val Acc 84.700\n",
      "L 10 Epoch 43 Loss 0.314 Val Acc 84.860\n",
      "L 10 Epoch 44 Loss 0.326 Val Acc 81.080\n",
      "L 10 Epoch 45 Loss 0.319 Val Acc 83.750\n",
      "L 10 Epoch 46 Loss 0.321 Val Acc 83.390\n",
      "L 10 Epoch 47 Loss 0.312 Val Acc 84.060\n",
      "L 10 Epoch 48 Loss 0.300 Val Acc 84.570\n",
      "L 10 Epoch 49 Loss 0.308 Val Acc 81.270\n",
      "L 10 Epoch 50 Loss 0.305 Val Acc 83.410\n",
      "L 10 Epoch 51 Loss 0.304 Val Acc 83.340\n",
      "L 10 Epoch 52 Loss 0.285 Val Acc 85.370\n",
      "L 10 Epoch 53 Loss 0.302 Val Acc 83.240\n",
      "==> Saving model ...\n",
      "L 10 Epoch 54 Loss 0.290 Val Acc 86.680\n",
      "L 10 Epoch 55 Loss 0.284 Val Acc 85.200\n",
      "L 10 Epoch 56 Loss 0.290 Val Acc 83.630\n",
      "L 10 Epoch 57 Loss 0.283 Val Acc 84.500\n",
      "L 10 Epoch 58 Loss 0.281 Val Acc 79.180\n",
      "L 10 Epoch 59 Loss 0.280 Val Acc 85.300\n",
      "L 10 Epoch 60 Loss 0.271 Val Acc 84.390\n",
      "L 10 Epoch 61 Loss 0.266 Val Acc 80.880\n",
      "L 10 Epoch 62 Loss 0.271 Val Acc 86.140\n",
      "L 10 Epoch 63 Loss 0.265 Val Acc 84.150\n",
      "L 10 Epoch 64 Loss 0.271 Val Acc 85.030\n",
      "L 10 Epoch 65 Loss 0.260 Val Acc 86.200\n",
      "L 10 Epoch 66 Loss 0.261 Val Acc 85.130\n",
      "L 10 Epoch 67 Loss 0.245 Val Acc 86.480\n",
      "L 10 Epoch 68 Loss 0.253 Val Acc 82.880\n",
      "L 10 Epoch 69 Loss 0.257 Val Acc 85.990\n",
      "L 10 Epoch 70 Loss 0.263 Val Acc 85.810\n",
      "L 10 Epoch 71 Loss 0.240 Val Acc 83.360\n",
      "==> Saving model ...\n",
      "L 10 Epoch 72 Loss 0.241 Val Acc 87.190\n",
      "L 10 Epoch 73 Loss 0.239 Val Acc 86.730\n",
      "==> Saving model ...\n",
      "L 10 Epoch 74 Loss 0.229 Val Acc 87.220\n",
      "L 10 Epoch 75 Loss 0.238 Val Acc 84.510\n",
      "L 10 Epoch 76 Loss 0.245 Val Acc 85.790\n",
      "L 10 Epoch 77 Loss 0.232 Val Acc 86.900\n",
      "L 10 Epoch 78 Loss 0.226 Val Acc 85.570\n",
      "L 10 Epoch 79 Loss 0.220 Val Acc 85.040\n",
      "==> Saving model ...\n",
      "L 10 Epoch 80 Loss 0.229 Val Acc 87.520\n",
      "L 10 Epoch 81 Loss 0.218 Val Acc 87.490\n",
      "L 10 Epoch 82 Loss 0.217 Val Acc 84.580\n",
      "L 10 Epoch 83 Loss 0.219 Val Acc 86.560\n",
      "L 10 Epoch 84 Loss 0.223 Val Acc 84.500\n",
      "==> Saving model ...\n",
      "L 10 Epoch 85 Loss 0.210 Val Acc 88.220\n",
      "L 10 Epoch 86 Loss 0.217 Val Acc 85.580\n",
      "L 10 Epoch 87 Loss 0.203 Val Acc 85.820\n",
      "L 10 Epoch 88 Loss 0.204 Val Acc 86.150\n",
      "L 10 Epoch 89 Loss 0.199 Val Acc 86.930\n",
      "L 10 Epoch 90 Loss 0.200 Val Acc 86.610\n",
      "L 10 Epoch 91 Loss 0.205 Val Acc 83.220\n",
      "L 10 Epoch 92 Loss 0.196 Val Acc 86.610\n",
      "L 10 Epoch 93 Loss 0.207 Val Acc 88.100\n",
      "L 10 Epoch 94 Loss 0.192 Val Acc 85.700\n",
      "L 10 Epoch 95 Loss 0.184 Val Acc 87.480\n",
      "L 10 Epoch 96 Loss 0.178 Val Acc 87.740\n",
      "L 10 Epoch 97 Loss 0.183 Val Acc 86.130\n",
      "==> Saving model ...\n",
      "L 10 Epoch 98 Loss 0.171 Val Acc 88.810\n",
      "L 10 Epoch 99 Loss 0.182 Val Acc 85.880\n",
      "L 10 Epoch 100 Loss 0.171 Val Acc 86.520\n",
      "L 10 Epoch 101 Loss 0.179 Val Acc 88.700\n",
      "L 10 Epoch 102 Loss 0.156 Val Acc 87.440\n",
      "==> Saving model ...\n",
      "L 10 Epoch 103 Loss 0.170 Val Acc 89.110\n",
      "L 10 Epoch 104 Loss 0.154 Val Acc 88.760\n",
      "L 10 Epoch 105 Loss 0.161 Val Acc 87.590\n",
      "L 10 Epoch 106 Loss 0.159 Val Acc 87.840\n",
      "==> Saving model ...\n",
      "L 10 Epoch 107 Loss 0.148 Val Acc 89.710\n",
      "L 10 Epoch 108 Loss 0.145 Val Acc 88.380\n",
      "L 10 Epoch 109 Loss 0.146 Val Acc 87.560\n",
      "L 10 Epoch 110 Loss 0.150 Val Acc 89.630\n",
      "L 10 Epoch 111 Loss 0.141 Val Acc 88.680\n",
      "L 10 Epoch 112 Loss 0.141 Val Acc 88.000\n",
      "L 10 Epoch 113 Loss 0.145 Val Acc 88.370\n",
      "L 10 Epoch 114 Loss 0.145 Val Acc 88.510\n",
      "L 10 Epoch 115 Loss 0.121 Val Acc 89.430\n",
      "L 10 Epoch 116 Loss 0.118 Val Acc 87.600\n",
      "L 10 Epoch 117 Loss 0.134 Val Acc 89.420\n",
      "L 10 Epoch 118 Loss 0.134 Val Acc 86.660\n",
      "L 10 Epoch 119 Loss 0.116 Val Acc 89.660\n",
      "L 10 Epoch 120 Loss 0.116 Val Acc 87.210\n",
      "==> Saving model ...\n",
      "L 10 Epoch 121 Loss 0.115 Val Acc 90.150\n",
      "L 10 Epoch 122 Loss 0.109 Val Acc 89.610\n",
      "==> Saving model ...\n",
      "L 10 Epoch 123 Loss 0.109 Val Acc 90.440\n",
      "L 10 Epoch 124 Loss 0.111 Val Acc 89.060\n",
      "L 10 Epoch 125 Loss 0.109 Val Acc 89.590\n",
      "L 10 Epoch 126 Loss 0.094 Val Acc 89.570\n",
      "L 10 Epoch 127 Loss 0.095 Val Acc 90.360\n",
      "L 10 Epoch 128 Loss 0.095 Val Acc 88.190\n",
      "L 10 Epoch 129 Loss 0.100 Val Acc 88.920\n",
      "L 10 Epoch 130 Loss 0.087 Val Acc 89.970\n",
      "L 10 Epoch 131 Loss 0.095 Val Acc 89.760\n",
      "==> Saving model ...\n",
      "L 10 Epoch 132 Loss 0.089 Val Acc 90.550\n",
      "==> Saving model ...\n",
      "L 10 Epoch 133 Loss 0.086 Val Acc 90.970\n",
      "L 10 Epoch 134 Loss 0.082 Val Acc 87.580\n",
      "L 10 Epoch 135 Loss 0.080 Val Acc 90.830\n",
      "L 10 Epoch 136 Loss 0.076 Val Acc 90.140\n",
      "L 10 Epoch 137 Loss 0.071 Val Acc 90.900\n",
      "L 10 Epoch 138 Loss 0.077 Val Acc 89.860\n",
      "L 10 Epoch 139 Loss 0.067 Val Acc 90.720\n",
      "==> Saving model ...\n",
      "L 10 Epoch 140 Loss 0.069 Val Acc 91.190\n",
      "L 10 Epoch 141 Loss 0.065 Val Acc 90.070\n",
      "L 10 Epoch 142 Loss 0.062 Val Acc 90.300\n",
      "L 10 Epoch 143 Loss 0.053 Val Acc 90.370\n",
      "L 10 Epoch 144 Loss 0.056 Val Acc 91.170\n",
      "==> Saving model ...\n",
      "L 10 Epoch 145 Loss 0.058 Val Acc 91.520\n",
      "L 10 Epoch 146 Loss 0.054 Val Acc 90.860\n",
      "==> Saving model ...\n",
      "L 10 Epoch 147 Loss 0.052 Val Acc 91.670\n",
      "==> Saving model ...\n",
      "L 10 Epoch 148 Loss 0.046 Val Acc 91.940\n",
      "L 10 Epoch 149 Loss 0.046 Val Acc 90.820\n",
      "L 10 Epoch 150 Loss 0.040 Val Acc 91.540\n",
      "L 10 Epoch 151 Loss 0.040 Val Acc 91.350\n",
      "==> Saving model ...\n",
      "L 10 Epoch 152 Loss 0.038 Val Acc 91.960\n",
      "L 10 Epoch 153 Loss 0.037 Val Acc 91.360\n",
      "==> Saving model ...\n",
      "L 10 Epoch 154 Loss 0.036 Val Acc 92.900\n",
      "L 10 Epoch 155 Loss 0.031 Val Acc 91.850\n",
      "L 10 Epoch 156 Loss 0.030 Val Acc 92.400\n",
      "L 10 Epoch 157 Loss 0.033 Val Acc 92.500\n",
      "L 10 Epoch 158 Loss 0.029 Val Acc 92.590\n",
      "L 10 Epoch 159 Loss 0.028 Val Acc 92.430\n",
      "L 10 Epoch 160 Loss 0.024 Val Acc 92.280\n",
      "==> Saving model ...\n",
      "L 10 Epoch 161 Loss 0.023 Val Acc 92.910\n",
      "==> Saving model ...\n",
      "L 10 Epoch 162 Loss 0.018 Val Acc 92.970\n",
      "L 10 Epoch 163 Loss 0.020 Val Acc 92.890\n",
      "L 10 Epoch 164 Loss 0.019 Val Acc 92.430\n",
      "==> Saving model ...\n",
      "L 10 Epoch 165 Loss 0.013 Val Acc 93.160\n",
      "==> Saving model ...\n",
      "L 10 Epoch 166 Loss 0.015 Val Acc 93.170\n",
      "==> Saving model ...\n",
      "L 10 Epoch 167 Loss 0.015 Val Acc 93.360\n",
      "L 10 Epoch 168 Loss 0.013 Val Acc 93.030\n",
      "==> Saving model ...\n",
      "L 10 Epoch 169 Loss 0.011 Val Acc 93.740\n",
      "L 10 Epoch 170 Loss 0.010 Val Acc 93.470\n",
      "L 10 Epoch 171 Loss 0.011 Val Acc 93.650\n",
      "L 10 Epoch 172 Loss 0.009 Val Acc 93.590\n",
      "L 10 Epoch 173 Loss 0.009 Val Acc 93.330\n",
      "L 10 Epoch 174 Loss 0.008 Val Acc 93.670\n",
      "L 10 Epoch 175 Loss 0.006 Val Acc 93.350\n",
      "L 10 Epoch 176 Loss 0.007 Val Acc 93.400\n",
      "==> Saving model ...\n",
      "L 10 Epoch 177 Loss 0.008 Val Acc 93.800\n",
      "==> Saving model ...\n",
      "L 10 Epoch 178 Loss 0.007 Val Acc 93.840\n",
      "==> Saving model ...\n",
      "L 10 Epoch 179 Loss 0.006 Val Acc 93.900\n",
      "L 10 Epoch 180 Loss 0.007 Val Acc 93.750\n",
      "L 10 Epoch 181 Loss 0.006 Val Acc 93.880\n",
      "L 10 Epoch 182 Loss 0.006 Val Acc 93.710\n",
      "L 10 Epoch 183 Loss 0.005 Val Acc 93.640\n",
      "L 10 Epoch 184 Loss 0.006 Val Acc 93.750\n",
      "==> Saving model ...\n",
      "L 10 Epoch 185 Loss 0.006 Val Acc 93.920\n",
      "L 10 Epoch 186 Loss 0.005 Val Acc 93.830\n",
      "==> Saving model ...\n",
      "L 10 Epoch 187 Loss 0.005 Val Acc 94.050\n",
      "==> Saving model ...\n",
      "L 10 Epoch 188 Loss 0.005 Val Acc 94.280\n",
      "L 10 Epoch 189 Loss 0.005 Val Acc 93.930\n",
      "L 10 Epoch 190 Loss 0.004 Val Acc 94.060\n",
      "L 10 Epoch 191 Loss 0.005 Val Acc 93.890\n",
      "L 10 Epoch 192 Loss 0.005 Val Acc 94.230\n",
      "L 10 Epoch 193 Loss 0.005 Val Acc 94.250\n",
      "L 10 Epoch 194 Loss 0.004 Val Acc 94.280\n",
      "L 10 Epoch 195 Loss 0.004 Val Acc 93.950\n",
      "L 10 Epoch 196 Loss 0.004 Val Acc 93.890\n",
      "==> Saving model ...\n",
      "L 10 Epoch 197 Loss 0.005 Val Acc 94.550\n",
      "L 10 Epoch 198 Loss 0.005 Val Acc 94.200\n",
      "L 10 Epoch 199 Loss 0.004 Val Acc 94.320\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465]).to(device)\n",
    "std = torch.tensor([0.2023, 0.1994, 0.2010]).to(device)\n",
    "normalize = K.Normalize(mean=mean, std=std)\n",
    "# define a sequence of augmentations\n",
    "aug_list = AugmentationSequential(\n",
    "    K.RandomHorizontalFlip(p=0.5),\n",
    "    K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.2),\n",
    "    K.RandomResizedCrop(size=(32,32), scale=(0.7, 1.0), p=0.5),\n",
    "    normalize,\n",
    "    same_on_batch=False\n",
    ").to(device)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "stats_dict = {}\n",
    "for L in Ls:\n",
    "    model = ScatResNet18(L).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
    "    \n",
    "    stats = {\n",
    "        'total_training_time': 0,\n",
    "        'loss': [],\n",
    "        'time_per_epoch': [],\n",
    "        'total_time_per_epoch': [],\n",
    "        'val_accuracy': [],\n",
    "        'max_val_accuracy': 0,\n",
    "        'allocated_memory': [], # Memory currently used by Tensors\n",
    "        'reserved_memory': [], # Memory held by the PyTorch caching allocator\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        iteration_losses = []\n",
    "        epoch_start_time = time.time()\n",
    "        for inputs, targets in trainloader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "    \n",
    "            inputs = aug_list(inputs)\n",
    "    \n",
    "            outputs = model(inputs)\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            iteration_losses.append(loss.item())\n",
    "    \n",
    "        scheduler.step()\n",
    "        epoch_end_time = time.time()\n",
    "    \n",
    "        model.eval()\n",
    "        val_accuracy = calculate_accuracy(model, valloader, device)\n",
    "    \n",
    "        # Track stats\n",
    "        if (epoch % 1) == 0:\n",
    "            stats['loss'].append(\n",
    "                np.mean(iteration_losses)\n",
    "            )\n",
    "            stats['val_accuracy'].append(\n",
    "                val_accuracy\n",
    "            )\n",
    "            stats['allocated_memory'].append(torch.cuda.memory_allocated())\n",
    "            stats['reserved_memory'].append(torch.cuda.memory_reserved())\n",
    "            stats['time_per_epoch'].append(epoch_end_time - epoch_start_time)\n",
    "            stats['total_time_per_epoch'].append(time.time() - start_time)\n",
    "    \n",
    "        # Store best model\n",
    "        if (val_accuracy > stats['max_val_accuracy']):\n",
    "            if (val_accuracy > val_accuracy_storing_threshold):\n",
    "                stats['max_val_accuracy'] = val_accuracy\n",
    "                print('==> Saving model ...')\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'acc':val_accuracy\n",
    "                }\n",
    "                save_path = checkpoint_dir / f\"{MODEL_NAME}_L{L}_max_acc.pth\"\n",
    "                torch.save(state, save_path)\n",
    "    \n",
    "        if DEBUG:\n",
    "            print('==> Saving model ... DEBUG')\n",
    "            state = {\n",
    "                'net': model.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'acc':val_accuracy\n",
    "            }\n",
    "            save_path = checkpoint_dir / f\"{MODEL_NAME}_L{L}_max_acc.pth\"\n",
    "            torch.save(state, save_path)\n",
    "            \n",
    "        # Print progress\n",
    "        if (epoch % print_progress_every) == 0:\n",
    "            print(f\"L {L} Epoch {epoch} Loss {stats['loss'][-1]:.3f} Val Acc {stats['val_accuracy'][-1]:.3f}\")\n",
    "    stats_dict[L] = stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47779f31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T23:57:18.986685Z",
     "iopub.status.busy": "2025-12-30T23:57:18.986392Z",
     "iopub.status.idle": "2025-12-30T23:57:57.835273Z",
     "shell.execute_reply": "2025-12-30T23:57:57.834541Z"
    },
    "papermill": {
     "duration": 38.888043,
     "end_time": "2025-12-30T23:57:57.855227",
     "exception": false,
     "start_time": "2025-12-30T23:57:18.967184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy for L 4 is: 93.460\n",
      "Final test accuracy for L 10 is: 93.970\n",
      "Final val accuracy for L 4 is: 94.190\n",
      "Final val accuracy for L 10 is: 94.550\n",
      "Final train accuracy for L 4 is: 100.000\n",
      "Final train accuracy for L 10 is: 100.000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
    "for L in Ls:\n",
    "    model = ScatResNet18(L).to(device)\n",
    "    checkpoint = torch.load(checkpoint_dir / f\"{MODEL_NAME}_L{L}_max_acc.pth\", map_location=device)\n",
    "    model.load_state_dict(checkpoint['net'])\n",
    "    model.eval()\n",
    "    \n",
    "    stats = stats_dict[L]\n",
    "    total_params, model_size_mb = get_model_summary(ScatResNet18(L))\n",
    "    stats['total_params'] = total_params\n",
    "    stats['model_size_mb'] = model_size_mb\n",
    "    stats['train_acc'] = calculate_accuracy(model, trainloader, device)\n",
    "    stats['val_acc'] = calculate_accuracy(model, valloader, device)\n",
    "    stats['test_acc'] = calculate_accuracy(model, testloader, device)\n",
    "    with open(training_stats_dir / f'{MODEL_NAME}_L{L}_stats.pkl', 'wb') as file:\n",
    "        pickle.dump(stats, file)\n",
    "    \n",
    "for L in Ls:\n",
    "    print(f'Final test accuracy for L {L} is: {stats_dict[L]['test_acc']:.3f}')\n",
    "for L in Ls:\n",
    "    print(f'Final val accuracy for L {L} is: {stats_dict[L]['val_acc']:.3f}')\n",
    "for L in Ls:\n",
    "    print(f'Final train accuracy for L {L} is: {stats_dict[L]['train_acc']:.3f}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOlKRvREyFsuJ9trrs14c5Y",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13250.068568,
   "end_time": "2025-12-30T23:58:00.517011",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-30T20:17:10.448443",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
