{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cf6b9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:07:47.011129Z",
     "iopub.status.busy": "2025-12-30T20:07:47.010665Z",
     "iopub.status.idle": "2025-12-30T20:08:07.450640Z",
     "shell.execute_reply": "2025-12-30T20:08:07.449976Z"
    },
    "executionInfo": {
     "elapsed": 5636,
     "status": "ok",
     "timestamp": 1766743875913,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "zNf1AQxVkZdi",
    "outputId": "7fd9b195-abbc-4da1-f986-982016920821",
    "papermill": {
     "duration": 20.445615,
     "end_time": "2025-12-30T20:08:07.452308",
     "exception": false,
     "start_time": "2025-12-30T20:07:47.006693",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kornia in /usr/local/lib/python3.12/dist-packages (0.8.2)\r\n",
      "Requirement already satisfied: kornia_rs>=0.1.9 in /usr/local/lib/python3.12/dist-packages (from kornia) (0.1.10)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kornia) (25.0)\r\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from kornia) (2.8.0+cu126)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.20.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (75.2.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.80)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (11.3.0.4)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (10.3.7.77)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (11.7.1.2)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.5.4.2)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (2.27.3)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.77)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (12.6.85)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (1.11.1.6)\r\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->kornia) (3.4.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->kornia) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->kornia) (3.0.3)\r\n",
      "Collecting kymatio\r\n",
      "  Downloading kymatio-0.3.0-py3-none-any.whl.metadata (9.6 kB)\r\n",
      "Collecting appdirs (from kymatio)\r\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\r\n",
      "Collecting configparser (from kymatio)\r\n",
      "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from kymatio) (2.0.2)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from kymatio) (25.0)\r\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from kymatio) (1.15.3)\r\n",
      "Downloading kymatio-0.3.0-py3-none-any.whl (87 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\r\n",
      "Downloading configparser-7.2.0-py3-none-any.whl (17 kB)\r\n",
      "Installing collected packages: appdirs, configparser, kymatio\r\n",
      "Successfully installed appdirs-1.4.4 configparser-7.2.0 kymatio-0.3.0\r\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms\n",
    "\n",
    "!pip install kornia\n",
    "!pip install kymatio\n",
    "from kornia import augmentation as K\n",
    "from kornia.augmentation import AugmentationSequential\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from kymatio.torch import Scattering2D\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "DEBUG = False\n",
    "MODEL_NAME = \"ScatResNet18_v1_100\"\n",
    "TRAIN_SIZE = 1000\n",
    "VALIDATION_SIZE = 5000\n",
    "env = 'kaggle' # 'kaggle' or 'colab'\n",
    "\n",
    "if env == 'colab':\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    base_dir = Path('/content/drive/MyDrive/dl_pj')    \n",
    "elif env == 'kaggle':\n",
    "    base_dir = Path('/kaggle/working/')\n",
    "\n",
    "checkpoint_dir = base_dir / 'checkpoints'\n",
    "checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "training_stats_dir = base_dir / 'stats'\n",
    "training_stats_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "Ls = [4, 6, 8, 10]\n",
    "N_TIMES = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "541bc283",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:08:07.459777Z",
     "iopub.status.busy": "2025-12-30T20:08:07.459318Z",
     "iopub.status.idle": "2025-12-30T20:08:07.470896Z",
     "shell.execute_reply": "2025-12-30T20:08:07.470334Z"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1766743054230,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "ElgGUpWbrrci",
    "papermill": {
     "duration": 0.016911,
     "end_time": "2025-12-30T20:08:07.472250",
     "exception": false,
     "start_time": "2025-12-30T20:08:07.455339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes,kernel_size=1,stride=stride,bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, L=8):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.L = L\n",
    "        self.scat_channels = (1 + L) * 3\n",
    "        self.conv1 = nn.Conv2d(3, 64 - self.scat_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.scat1 = Scattering2D(J=1, shape=(32, 32), L=L, max_order=2, backend='torch')\n",
    "        self.deconv1 = nn.ConvTranspose2d(in_channels=self.scat_channels, out_channels=self.scat_channels, groups=self.scat_channels, kernel_size=2, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layer = []\n",
    "        for s in strides:\n",
    "            layer.append(block(self.in_planes, planes, s))\n",
    "            self.in_planes = planes\n",
    "        return nn.Sequential(*layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        scat = self.scat1(x)\n",
    "        scat = scat.view(scat.size(0), -1, 16,16)\n",
    "        scat = self.deconv1(scat)\n",
    "        out = torch.cat((out, scat), dim=1)\n",
    "        out = F.relu(self.bn1(out))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ScatResNet18(L):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], L=L)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac8d408d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:08:07.478438Z",
     "iopub.status.busy": "2025-12-30T20:08:07.478203Z",
     "iopub.status.idle": "2025-12-30T20:08:08.064752Z",
     "shell.execute_reply": "2025-12-30T20:08:08.063852Z"
    },
    "papermill": {
     "duration": 0.591543,
     "end_time": "2025-12-30T20:08:08.066394",
     "exception": false,
     "start_time": "2025-12-30T20:08:07.474851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L 4 Total Parameters: 11,173,632\n",
      "L 4 Model Size: 42.62 MB\n",
      "L 6 Total Parameters: 11,173,500\n",
      "L 6 Model Size: 42.62 MB\n",
      "L 8 Total Parameters: 11,173,368\n",
      "L 8 Model Size: 42.62 MB\n",
      "L 10 Total Parameters: 11,173,236\n",
      "L 10 Model Size: 42.62 MB\n"
     ]
    }
   ],
   "source": [
    "def get_model_summary(model):\n",
    "    num_params = sum(p.numel() for p in model.parameters())\n",
    "    total_bytes = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    size_mb = total_bytes / (1024 ** 2)\n",
    "    return num_params, size_mb\n",
    "for L in Ls:\n",
    "    total_params, model_size_mb = get_model_summary(ScatResNet18(L))\n",
    "    print(f\"L {L} Total Parameters: {total_params:,}\")\n",
    "    print(f\"L {L} Model Size: {model_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e484494",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:08:08.073299Z",
     "iopub.status.busy": "2025-12-30T20:08:08.073039Z",
     "iopub.status.idle": "2025-12-30T20:08:08.077829Z",
     "shell.execute_reply": "2025-12-30T20:08:08.077248Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1766743013043,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "k3Z55XXW407W",
    "papermill": {
     "duration": 0.009796,
     "end_time": "2025-12-30T20:08:08.079123",
     "exception": false,
     "start_time": "2025-12-30T20:08:08.069327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, dataloader, device):\n",
    "    model.eval() # put in evaluation mode,  turn off Dropout, BatchNorm uses learned statistics\n",
    "    total_correct = 0\n",
    "    total_images = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            images = normalize(images)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.argmax(outputs, dim=-1)\n",
    "            total_images += labels.size(0)\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "\n",
    "    model_accuracy = total_correct / total_images * 100\n",
    "    return model_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753e97f",
   "metadata": {
    "id": "kJ7YrZBl6lho",
    "papermill": {
     "duration": 0.002812,
     "end_time": "2025-12-30T20:08:08.084851",
     "exception": false,
     "start_time": "2025-12-30T20:08:08.082039",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Split data set into train-validation-test.\n",
    "We are using 80% train, 20% validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5487410f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:08:08.091465Z",
     "iopub.status.busy": "2025-12-30T20:08:08.091207Z",
     "iopub.status.idle": "2025-12-30T20:08:23.313711Z",
     "shell.execute_reply": "2025-12-30T20:08:23.312912Z"
    },
    "executionInfo": {
     "elapsed": 1992,
     "status": "ok",
     "timestamp": 1766743016313,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "rkI3-MRt58hj",
    "papermill": {
     "duration": 15.227934,
     "end_time": "2025-12-30T20:08:23.315494",
     "exception": false,
     "start_time": "2025-12-30T20:08:08.087560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:10<00:00, 15.5MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 50000\n",
      "Train size: 1000\n",
      "Val size: 5000\n",
      "Samples per class Counter({np.int64(7): 100, np.int64(1): 100, np.int64(8): 100, np.int64(4): 100, np.int64(3): 100, np.int64(9): 100, np.int64(5): 100, np.int64(0): 100, np.int64(2): 100, np.int64(6): 100})\n",
      "Samples per class Counter({np.int64(1): 500, np.int64(9): 500, np.int64(2): 500, np.int64(4): 500, np.int64(6): 500, np.int64(5): 500, np.int64(0): 500, np.int64(3): 500, np.int64(8): 500, np.int64(7): 500})\n"
     ]
    }
   ],
   "source": [
    "transform = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "# 80/20% split\n",
    "train_val_set = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "targets = np.array(train_val_set.targets)\n",
    "\n",
    "indices = np.arange(len(targets))\n",
    "train_indices, remaining_indices = train_test_split(\n",
    "    indices,\n",
    "    train_size=TRAIN_SIZE,\n",
    "    stratify=targets,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "validation_indices, _ = train_test_split(\n",
    "    remaining_indices,\n",
    "    train_size=VALIDATION_SIZE,\n",
    "    stratify=targets[remaining_indices],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "trainset = Subset(train_val_set, train_indices)\n",
    "valset = Subset(train_val_set, validation_indices)\n",
    "print(f\"Original size: {len(train_val_set)}\")\n",
    "print(f\"Train size: {len(trainset)}\")\n",
    "print(f\"Val size: {len(valset)}\")\n",
    "\n",
    "from collections import Counter \n",
    "subset_labels = [targets[i] for i in train_indices]\n",
    "print(\"Samples per class\", Counter(subset_labels))\n",
    "subset_labels = [targets[i] for i in validation_indices]\n",
    "print(\"Samples per class\", Counter(subset_labels))\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b92f7bb8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:08:23.327117Z",
     "iopub.status.busy": "2025-12-30T20:08:23.326537Z",
     "iopub.status.idle": "2025-12-30T20:08:23.330284Z",
     "shell.execute_reply": "2025-12-30T20:08:23.329720Z"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1766743919319,
     "user": {
      "displayName": "Gilad Navok",
      "userId": "06092819627906668279"
     },
     "user_tz": -120
    },
    "id": "Zy87RRrf7Vh6",
    "papermill": {
     "duration": 0.010903,
     "end_time": "2025-12-30T20:08:23.331708",
     "exception": false,
     "start_time": "2025-12-30T20:08:23.320805",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hyperparamters\n",
    "batch_size = 128\n",
    "\n",
    "lr = 0.1\n",
    "momentum = 0.9\n",
    "weight_decay = 5e-4\n",
    "\n",
    "T_max = 200\n",
    "\n",
    "n_epochs = 1 if DEBUG else 200\n",
    "\n",
    "print_progress_every = 1\n",
    "val_accuracy_storing_threshold = 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83e89e69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T20:08:23.342587Z",
     "iopub.status.busy": "2025-12-30T20:08:23.342001Z",
     "iopub.status.idle": "2025-12-30T23:15:34.853824Z",
     "shell.execute_reply": "2025-12-30T23:15:34.853099Z"
    },
    "id": "W45Vot2zvrLE",
    "papermill": {
     "duration": 11231.518861,
     "end_time": "2025-12-30T23:15:34.855326",
     "exception": false,
     "start_time": "2025-12-30T20:08:23.336465",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L 4 Epoch 0 Loss 4.483 Val Acc 10.000\n",
      "L 4 Epoch 1 Loss 4.255 Val Acc 10.020\n",
      "L 4 Epoch 2 Loss 3.398 Val Acc 10.180\n",
      "L 4 Epoch 3 Loss 2.876 Val Acc 11.360\n",
      "L 4 Epoch 4 Loss 2.877 Val Acc 14.760\n",
      "L 4 Epoch 5 Loss 3.047 Val Acc 20.020\n",
      "L 4 Epoch 6 Loss 2.600 Val Acc 21.460\n",
      "L 4 Epoch 7 Loss 2.300 Val Acc 23.340\n",
      "L 4 Epoch 8 Loss 2.132 Val Acc 24.640\n",
      "L 4 Epoch 9 Loss 2.095 Val Acc 26.920\n",
      "L 4 Epoch 10 Loss 2.058 Val Acc 25.320\n",
      "L 4 Epoch 11 Loss 1.919 Val Acc 28.240\n",
      "L 4 Epoch 12 Loss 1.836 Val Acc 29.880\n",
      "L 4 Epoch 13 Loss 1.784 Val Acc 29.440\n",
      "L 4 Epoch 14 Loss 1.746 Val Acc 30.780\n",
      "L 4 Epoch 15 Loss 1.673 Val Acc 35.920\n",
      "L 4 Epoch 16 Loss 1.628 Val Acc 37.040\n",
      "L 4 Epoch 17 Loss 1.549 Val Acc 37.240\n",
      "L 4 Epoch 18 Loss 1.596 Val Acc 37.600\n",
      "L 4 Epoch 19 Loss 1.503 Val Acc 39.100\n",
      "L 4 Epoch 20 Loss 1.530 Val Acc 37.400\n",
      "L 4 Epoch 21 Loss 1.477 Val Acc 36.760\n",
      "L 4 Epoch 22 Loss 1.517 Val Acc 37.400\n",
      "==> Saving model ...\n",
      "L 4 Epoch 23 Loss 1.467 Val Acc 40.740\n",
      "==> Saving model ...\n",
      "L 4 Epoch 24 Loss 1.404 Val Acc 41.740\n",
      "L 4 Epoch 25 Loss 1.381 Val Acc 40.540\n",
      "L 4 Epoch 26 Loss 1.343 Val Acc 39.600\n",
      "==> Saving model ...\n",
      "L 4 Epoch 27 Loss 1.261 Val Acc 42.700\n",
      "==> Saving model ...\n",
      "L 4 Epoch 28 Loss 1.232 Val Acc 44.420\n",
      "L 4 Epoch 29 Loss 1.251 Val Acc 44.380\n",
      "L 4 Epoch 30 Loss 1.212 Val Acc 42.680\n",
      "L 4 Epoch 31 Loss 1.200 Val Acc 43.320\n",
      "L 4 Epoch 32 Loss 1.069 Val Acc 42.500\n",
      "L 4 Epoch 33 Loss 1.093 Val Acc 43.320\n",
      "L 4 Epoch 34 Loss 1.097 Val Acc 42.460\n",
      "==> Saving model ...\n",
      "L 4 Epoch 35 Loss 1.061 Val Acc 44.440\n",
      "L 4 Epoch 36 Loss 1.061 Val Acc 42.240\n",
      "==> Saving model ...\n",
      "L 4 Epoch 37 Loss 0.901 Val Acc 47.200\n",
      "L 4 Epoch 38 Loss 0.833 Val Acc 46.020\n",
      "L 4 Epoch 39 Loss 0.947 Val Acc 43.400\n",
      "L 4 Epoch 40 Loss 1.053 Val Acc 44.940\n",
      "L 4 Epoch 41 Loss 0.954 Val Acc 45.820\n",
      "==> Saving model ...\n",
      "L 4 Epoch 42 Loss 0.806 Val Acc 47.460\n",
      "==> Saving model ...\n",
      "L 4 Epoch 43 Loss 0.827 Val Acc 47.600\n",
      "L 4 Epoch 44 Loss 0.735 Val Acc 45.280\n",
      "==> Saving model ...\n",
      "L 4 Epoch 45 Loss 0.772 Val Acc 47.900\n",
      "L 4 Epoch 46 Loss 0.736 Val Acc 43.980\n",
      "L 4 Epoch 47 Loss 0.587 Val Acc 46.740\n",
      "L 4 Epoch 48 Loss 0.533 Val Acc 47.660\n",
      "L 4 Epoch 49 Loss 0.726 Val Acc 45.920\n",
      "L 4 Epoch 50 Loss 0.680 Val Acc 44.900\n",
      "L 4 Epoch 51 Loss 0.617 Val Acc 47.340\n",
      "==> Saving model ...\n",
      "L 4 Epoch 52 Loss 0.412 Val Acc 49.240\n",
      "==> Saving model ...\n",
      "L 4 Epoch 53 Loss 0.466 Val Acc 49.740\n",
      "==> Saving model ...\n",
      "L 4 Epoch 54 Loss 0.395 Val Acc 50.560\n",
      "L 4 Epoch 55 Loss 0.519 Val Acc 47.300\n",
      "L 4 Epoch 56 Loss 0.546 Val Acc 47.720\n",
      "L 4 Epoch 57 Loss 0.494 Val Acc 47.600\n",
      "L 4 Epoch 58 Loss 0.371 Val Acc 49.520\n",
      "L 4 Epoch 59 Loss 0.343 Val Acc 48.740\n",
      "L 4 Epoch 60 Loss 0.404 Val Acc 50.100\n",
      "L 4 Epoch 61 Loss 0.404 Val Acc 47.320\n",
      "L 4 Epoch 62 Loss 0.357 Val Acc 49.800\n",
      "==> Saving model ...\n",
      "L 4 Epoch 63 Loss 0.383 Val Acc 51.040\n",
      "==> Saving model ...\n",
      "L 4 Epoch 64 Loss 0.398 Val Acc 52.180\n",
      "L 4 Epoch 65 Loss 0.335 Val Acc 46.700\n",
      "L 4 Epoch 66 Loss 0.279 Val Acc 51.340\n",
      "L 4 Epoch 67 Loss 0.287 Val Acc 50.380\n",
      "L 4 Epoch 68 Loss 0.278 Val Acc 51.120\n",
      "L 4 Epoch 69 Loss 0.182 Val Acc 52.020\n",
      "L 4 Epoch 70 Loss 0.238 Val Acc 51.260\n",
      "L 4 Epoch 71 Loss 0.304 Val Acc 47.880\n",
      "L 4 Epoch 72 Loss 0.233 Val Acc 49.740\n",
      "L 4 Epoch 73 Loss 0.389 Val Acc 50.300\n",
      "L 4 Epoch 74 Loss 0.225 Val Acc 50.480\n",
      "L 4 Epoch 75 Loss 0.168 Val Acc 50.220\n",
      "==> Saving model ...\n",
      "L 4 Epoch 76 Loss 0.106 Val Acc 52.220\n",
      "==> Saving model ...\n",
      "L 4 Epoch 77 Loss 0.135 Val Acc 52.540\n",
      "L 4 Epoch 78 Loss 0.194 Val Acc 51.880\n",
      "L 4 Epoch 79 Loss 0.180 Val Acc 50.460\n",
      "L 4 Epoch 80 Loss 0.191 Val Acc 48.980\n",
      "L 4 Epoch 81 Loss 0.239 Val Acc 51.780\n",
      "L 4 Epoch 82 Loss 0.207 Val Acc 51.420\n",
      "==> Saving model ...\n",
      "L 4 Epoch 83 Loss 0.150 Val Acc 52.660\n",
      "L 4 Epoch 84 Loss 0.163 Val Acc 51.020\n",
      "L 4 Epoch 85 Loss 0.086 Val Acc 52.240\n",
      "L 4 Epoch 86 Loss 0.115 Val Acc 51.460\n",
      "==> Saving model ...\n",
      "L 4 Epoch 87 Loss 0.098 Val Acc 52.760\n",
      "L 4 Epoch 88 Loss 0.175 Val Acc 51.300\n",
      "==> Saving model ...\n",
      "L 4 Epoch 89 Loss 0.155 Val Acc 53.480\n",
      "L 4 Epoch 90 Loss 0.123 Val Acc 53.100\n",
      "L 4 Epoch 91 Loss 0.128 Val Acc 52.560\n",
      "L 4 Epoch 92 Loss 0.123 Val Acc 52.620\n",
      "==> Saving model ...\n",
      "L 4 Epoch 93 Loss 0.114 Val Acc 53.760\n",
      "L 4 Epoch 94 Loss 0.079 Val Acc 53.020\n",
      "L 4 Epoch 95 Loss 0.122 Val Acc 53.620\n",
      "L 4 Epoch 96 Loss 0.089 Val Acc 52.220\n",
      "L 4 Epoch 97 Loss 0.079 Val Acc 51.940\n",
      "L 4 Epoch 98 Loss 0.077 Val Acc 53.040\n",
      "L 4 Epoch 99 Loss 0.061 Val Acc 53.100\n",
      "==> Saving model ...\n",
      "L 4 Epoch 100 Loss 0.073 Val Acc 54.820\n",
      "L 4 Epoch 101 Loss 0.086 Val Acc 54.020\n",
      "L 4 Epoch 102 Loss 0.046 Val Acc 53.420\n",
      "L 4 Epoch 103 Loss 0.088 Val Acc 52.060\n",
      "L 4 Epoch 104 Loss 0.066 Val Acc 52.820\n",
      "L 4 Epoch 105 Loss 0.072 Val Acc 53.600\n",
      "L 4 Epoch 106 Loss 0.081 Val Acc 53.960\n",
      "L 4 Epoch 107 Loss 0.075 Val Acc 53.560\n",
      "==> Saving model ...\n",
      "L 4 Epoch 108 Loss 0.087 Val Acc 54.880\n",
      "L 4 Epoch 109 Loss 0.053 Val Acc 54.260\n",
      "L 4 Epoch 110 Loss 0.040 Val Acc 53.680\n",
      "L 4 Epoch 111 Loss 0.082 Val Acc 54.520\n",
      "L 4 Epoch 112 Loss 0.052 Val Acc 54.660\n",
      "L 4 Epoch 113 Loss 0.043 Val Acc 53.740\n",
      "L 4 Epoch 114 Loss 0.074 Val Acc 54.560\n",
      "L 4 Epoch 115 Loss 0.038 Val Acc 54.760\n",
      "==> Saving model ...\n",
      "L 4 Epoch 116 Loss 0.045 Val Acc 54.960\n",
      "L 4 Epoch 117 Loss 0.042 Val Acc 54.820\n",
      "==> Saving model ...\n",
      "L 4 Epoch 118 Loss 0.060 Val Acc 55.640\n",
      "L 4 Epoch 119 Loss 0.048 Val Acc 54.880\n",
      "L 4 Epoch 120 Loss 0.051 Val Acc 54.680\n",
      "L 4 Epoch 121 Loss 0.033 Val Acc 54.660\n",
      "L 4 Epoch 122 Loss 0.052 Val Acc 53.400\n",
      "L 4 Epoch 123 Loss 0.045 Val Acc 53.200\n",
      "L 4 Epoch 124 Loss 0.033 Val Acc 54.180\n",
      "L 4 Epoch 125 Loss 0.030 Val Acc 54.560\n",
      "L 4 Epoch 126 Loss 0.059 Val Acc 54.380\n",
      "L 4 Epoch 127 Loss 0.034 Val Acc 54.240\n",
      "L 4 Epoch 128 Loss 0.034 Val Acc 54.420\n",
      "L 4 Epoch 129 Loss 0.039 Val Acc 55.120\n",
      "L 4 Epoch 130 Loss 0.013 Val Acc 55.360\n",
      "L 4 Epoch 131 Loss 0.028 Val Acc 55.340\n",
      "L 4 Epoch 132 Loss 0.014 Val Acc 54.940\n",
      "L 4 Epoch 133 Loss 0.043 Val Acc 54.160\n",
      "L 4 Epoch 134 Loss 0.013 Val Acc 54.400\n",
      "L 4 Epoch 135 Loss 0.035 Val Acc 55.080\n",
      "L 4 Epoch 136 Loss 0.025 Val Acc 55.300\n",
      "L 4 Epoch 137 Loss 0.017 Val Acc 55.600\n",
      "L 4 Epoch 138 Loss 0.024 Val Acc 55.600\n",
      "L 4 Epoch 139 Loss 0.049 Val Acc 55.160\n",
      "L 4 Epoch 140 Loss 0.020 Val Acc 55.000\n",
      "L 4 Epoch 141 Loss 0.028 Val Acc 54.600\n",
      "L 4 Epoch 142 Loss 0.020 Val Acc 54.880\n",
      "L 4 Epoch 143 Loss 0.007 Val Acc 55.580\n",
      "L 4 Epoch 144 Loss 0.026 Val Acc 54.360\n",
      "L 4 Epoch 145 Loss 0.020 Val Acc 54.480\n",
      "L 4 Epoch 146 Loss 0.013 Val Acc 54.780\n",
      "L 4 Epoch 147 Loss 0.025 Val Acc 54.840\n",
      "L 4 Epoch 148 Loss 0.013 Val Acc 55.160\n",
      "L 4 Epoch 149 Loss 0.028 Val Acc 54.960\n",
      "L 4 Epoch 150 Loss 0.011 Val Acc 55.060\n",
      "L 4 Epoch 151 Loss 0.016 Val Acc 55.320\n",
      "==> Saving model ...\n",
      "L 4 Epoch 152 Loss 0.007 Val Acc 55.900\n",
      "==> Saving model ...\n",
      "L 4 Epoch 153 Loss 0.022 Val Acc 56.200\n",
      "==> Saving model ...\n",
      "L 4 Epoch 154 Loss 0.011 Val Acc 56.360\n",
      "==> Saving model ...\n",
      "L 4 Epoch 155 Loss 0.017 Val Acc 56.580\n",
      "L 4 Epoch 156 Loss 0.008 Val Acc 56.560\n",
      "==> Saving model ...\n",
      "L 4 Epoch 157 Loss 0.006 Val Acc 56.600\n",
      "L 4 Epoch 158 Loss 0.015 Val Acc 56.280\n",
      "==> Saving model ...\n",
      "L 4 Epoch 159 Loss 0.009 Val Acc 56.680\n",
      "L 4 Epoch 160 Loss 0.019 Val Acc 55.940\n",
      "L 4 Epoch 161 Loss 0.009 Val Acc 55.960\n",
      "L 4 Epoch 162 Loss 0.011 Val Acc 55.720\n",
      "L 4 Epoch 163 Loss 0.025 Val Acc 54.900\n",
      "L 4 Epoch 164 Loss 0.013 Val Acc 55.280\n",
      "L 4 Epoch 165 Loss 0.018 Val Acc 55.520\n",
      "L 4 Epoch 166 Loss 0.020 Val Acc 55.600\n",
      "L 4 Epoch 167 Loss 0.008 Val Acc 55.860\n",
      "L 4 Epoch 168 Loss 0.016 Val Acc 56.260\n",
      "L 4 Epoch 169 Loss 0.010 Val Acc 56.140\n",
      "L 4 Epoch 170 Loss 0.014 Val Acc 56.180\n",
      "L 4 Epoch 171 Loss 0.019 Val Acc 55.680\n",
      "L 4 Epoch 172 Loss 0.018 Val Acc 55.800\n",
      "L 4 Epoch 173 Loss 0.016 Val Acc 55.600\n",
      "L 4 Epoch 174 Loss 0.007 Val Acc 56.060\n",
      "L 4 Epoch 175 Loss 0.011 Val Acc 56.000\n",
      "L 4 Epoch 176 Loss 0.019 Val Acc 56.000\n",
      "L 4 Epoch 177 Loss 0.013 Val Acc 55.900\n",
      "L 4 Epoch 178 Loss 0.017 Val Acc 55.560\n",
      "L 4 Epoch 179 Loss 0.015 Val Acc 55.840\n",
      "L 4 Epoch 180 Loss 0.026 Val Acc 55.940\n",
      "L 4 Epoch 181 Loss 0.007 Val Acc 55.980\n",
      "L 4 Epoch 182 Loss 0.007 Val Acc 56.060\n",
      "L 4 Epoch 183 Loss 0.015 Val Acc 55.680\n",
      "L 4 Epoch 184 Loss 0.011 Val Acc 56.060\n",
      "L 4 Epoch 185 Loss 0.021 Val Acc 55.840\n",
      "L 4 Epoch 186 Loss 0.007 Val Acc 56.000\n",
      "L 4 Epoch 187 Loss 0.020 Val Acc 55.600\n",
      "L 4 Epoch 188 Loss 0.011 Val Acc 55.860\n",
      "L 4 Epoch 189 Loss 0.013 Val Acc 55.540\n",
      "L 4 Epoch 190 Loss 0.012 Val Acc 55.400\n",
      "L 4 Epoch 191 Loss 0.011 Val Acc 55.800\n",
      "L 4 Epoch 192 Loss 0.007 Val Acc 56.160\n",
      "L 4 Epoch 193 Loss 0.009 Val Acc 56.440\n",
      "L 4 Epoch 194 Loss 0.006 Val Acc 56.400\n",
      "L 4 Epoch 195 Loss 0.019 Val Acc 56.140\n",
      "L 4 Epoch 196 Loss 0.007 Val Acc 56.300\n",
      "L 4 Epoch 197 Loss 0.015 Val Acc 56.020\n",
      "L 4 Epoch 198 Loss 0.013 Val Acc 55.960\n",
      "L 4 Epoch 199 Loss 0.009 Val Acc 56.120\n",
      "L 6 Epoch 0 Loss 4.247 Val Acc 10.000\n",
      "L 6 Epoch 1 Loss 2.413 Val Acc 11.640\n",
      "L 6 Epoch 2 Loss 2.082 Val Acc 15.840\n",
      "L 6 Epoch 3 Loss 1.999 Val Acc 18.340\n",
      "L 6 Epoch 4 Loss 1.823 Val Acc 16.980\n",
      "L 6 Epoch 5 Loss 1.804 Val Acc 19.560\n",
      "L 6 Epoch 6 Loss 1.742 Val Acc 24.960\n",
      "L 6 Epoch 7 Loss 1.673 Val Acc 33.100\n",
      "L 6 Epoch 8 Loss 1.614 Val Acc 37.760\n",
      "L 6 Epoch 9 Loss 1.587 Val Acc 37.160\n",
      "L 6 Epoch 10 Loss 1.540 Val Acc 38.460\n",
      "L 6 Epoch 11 Loss 1.488 Val Acc 33.940\n",
      "L 6 Epoch 12 Loss 1.455 Val Acc 39.500\n",
      "==> Saving model ...\n",
      "L 6 Epoch 13 Loss 1.469 Val Acc 40.960\n",
      "L 6 Epoch 14 Loss 1.379 Val Acc 38.140\n",
      "==> Saving model ...\n",
      "L 6 Epoch 15 Loss 1.391 Val Acc 42.240\n",
      "L 6 Epoch 16 Loss 1.290 Val Acc 40.720\n",
      "L 6 Epoch 17 Loss 1.228 Val Acc 41.040\n",
      "L 6 Epoch 18 Loss 1.278 Val Acc 42.200\n",
      "==> Saving model ...\n",
      "L 6 Epoch 19 Loss 1.227 Val Acc 42.580\n",
      "==> Saving model ...\n",
      "L 6 Epoch 20 Loss 1.181 Val Acc 43.340\n",
      "==> Saving model ...\n",
      "L 6 Epoch 21 Loss 1.201 Val Acc 45.940\n",
      "L 6 Epoch 22 Loss 1.125 Val Acc 42.820\n",
      "L 6 Epoch 23 Loss 1.098 Val Acc 44.460\n",
      "L 6 Epoch 24 Loss 1.071 Val Acc 45.540\n",
      "L 6 Epoch 25 Loss 1.143 Val Acc 44.820\n",
      "==> Saving model ...\n",
      "L 6 Epoch 26 Loss 1.016 Val Acc 48.220\n",
      "L 6 Epoch 27 Loss 1.000 Val Acc 42.000\n",
      "L 6 Epoch 28 Loss 0.908 Val Acc 47.000\n",
      "==> Saving model ...\n",
      "L 6 Epoch 29 Loss 0.907 Val Acc 48.260\n",
      "L 6 Epoch 30 Loss 0.908 Val Acc 44.660\n",
      "L 6 Epoch 31 Loss 0.868 Val Acc 43.000\n",
      "L 6 Epoch 32 Loss 0.701 Val Acc 48.120\n",
      "==> Saving model ...\n",
      "L 6 Epoch 33 Loss 0.756 Val Acc 48.380\n",
      "L 6 Epoch 34 Loss 0.689 Val Acc 45.940\n",
      "L 6 Epoch 35 Loss 0.621 Val Acc 46.260\n",
      "==> Saving model ...\n",
      "L 6 Epoch 36 Loss 0.717 Val Acc 50.380\n",
      "==> Saving model ...\n",
      "L 6 Epoch 37 Loss 0.548 Val Acc 51.360\n",
      "L 6 Epoch 38 Loss 0.632 Val Acc 48.160\n",
      "L 6 Epoch 39 Loss 0.485 Val Acc 49.520\n",
      "L 6 Epoch 40 Loss 0.523 Val Acc 47.980\n",
      "L 6 Epoch 41 Loss 0.517 Val Acc 46.240\n",
      "L 6 Epoch 42 Loss 0.446 Val Acc 48.500\n",
      "L 6 Epoch 43 Loss 0.387 Val Acc 50.460\n",
      "==> Saving model ...\n",
      "L 6 Epoch 44 Loss 0.357 Val Acc 52.980\n",
      "L 6 Epoch 45 Loss 0.353 Val Acc 48.540\n",
      "L 6 Epoch 46 Loss 0.475 Val Acc 46.000\n",
      "L 6 Epoch 47 Loss 0.439 Val Acc 48.100\n",
      "L 6 Epoch 48 Loss 0.492 Val Acc 46.160\n",
      "L 6 Epoch 49 Loss 0.483 Val Acc 46.660\n",
      "L 6 Epoch 50 Loss 0.361 Val Acc 51.800\n",
      "L 6 Epoch 51 Loss 0.375 Val Acc 52.360\n",
      "L 6 Epoch 52 Loss 0.270 Val Acc 52.860\n",
      "L 6 Epoch 53 Loss 0.230 Val Acc 49.360\n",
      "L 6 Epoch 54 Loss 0.349 Val Acc 48.920\n",
      "L 6 Epoch 55 Loss 0.271 Val Acc 51.460\n",
      "L 6 Epoch 56 Loss 0.151 Val Acc 52.660\n",
      "L 6 Epoch 57 Loss 0.221 Val Acc 52.500\n",
      "L 6 Epoch 58 Loss 0.234 Val Acc 52.660\n",
      "L 6 Epoch 59 Loss 0.159 Val Acc 50.980\n",
      "L 6 Epoch 60 Loss 0.264 Val Acc 51.940\n",
      "==> Saving model ...\n",
      "L 6 Epoch 61 Loss 0.171 Val Acc 53.160\n",
      "L 6 Epoch 62 Loss 0.263 Val Acc 50.780\n",
      "L 6 Epoch 63 Loss 0.233 Val Acc 52.840\n",
      "L 6 Epoch 64 Loss 0.229 Val Acc 52.680\n",
      "L 6 Epoch 65 Loss 0.277 Val Acc 51.160\n",
      "L 6 Epoch 66 Loss 0.173 Val Acc 53.080\n",
      "==> Saving model ...\n",
      "L 6 Epoch 67 Loss 0.254 Val Acc 53.660\n",
      "L 6 Epoch 68 Loss 0.205 Val Acc 51.000\n",
      "L 6 Epoch 69 Loss 0.216 Val Acc 51.300\n",
      "L 6 Epoch 70 Loss 0.101 Val Acc 53.660\n",
      "==> Saving model ...\n",
      "L 6 Epoch 71 Loss 0.128 Val Acc 54.320\n",
      "L 6 Epoch 72 Loss 0.133 Val Acc 52.900\n",
      "==> Saving model ...\n",
      "L 6 Epoch 73 Loss 0.095 Val Acc 54.400\n",
      "==> Saving model ...\n",
      "L 6 Epoch 74 Loss 0.088 Val Acc 55.040\n",
      "L 6 Epoch 75 Loss 0.127 Val Acc 53.540\n",
      "==> Saving model ...\n",
      "L 6 Epoch 76 Loss 0.147 Val Acc 56.000\n",
      "L 6 Epoch 77 Loss 0.154 Val Acc 52.660\n",
      "L 6 Epoch 78 Loss 0.075 Val Acc 52.840\n",
      "L 6 Epoch 79 Loss 0.099 Val Acc 54.060\n",
      "L 6 Epoch 80 Loss 0.069 Val Acc 54.620\n",
      "L 6 Epoch 81 Loss 0.108 Val Acc 55.040\n",
      "L 6 Epoch 82 Loss 0.152 Val Acc 53.160\n",
      "L 6 Epoch 83 Loss 0.088 Val Acc 52.560\n",
      "L 6 Epoch 84 Loss 0.096 Val Acc 53.920\n",
      "L 6 Epoch 85 Loss 0.078 Val Acc 55.880\n",
      "L 6 Epoch 86 Loss 0.094 Val Acc 54.560\n",
      "L 6 Epoch 87 Loss 0.069 Val Acc 54.020\n",
      "L 6 Epoch 88 Loss 0.078 Val Acc 55.400\n",
      "L 6 Epoch 89 Loss 0.103 Val Acc 55.540\n",
      "L 6 Epoch 90 Loss 0.065 Val Acc 54.920\n",
      "L 6 Epoch 91 Loss 0.065 Val Acc 54.460\n",
      "L 6 Epoch 92 Loss 0.076 Val Acc 54.960\n",
      "==> Saving model ...\n",
      "L 6 Epoch 93 Loss 0.100 Val Acc 56.120\n",
      "L 6 Epoch 94 Loss 0.073 Val Acc 55.080\n",
      "L 6 Epoch 95 Loss 0.063 Val Acc 55.300\n",
      "L 6 Epoch 96 Loss 0.076 Val Acc 53.040\n",
      "L 6 Epoch 97 Loss 0.095 Val Acc 53.920\n",
      "L 6 Epoch 98 Loss 0.065 Val Acc 54.480\n",
      "L 6 Epoch 99 Loss 0.078 Val Acc 54.740\n",
      "L 6 Epoch 100 Loss 0.035 Val Acc 54.720\n",
      "L 6 Epoch 101 Loss 0.052 Val Acc 55.600\n",
      "L 6 Epoch 102 Loss 0.067 Val Acc 53.840\n",
      "L 6 Epoch 103 Loss 0.041 Val Acc 55.460\n",
      "L 6 Epoch 104 Loss 0.075 Val Acc 55.380\n",
      "L 6 Epoch 105 Loss 0.076 Val Acc 55.680\n",
      "L 6 Epoch 106 Loss 0.070 Val Acc 53.840\n",
      "L 6 Epoch 107 Loss 0.065 Val Acc 55.340\n",
      "==> Saving model ...\n",
      "L 6 Epoch 108 Loss 0.063 Val Acc 56.260\n",
      "L 6 Epoch 109 Loss 0.020 Val Acc 55.480\n",
      "==> Saving model ...\n",
      "L 6 Epoch 110 Loss 0.087 Val Acc 57.160\n",
      "L 6 Epoch 111 Loss 0.052 Val Acc 55.920\n",
      "L 6 Epoch 112 Loss 0.034 Val Acc 55.920\n",
      "L 6 Epoch 113 Loss 0.046 Val Acc 56.160\n",
      "L 6 Epoch 114 Loss 0.040 Val Acc 56.360\n",
      "L 6 Epoch 115 Loss 0.027 Val Acc 55.500\n",
      "L 6 Epoch 116 Loss 0.043 Val Acc 55.880\n",
      "L 6 Epoch 117 Loss 0.023 Val Acc 55.860\n",
      "L 6 Epoch 118 Loss 0.021 Val Acc 56.880\n",
      "L 6 Epoch 119 Loss 0.030 Val Acc 56.580\n",
      "L 6 Epoch 120 Loss 0.032 Val Acc 56.320\n",
      "L 6 Epoch 121 Loss 0.026 Val Acc 55.780\n",
      "L 6 Epoch 122 Loss 0.040 Val Acc 55.840\n",
      "L 6 Epoch 123 Loss 0.014 Val Acc 56.840\n",
      "==> Saving model ...\n",
      "L 6 Epoch 124 Loss 0.015 Val Acc 57.240\n",
      "==> Saving model ...\n",
      "L 6 Epoch 125 Loss 0.023 Val Acc 57.920\n",
      "L 6 Epoch 126 Loss 0.037 Val Acc 57.480\n",
      "L 6 Epoch 127 Loss 0.019 Val Acc 57.740\n",
      "==> Saving model ...\n",
      "L 6 Epoch 128 Loss 0.010 Val Acc 58.160\n",
      "==> Saving model ...\n",
      "L 6 Epoch 129 Loss 0.003 Val Acc 58.660\n",
      "L 6 Epoch 130 Loss 0.031 Val Acc 57.880\n",
      "L 6 Epoch 131 Loss 0.018 Val Acc 57.220\n",
      "L 6 Epoch 132 Loss 0.012 Val Acc 56.780\n",
      "L 6 Epoch 133 Loss 0.021 Val Acc 57.560\n",
      "L 6 Epoch 134 Loss 0.016 Val Acc 58.100\n",
      "L 6 Epoch 135 Loss 0.017 Val Acc 57.780\n",
      "L 6 Epoch 136 Loss 0.027 Val Acc 57.400\n",
      "L 6 Epoch 137 Loss 0.028 Val Acc 57.540\n",
      "L 6 Epoch 138 Loss 0.020 Val Acc 57.860\n",
      "L 6 Epoch 139 Loss 0.018 Val Acc 58.020\n",
      "L 6 Epoch 140 Loss 0.018 Val Acc 57.620\n",
      "L 6 Epoch 141 Loss 0.017 Val Acc 57.540\n",
      "L 6 Epoch 142 Loss 0.010 Val Acc 57.920\n",
      "L 6 Epoch 143 Loss 0.015 Val Acc 57.920\n",
      "L 6 Epoch 144 Loss 0.014 Val Acc 58.040\n",
      "L 6 Epoch 145 Loss 0.013 Val Acc 58.180\n",
      "L 6 Epoch 146 Loss 0.024 Val Acc 58.060\n",
      "L 6 Epoch 147 Loss 0.006 Val Acc 58.260\n",
      "L 6 Epoch 148 Loss 0.003 Val Acc 58.580\n",
      "==> Saving model ...\n",
      "L 6 Epoch 149 Loss 0.012 Val Acc 58.680\n",
      "==> Saving model ...\n",
      "L 6 Epoch 150 Loss 0.011 Val Acc 58.780\n",
      "==> Saving model ...\n",
      "L 6 Epoch 151 Loss 0.012 Val Acc 58.980\n",
      "L 6 Epoch 152 Loss 0.019 Val Acc 58.440\n",
      "L 6 Epoch 153 Loss 0.016 Val Acc 58.360\n",
      "L 6 Epoch 154 Loss 0.010 Val Acc 58.400\n",
      "L 6 Epoch 155 Loss 0.017 Val Acc 57.880\n",
      "L 6 Epoch 156 Loss 0.010 Val Acc 58.420\n",
      "L 6 Epoch 157 Loss 0.015 Val Acc 58.120\n",
      "L 6 Epoch 158 Loss 0.008 Val Acc 58.100\n",
      "L 6 Epoch 159 Loss 0.016 Val Acc 57.800\n",
      "L 6 Epoch 160 Loss 0.007 Val Acc 58.220\n",
      "L 6 Epoch 161 Loss 0.014 Val Acc 58.080\n",
      "L 6 Epoch 162 Loss 0.008 Val Acc 58.080\n",
      "L 6 Epoch 163 Loss 0.007 Val Acc 58.340\n",
      "L 6 Epoch 164 Loss 0.025 Val Acc 57.480\n",
      "L 6 Epoch 165 Loss 0.009 Val Acc 58.120\n",
      "L 6 Epoch 166 Loss 0.008 Val Acc 58.540\n",
      "L 6 Epoch 167 Loss 0.020 Val Acc 58.100\n",
      "L 6 Epoch 168 Loss 0.009 Val Acc 58.380\n",
      "L 6 Epoch 169 Loss 0.016 Val Acc 57.920\n",
      "L 6 Epoch 170 Loss 0.019 Val Acc 58.160\n",
      "L 6 Epoch 171 Loss 0.008 Val Acc 58.060\n",
      "L 6 Epoch 172 Loss 0.017 Val Acc 57.980\n",
      "L 6 Epoch 173 Loss 0.010 Val Acc 58.460\n",
      "L 6 Epoch 174 Loss 0.008 Val Acc 58.200\n",
      "L 6 Epoch 175 Loss 0.009 Val Acc 58.320\n",
      "L 6 Epoch 176 Loss 0.012 Val Acc 58.380\n",
      "L 6 Epoch 177 Loss 0.008 Val Acc 58.360\n",
      "L 6 Epoch 178 Loss 0.019 Val Acc 58.360\n",
      "L 6 Epoch 179 Loss 0.008 Val Acc 58.500\n",
      "L 6 Epoch 180 Loss 0.013 Val Acc 58.340\n",
      "L 6 Epoch 181 Loss 0.019 Val Acc 58.160\n",
      "L 6 Epoch 182 Loss 0.010 Val Acc 58.260\n",
      "L 6 Epoch 183 Loss 0.012 Val Acc 58.320\n",
      "L 6 Epoch 184 Loss 0.006 Val Acc 58.360\n",
      "L 6 Epoch 185 Loss 0.007 Val Acc 58.660\n",
      "L 6 Epoch 186 Loss 0.008 Val Acc 58.600\n",
      "L 6 Epoch 187 Loss 0.019 Val Acc 58.580\n",
      "L 6 Epoch 188 Loss 0.011 Val Acc 58.320\n",
      "L 6 Epoch 189 Loss 0.007 Val Acc 58.340\n",
      "L 6 Epoch 190 Loss 0.018 Val Acc 58.160\n",
      "L 6 Epoch 191 Loss 0.017 Val Acc 58.040\n",
      "L 6 Epoch 192 Loss 0.010 Val Acc 58.240\n",
      "L 6 Epoch 193 Loss 0.016 Val Acc 57.980\n",
      "L 6 Epoch 194 Loss 0.009 Val Acc 58.400\n",
      "L 6 Epoch 195 Loss 0.011 Val Acc 58.660\n",
      "L 6 Epoch 196 Loss 0.007 Val Acc 58.560\n",
      "L 6 Epoch 197 Loss 0.014 Val Acc 58.520\n",
      "L 6 Epoch 198 Loss 0.005 Val Acc 58.520\n",
      "L 6 Epoch 199 Loss 0.007 Val Acc 58.520\n",
      "L 8 Epoch 0 Loss 4.477 Val Acc 9.980\n",
      "L 8 Epoch 1 Loss 3.097 Val Acc 11.220\n",
      "L 8 Epoch 2 Loss 2.626 Val Acc 9.500\n",
      "L 8 Epoch 3 Loss 2.351 Val Acc 10.160\n",
      "L 8 Epoch 4 Loss 2.145 Val Acc 18.140\n",
      "L 8 Epoch 5 Loss 1.953 Val Acc 22.980\n",
      "L 8 Epoch 6 Loss 1.860 Val Acc 25.200\n",
      "L 8 Epoch 7 Loss 1.818 Val Acc 26.260\n",
      "L 8 Epoch 8 Loss 1.719 Val Acc 29.960\n",
      "L 8 Epoch 9 Loss 1.689 Val Acc 30.560\n",
      "L 8 Epoch 10 Loss 1.674 Val Acc 30.840\n",
      "L 8 Epoch 11 Loss 1.631 Val Acc 34.580\n",
      "L 8 Epoch 12 Loss 1.577 Val Acc 36.180\n",
      "L 8 Epoch 13 Loss 1.564 Val Acc 35.300\n",
      "L 8 Epoch 14 Loss 1.557 Val Acc 34.900\n",
      "L 8 Epoch 15 Loss 1.599 Val Acc 36.240\n",
      "L 8 Epoch 16 Loss 1.504 Val Acc 37.980\n",
      "L 8 Epoch 17 Loss 1.504 Val Acc 37.660\n",
      "L 8 Epoch 18 Loss 1.435 Val Acc 39.000\n",
      "L 8 Epoch 19 Loss 1.441 Val Acc 39.540\n",
      "==> Saving model ...\n",
      "L 8 Epoch 20 Loss 1.381 Val Acc 40.680\n",
      "L 8 Epoch 21 Loss 1.297 Val Acc 39.400\n",
      "L 8 Epoch 22 Loss 1.297 Val Acc 38.140\n",
      "L 8 Epoch 23 Loss 1.262 Val Acc 39.080\n",
      "==> Saving model ...\n",
      "L 8 Epoch 24 Loss 1.291 Val Acc 41.500\n",
      "==> Saving model ...\n",
      "L 8 Epoch 25 Loss 1.179 Val Acc 42.420\n",
      "L 8 Epoch 26 Loss 1.189 Val Acc 40.880\n",
      "L 8 Epoch 27 Loss 1.086 Val Acc 40.660\n",
      "==> Saving model ...\n",
      "L 8 Epoch 28 Loss 1.148 Val Acc 44.160\n",
      "L 8 Epoch 29 Loss 1.124 Val Acc 41.840\n",
      "L 8 Epoch 30 Loss 1.031 Val Acc 42.060\n",
      "==> Saving model ...\n",
      "L 8 Epoch 31 Loss 1.035 Val Acc 44.860\n",
      "==> Saving model ...\n",
      "L 8 Epoch 32 Loss 0.909 Val Acc 47.500\n",
      "L 8 Epoch 33 Loss 1.003 Val Acc 47.120\n",
      "==> Saving model ...\n",
      "L 8 Epoch 34 Loss 0.968 Val Acc 47.540\n",
      "==> Saving model ...\n",
      "L 8 Epoch 35 Loss 0.845 Val Acc 47.900\n",
      "==> Saving model ...\n",
      "L 8 Epoch 36 Loss 0.749 Val Acc 48.280\n",
      "L 8 Epoch 37 Loss 0.947 Val Acc 42.800\n",
      "==> Saving model ...\n",
      "L 8 Epoch 38 Loss 0.925 Val Acc 48.380\n",
      "L 8 Epoch 39 Loss 0.871 Val Acc 43.320\n",
      "==> Saving model ...\n",
      "L 8 Epoch 40 Loss 0.653 Val Acc 49.260\n",
      "L 8 Epoch 41 Loss 0.728 Val Acc 45.420\n",
      "==> Saving model ...\n",
      "L 8 Epoch 42 Loss 0.751 Val Acc 49.500\n",
      "L 8 Epoch 43 Loss 0.516 Val Acc 47.880\n",
      "L 8 Epoch 44 Loss 0.487 Val Acc 43.520\n",
      "L 8 Epoch 45 Loss 0.550 Val Acc 48.280\n",
      "==> Saving model ...\n",
      "L 8 Epoch 46 Loss 0.612 Val Acc 49.840\n",
      "L 8 Epoch 47 Loss 0.558 Val Acc 46.800\n",
      "L 8 Epoch 48 Loss 0.540 Val Acc 45.020\n",
      "L 8 Epoch 49 Loss 0.544 Val Acc 49.800\n",
      "==> Saving model ...\n",
      "L 8 Epoch 50 Loss 0.508 Val Acc 49.940\n",
      "L 8 Epoch 51 Loss 0.503 Val Acc 48.540\n",
      "==> Saving model ...\n",
      "L 8 Epoch 52 Loss 0.429 Val Acc 50.700\n",
      "L 8 Epoch 53 Loss 0.384 Val Acc 50.320\n",
      "L 8 Epoch 54 Loss 0.428 Val Acc 49.880\n",
      "L 8 Epoch 55 Loss 0.403 Val Acc 48.680\n",
      "L 8 Epoch 56 Loss 0.366 Val Acc 50.280\n",
      "==> Saving model ...\n",
      "L 8 Epoch 57 Loss 0.399 Val Acc 51.140\n",
      "L 8 Epoch 58 Loss 0.429 Val Acc 49.840\n",
      "L 8 Epoch 59 Loss 0.400 Val Acc 49.620\n",
      "L 8 Epoch 60 Loss 0.307 Val Acc 50.580\n",
      "L 8 Epoch 61 Loss 0.401 Val Acc 49.180\n",
      "==> Saving model ...\n",
      "L 8 Epoch 62 Loss 0.240 Val Acc 52.980\n",
      "L 8 Epoch 63 Loss 0.268 Val Acc 52.620\n",
      "L 8 Epoch 64 Loss 0.310 Val Acc 49.220\n",
      "L 8 Epoch 65 Loss 0.189 Val Acc 51.040\n",
      "==> Saving model ...\n",
      "L 8 Epoch 66 Loss 0.160 Val Acc 53.360\n",
      "L 8 Epoch 67 Loss 0.215 Val Acc 48.680\n",
      "L 8 Epoch 68 Loss 0.119 Val Acc 52.960\n",
      "L 8 Epoch 69 Loss 0.207 Val Acc 51.600\n",
      "L 8 Epoch 70 Loss 0.175 Val Acc 50.720\n",
      "L 8 Epoch 71 Loss 0.181 Val Acc 52.960\n",
      "L 8 Epoch 72 Loss 0.259 Val Acc 52.440\n",
      "L 8 Epoch 73 Loss 0.158 Val Acc 50.660\n",
      "L 8 Epoch 74 Loss 0.242 Val Acc 51.160\n",
      "==> Saving model ...\n",
      "L 8 Epoch 75 Loss 0.193 Val Acc 53.500\n",
      "L 8 Epoch 76 Loss 0.165 Val Acc 52.580\n",
      "L 8 Epoch 77 Loss 0.143 Val Acc 53.460\n",
      "L 8 Epoch 78 Loss 0.162 Val Acc 52.160\n",
      "L 8 Epoch 79 Loss 0.184 Val Acc 50.660\n",
      "L 8 Epoch 80 Loss 0.103 Val Acc 53.060\n",
      "==> Saving model ...\n",
      "L 8 Epoch 81 Loss 0.122 Val Acc 53.540\n",
      "L 8 Epoch 82 Loss 0.147 Val Acc 52.060\n",
      "L 8 Epoch 83 Loss 0.146 Val Acc 52.780\n",
      "L 8 Epoch 84 Loss 0.162 Val Acc 50.820\n",
      "L 8 Epoch 85 Loss 0.201 Val Acc 52.560\n",
      "L 8 Epoch 86 Loss 0.151 Val Acc 50.660\n",
      "L 8 Epoch 87 Loss 0.197 Val Acc 52.500\n",
      "L 8 Epoch 88 Loss 0.203 Val Acc 52.680\n",
      "==> Saving model ...\n",
      "L 8 Epoch 89 Loss 0.194 Val Acc 53.960\n",
      "L 8 Epoch 90 Loss 0.151 Val Acc 52.340\n",
      "L 8 Epoch 91 Loss 0.169 Val Acc 52.100\n",
      "L 8 Epoch 92 Loss 0.113 Val Acc 52.380\n",
      "==> Saving model ...\n",
      "L 8 Epoch 93 Loss 0.070 Val Acc 54.380\n",
      "==> Saving model ...\n",
      "L 8 Epoch 94 Loss 0.099 Val Acc 54.500\n",
      "==> Saving model ...\n",
      "L 8 Epoch 95 Loss 0.075 Val Acc 54.580\n",
      "L 8 Epoch 96 Loss 0.060 Val Acc 50.940\n",
      "==> Saving model ...\n",
      "L 8 Epoch 97 Loss 0.090 Val Acc 54.600\n",
      "==> Saving model ...\n",
      "L 8 Epoch 98 Loss 0.118 Val Acc 55.140\n",
      "L 8 Epoch 99 Loss 0.086 Val Acc 53.520\n",
      "L 8 Epoch 100 Loss 0.096 Val Acc 52.040\n",
      "L 8 Epoch 101 Loss 0.125 Val Acc 52.860\n",
      "L 8 Epoch 102 Loss 0.051 Val Acc 53.720\n",
      "==> Saving model ...\n",
      "L 8 Epoch 103 Loss 0.040 Val Acc 55.340\n",
      "L 8 Epoch 104 Loss 0.067 Val Acc 53.840\n",
      "L 8 Epoch 105 Loss 0.064 Val Acc 55.040\n",
      "L 8 Epoch 106 Loss 0.056 Val Acc 54.680\n",
      "L 8 Epoch 107 Loss 0.032 Val Acc 55.320\n",
      "==> Saving model ...\n",
      "L 8 Epoch 108 Loss 0.056 Val Acc 55.400\n",
      "==> Saving model ...\n",
      "L 8 Epoch 109 Loss 0.032 Val Acc 55.560\n",
      "L 8 Epoch 110 Loss 0.025 Val Acc 55.040\n",
      "L 8 Epoch 111 Loss 0.051 Val Acc 55.040\n",
      "L 8 Epoch 112 Loss 0.070 Val Acc 54.360\n",
      "L 8 Epoch 113 Loss 0.039 Val Acc 55.560\n",
      "L 8 Epoch 114 Loss 0.044 Val Acc 55.320\n",
      "L 8 Epoch 115 Loss 0.048 Val Acc 55.260\n",
      "L 8 Epoch 116 Loss 0.063 Val Acc 55.100\n",
      "L 8 Epoch 117 Loss 0.032 Val Acc 54.820\n",
      "L 8 Epoch 118 Loss 0.034 Val Acc 55.360\n",
      "L 8 Epoch 119 Loss 0.012 Val Acc 55.020\n",
      "L 8 Epoch 120 Loss 0.037 Val Acc 54.920\n",
      "L 8 Epoch 121 Loss 0.044 Val Acc 54.960\n",
      "==> Saving model ...\n",
      "L 8 Epoch 122 Loss 0.029 Val Acc 55.760\n",
      "L 8 Epoch 123 Loss 0.044 Val Acc 55.280\n",
      "L 8 Epoch 124 Loss 0.044 Val Acc 55.220\n",
      "L 8 Epoch 125 Loss 0.056 Val Acc 54.460\n",
      "L 8 Epoch 126 Loss 0.032 Val Acc 55.360\n",
      "L 8 Epoch 127 Loss 0.030 Val Acc 54.940\n",
      "L 8 Epoch 128 Loss 0.039 Val Acc 55.180\n",
      "L 8 Epoch 129 Loss 0.039 Val Acc 55.420\n",
      "L 8 Epoch 130 Loss 0.029 Val Acc 55.500\n",
      "L 8 Epoch 131 Loss 0.031 Val Acc 54.980\n",
      "L 8 Epoch 132 Loss 0.032 Val Acc 55.480\n",
      "==> Saving model ...\n",
      "L 8 Epoch 133 Loss 0.004 Val Acc 55.940\n",
      "==> Saving model ...\n",
      "L 8 Epoch 134 Loss 0.014 Val Acc 56.340\n",
      "==> Saving model ...\n",
      "L 8 Epoch 135 Loss 0.037 Val Acc 56.420\n",
      "L 8 Epoch 136 Loss 0.024 Val Acc 56.000\n",
      "L 8 Epoch 137 Loss 0.018 Val Acc 55.920\n",
      "L 8 Epoch 138 Loss 0.022 Val Acc 56.020\n",
      "L 8 Epoch 139 Loss 0.046 Val Acc 55.920\n",
      "L 8 Epoch 140 Loss 0.019 Val Acc 56.000\n",
      "==> Saving model ...\n",
      "L 8 Epoch 141 Loss 0.029 Val Acc 56.440\n",
      "==> Saving model ...\n",
      "L 8 Epoch 142 Loss 0.023 Val Acc 56.500\n",
      "L 8 Epoch 143 Loss 0.016 Val Acc 56.480\n",
      "L 8 Epoch 144 Loss 0.036 Val Acc 56.000\n",
      "L 8 Epoch 145 Loss 0.025 Val Acc 56.120\n",
      "L 8 Epoch 146 Loss 0.031 Val Acc 56.060\n",
      "L 8 Epoch 147 Loss 0.031 Val Acc 55.600\n",
      "L 8 Epoch 148 Loss 0.028 Val Acc 55.620\n",
      "L 8 Epoch 149 Loss 0.019 Val Acc 56.180\n",
      "L 8 Epoch 150 Loss 0.013 Val Acc 56.380\n",
      "L 8 Epoch 151 Loss 0.033 Val Acc 56.480\n",
      "==> Saving model ...\n",
      "L 8 Epoch 152 Loss 0.005 Val Acc 57.220\n",
      "==> Saving model ...\n",
      "L 8 Epoch 153 Loss 0.004 Val Acc 57.460\n",
      "L 8 Epoch 154 Loss 0.008 Val Acc 57.140\n",
      "L 8 Epoch 155 Loss 0.008 Val Acc 57.100\n",
      "L 8 Epoch 156 Loss 0.014 Val Acc 56.660\n",
      "L 8 Epoch 157 Loss 0.011 Val Acc 56.540\n",
      "L 8 Epoch 158 Loss 0.013 Val Acc 56.500\n",
      "L 8 Epoch 159 Loss 0.016 Val Acc 56.680\n",
      "L 8 Epoch 160 Loss 0.014 Val Acc 56.680\n",
      "L 8 Epoch 161 Loss 0.015 Val Acc 56.520\n",
      "L 8 Epoch 162 Loss 0.015 Val Acc 56.580\n",
      "L 8 Epoch 163 Loss 0.012 Val Acc 56.660\n",
      "L 8 Epoch 164 Loss 0.006 Val Acc 56.940\n",
      "L 8 Epoch 165 Loss 0.010 Val Acc 56.880\n",
      "L 8 Epoch 166 Loss 0.017 Val Acc 56.480\n",
      "L 8 Epoch 167 Loss 0.032 Val Acc 56.320\n",
      "L 8 Epoch 168 Loss 0.010 Val Acc 56.460\n",
      "L 8 Epoch 169 Loss 0.007 Val Acc 56.760\n",
      "L 8 Epoch 170 Loss 0.009 Val Acc 56.560\n",
      "L 8 Epoch 171 Loss 0.010 Val Acc 56.740\n",
      "L 8 Epoch 172 Loss 0.018 Val Acc 56.340\n",
      "L 8 Epoch 173 Loss 0.011 Val Acc 56.680\n",
      "L 8 Epoch 174 Loss 0.007 Val Acc 56.880\n",
      "L 8 Epoch 175 Loss 0.004 Val Acc 56.980\n",
      "L 8 Epoch 176 Loss 0.013 Val Acc 56.320\n",
      "L 8 Epoch 177 Loss 0.016 Val Acc 56.200\n",
      "L 8 Epoch 178 Loss 0.007 Val Acc 56.560\n",
      "L 8 Epoch 179 Loss 0.011 Val Acc 56.720\n",
      "L 8 Epoch 180 Loss 0.002 Val Acc 56.900\n",
      "L 8 Epoch 181 Loss 0.015 Val Acc 56.720\n",
      "L 8 Epoch 182 Loss 0.013 Val Acc 56.840\n",
      "L 8 Epoch 183 Loss 0.011 Val Acc 56.560\n",
      "L 8 Epoch 184 Loss 0.016 Val Acc 56.500\n",
      "L 8 Epoch 185 Loss 0.014 Val Acc 56.680\n",
      "L 8 Epoch 186 Loss 0.008 Val Acc 56.740\n",
      "L 8 Epoch 187 Loss 0.011 Val Acc 56.800\n",
      "L 8 Epoch 188 Loss 0.006 Val Acc 56.600\n",
      "L 8 Epoch 189 Loss 0.020 Val Acc 56.620\n",
      "L 8 Epoch 190 Loss 0.018 Val Acc 56.340\n",
      "L 8 Epoch 191 Loss 0.013 Val Acc 57.040\n",
      "L 8 Epoch 192 Loss 0.008 Val Acc 56.960\n",
      "L 8 Epoch 193 Loss 0.011 Val Acc 56.680\n",
      "L 8 Epoch 194 Loss 0.004 Val Acc 56.620\n",
      "L 8 Epoch 195 Loss 0.004 Val Acc 56.840\n",
      "L 8 Epoch 196 Loss 0.010 Val Acc 56.600\n",
      "L 8 Epoch 197 Loss 0.011 Val Acc 56.640\n",
      "L 8 Epoch 198 Loss 0.020 Val Acc 56.380\n",
      "L 8 Epoch 199 Loss 0.005 Val Acc 56.580\n",
      "L 10 Epoch 0 Loss 3.822 Val Acc 10.000\n",
      "L 10 Epoch 1 Loss 2.589 Val Acc 10.000\n",
      "L 10 Epoch 2 Loss 2.207 Val Acc 11.240\n",
      "L 10 Epoch 3 Loss 1.983 Val Acc 11.800\n",
      "L 10 Epoch 4 Loss 1.921 Val Acc 14.460\n",
      "L 10 Epoch 5 Loss 1.894 Val Acc 17.860\n",
      "L 10 Epoch 6 Loss 1.746 Val Acc 19.180\n",
      "L 10 Epoch 7 Loss 1.677 Val Acc 25.140\n",
      "L 10 Epoch 8 Loss 1.587 Val Acc 34.000\n",
      "L 10 Epoch 9 Loss 1.517 Val Acc 36.580\n",
      "L 10 Epoch 10 Loss 1.459 Val Acc 36.960\n",
      "==> Saving model ...\n",
      "L 10 Epoch 11 Loss 1.507 Val Acc 40.560\n",
      "==> Saving model ...\n",
      "L 10 Epoch 12 Loss 1.460 Val Acc 41.300\n",
      "L 10 Epoch 13 Loss 1.375 Val Acc 39.780\n",
      "==> Saving model ...\n",
      "L 10 Epoch 14 Loss 1.379 Val Acc 44.920\n",
      "L 10 Epoch 15 Loss 1.287 Val Acc 43.780\n",
      "L 10 Epoch 16 Loss 1.237 Val Acc 43.740\n",
      "L 10 Epoch 17 Loss 1.319 Val Acc 38.000\n",
      "==> Saving model ...\n",
      "L 10 Epoch 18 Loss 1.166 Val Acc 45.240\n",
      "L 10 Epoch 19 Loss 1.155 Val Acc 43.740\n",
      "==> Saving model ...\n",
      "L 10 Epoch 20 Loss 1.196 Val Acc 46.560\n",
      "L 10 Epoch 21 Loss 1.131 Val Acc 43.540\n",
      "L 10 Epoch 22 Loss 1.132 Val Acc 45.720\n",
      "L 10 Epoch 23 Loss 1.070 Val Acc 42.400\n",
      "==> Saving model ...\n",
      "L 10 Epoch 24 Loss 1.015 Val Acc 46.720\n",
      "==> Saving model ...\n",
      "L 10 Epoch 25 Loss 0.908 Val Acc 47.700\n",
      "L 10 Epoch 26 Loss 0.887 Val Acc 42.700\n",
      "L 10 Epoch 27 Loss 0.839 Val Acc 41.380\n",
      "L 10 Epoch 28 Loss 0.930 Val Acc 44.480\n",
      "==> Saving model ...\n",
      "L 10 Epoch 29 Loss 0.729 Val Acc 48.820\n",
      "L 10 Epoch 30 Loss 0.693 Val Acc 46.500\n",
      "L 10 Epoch 31 Loss 0.760 Val Acc 48.060\n",
      "==> Saving model ...\n",
      "L 10 Epoch 32 Loss 0.594 Val Acc 50.460\n",
      "L 10 Epoch 33 Loss 0.644 Val Acc 46.220\n",
      "L 10 Epoch 34 Loss 0.618 Val Acc 49.680\n",
      "L 10 Epoch 35 Loss 0.618 Val Acc 48.840\n",
      "L 10 Epoch 36 Loss 0.486 Val Acc 46.920\n",
      "L 10 Epoch 37 Loss 0.549 Val Acc 46.700\n",
      "L 10 Epoch 38 Loss 0.513 Val Acc 49.540\n",
      "L 10 Epoch 39 Loss 0.525 Val Acc 48.620\n",
      "L 10 Epoch 40 Loss 0.492 Val Acc 46.260\n",
      "L 10 Epoch 41 Loss 0.435 Val Acc 50.340\n",
      "L 10 Epoch 42 Loss 0.447 Val Acc 48.800\n",
      "L 10 Epoch 43 Loss 0.410 Val Acc 48.560\n",
      "L 10 Epoch 44 Loss 0.325 Val Acc 48.840\n",
      "==> Saving model ...\n",
      "L 10 Epoch 45 Loss 0.424 Val Acc 51.360\n",
      "L 10 Epoch 46 Loss 0.319 Val Acc 47.840\n",
      "L 10 Epoch 47 Loss 0.311 Val Acc 51.000\n",
      "L 10 Epoch 48 Loss 0.428 Val Acc 48.700\n",
      "L 10 Epoch 49 Loss 0.318 Val Acc 49.720\n",
      "L 10 Epoch 50 Loss 0.343 Val Acc 47.800\n",
      "==> Saving model ...\n",
      "L 10 Epoch 51 Loss 0.317 Val Acc 51.940\n",
      "L 10 Epoch 52 Loss 0.391 Val Acc 47.460\n",
      "L 10 Epoch 53 Loss 0.289 Val Acc 51.660\n",
      "==> Saving model ...\n",
      "L 10 Epoch 54 Loss 0.259 Val Acc 52.060\n",
      "==> Saving model ...\n",
      "L 10 Epoch 55 Loss 0.261 Val Acc 52.780\n",
      "==> Saving model ...\n",
      "L 10 Epoch 56 Loss 0.241 Val Acc 53.160\n",
      "L 10 Epoch 57 Loss 0.161 Val Acc 52.680\n",
      "==> Saving model ...\n",
      "L 10 Epoch 58 Loss 0.263 Val Acc 53.240\n",
      "==> Saving model ...\n",
      "L 10 Epoch 59 Loss 0.191 Val Acc 53.600\n",
      "L 10 Epoch 60 Loss 0.230 Val Acc 49.380\n",
      "L 10 Epoch 61 Loss 0.141 Val Acc 47.700\n",
      "L 10 Epoch 62 Loss 0.179 Val Acc 51.720\n",
      "==> Saving model ...\n",
      "L 10 Epoch 63 Loss 0.166 Val Acc 53.900\n",
      "L 10 Epoch 64 Loss 0.115 Val Acc 52.400\n",
      "L 10 Epoch 65 Loss 0.186 Val Acc 48.340\n",
      "L 10 Epoch 66 Loss 0.129 Val Acc 50.960\n",
      "==> Saving model ...\n",
      "L 10 Epoch 67 Loss 0.127 Val Acc 54.460\n",
      "L 10 Epoch 68 Loss 0.112 Val Acc 53.380\n",
      "==> Saving model ...\n",
      "L 10 Epoch 69 Loss 0.113 Val Acc 54.660\n",
      "L 10 Epoch 70 Loss 0.153 Val Acc 52.560\n",
      "L 10 Epoch 71 Loss 0.122 Val Acc 53.320\n",
      "L 10 Epoch 72 Loss 0.101 Val Acc 52.580\n",
      "L 10 Epoch 73 Loss 0.136 Val Acc 52.100\n",
      "L 10 Epoch 74 Loss 0.146 Val Acc 54.540\n",
      "L 10 Epoch 75 Loss 0.114 Val Acc 54.020\n",
      "L 10 Epoch 76 Loss 0.143 Val Acc 52.440\n",
      "L 10 Epoch 77 Loss 0.189 Val Acc 51.980\n",
      "L 10 Epoch 78 Loss 0.130 Val Acc 52.240\n",
      "L 10 Epoch 79 Loss 0.120 Val Acc 54.060\n",
      "L 10 Epoch 80 Loss 0.138 Val Acc 53.720\n",
      "L 10 Epoch 81 Loss 0.095 Val Acc 51.280\n",
      "L 10 Epoch 82 Loss 0.116 Val Acc 53.860\n",
      "L 10 Epoch 83 Loss 0.060 Val Acc 54.580\n",
      "L 10 Epoch 84 Loss 0.063 Val Acc 54.500\n",
      "==> Saving model ...\n",
      "L 10 Epoch 85 Loss 0.046 Val Acc 55.140\n",
      "L 10 Epoch 86 Loss 0.094 Val Acc 54.780\n",
      "L 10 Epoch 87 Loss 0.050 Val Acc 53.660\n",
      "L 10 Epoch 88 Loss 0.035 Val Acc 54.080\n",
      "L 10 Epoch 89 Loss 0.046 Val Acc 53.360\n",
      "==> Saving model ...\n",
      "L 10 Epoch 90 Loss 0.051 Val Acc 55.160\n",
      "L 10 Epoch 91 Loss 0.054 Val Acc 55.160\n",
      "L 10 Epoch 92 Loss 0.085 Val Acc 53.820\n",
      "L 10 Epoch 93 Loss 0.084 Val Acc 53.900\n",
      "L 10 Epoch 94 Loss 0.018 Val Acc 54.680\n",
      "L 10 Epoch 95 Loss 0.048 Val Acc 54.800\n",
      "L 10 Epoch 96 Loss 0.035 Val Acc 54.100\n",
      "==> Saving model ...\n",
      "L 10 Epoch 97 Loss 0.041 Val Acc 55.860\n",
      "L 10 Epoch 98 Loss 0.063 Val Acc 55.400\n",
      "L 10 Epoch 99 Loss 0.039 Val Acc 55.220\n",
      "L 10 Epoch 100 Loss 0.053 Val Acc 55.580\n",
      "L 10 Epoch 101 Loss 0.033 Val Acc 55.460\n",
      "L 10 Epoch 102 Loss 0.052 Val Acc 53.580\n",
      "L 10 Epoch 103 Loss 0.032 Val Acc 54.540\n",
      "==> Saving model ...\n",
      "L 10 Epoch 104 Loss 0.040 Val Acc 56.560\n",
      "==> Saving model ...\n",
      "L 10 Epoch 105 Loss 0.040 Val Acc 57.160\n",
      "L 10 Epoch 106 Loss 0.044 Val Acc 56.580\n",
      "L 10 Epoch 107 Loss 0.031 Val Acc 55.520\n",
      "L 10 Epoch 108 Loss 0.037 Val Acc 56.060\n",
      "L 10 Epoch 109 Loss 0.030 Val Acc 56.400\n",
      "L 10 Epoch 110 Loss 0.039 Val Acc 55.780\n",
      "L 10 Epoch 111 Loss 0.041 Val Acc 55.260\n",
      "L 10 Epoch 112 Loss 0.025 Val Acc 55.620\n",
      "L 10 Epoch 113 Loss 0.036 Val Acc 55.180\n",
      "L 10 Epoch 114 Loss 0.035 Val Acc 57.000\n",
      "L 10 Epoch 115 Loss 0.026 Val Acc 57.080\n",
      "L 10 Epoch 116 Loss 0.051 Val Acc 56.900\n",
      "L 10 Epoch 117 Loss 0.036 Val Acc 55.940\n",
      "L 10 Epoch 118 Loss 0.045 Val Acc 55.540\n",
      "L 10 Epoch 119 Loss 0.017 Val Acc 55.680\n",
      "L 10 Epoch 120 Loss 0.042 Val Acc 56.280\n",
      "L 10 Epoch 121 Loss 0.016 Val Acc 56.100\n",
      "L 10 Epoch 122 Loss 0.023 Val Acc 56.920\n",
      "L 10 Epoch 123 Loss 0.016 Val Acc 56.460\n",
      "L 10 Epoch 124 Loss 0.038 Val Acc 56.500\n",
      "L 10 Epoch 125 Loss 0.022 Val Acc 56.160\n",
      "L 10 Epoch 126 Loss 0.018 Val Acc 56.720\n",
      "L 10 Epoch 127 Loss 0.018 Val Acc 56.920\n",
      "L 10 Epoch 128 Loss 0.018 Val Acc 56.740\n",
      "L 10 Epoch 129 Loss 0.011 Val Acc 56.660\n",
      "L 10 Epoch 130 Loss 0.009 Val Acc 57.120\n",
      "L 10 Epoch 131 Loss 0.029 Val Acc 57.160\n",
      "L 10 Epoch 132 Loss 0.019 Val Acc 56.400\n",
      "L 10 Epoch 133 Loss 0.028 Val Acc 55.800\n",
      "L 10 Epoch 134 Loss 0.021 Val Acc 56.360\n",
      "L 10 Epoch 135 Loss 0.011 Val Acc 56.560\n",
      "L 10 Epoch 136 Loss 0.017 Val Acc 56.780\n",
      "L 10 Epoch 137 Loss 0.002 Val Acc 57.060\n",
      "==> Saving model ...\n",
      "L 10 Epoch 138 Loss 0.012 Val Acc 57.540\n",
      "L 10 Epoch 139 Loss 0.013 Val Acc 57.300\n",
      "L 10 Epoch 140 Loss 0.012 Val Acc 57.380\n",
      "L 10 Epoch 141 Loss 0.018 Val Acc 56.860\n",
      "L 10 Epoch 142 Loss 0.021 Val Acc 56.600\n",
      "L 10 Epoch 143 Loss 0.010 Val Acc 57.120\n",
      "L 10 Epoch 144 Loss 0.024 Val Acc 57.000\n",
      "L 10 Epoch 145 Loss 0.020 Val Acc 56.920\n",
      "L 10 Epoch 146 Loss 0.005 Val Acc 56.820\n",
      "L 10 Epoch 147 Loss 0.012 Val Acc 56.560\n",
      "L 10 Epoch 148 Loss 0.008 Val Acc 56.600\n",
      "L 10 Epoch 149 Loss 0.017 Val Acc 56.600\n",
      "L 10 Epoch 150 Loss 0.011 Val Acc 57.000\n",
      "L 10 Epoch 151 Loss 0.007 Val Acc 57.380\n",
      "L 10 Epoch 152 Loss 0.007 Val Acc 57.480\n",
      "L 10 Epoch 153 Loss 0.011 Val Acc 57.400\n",
      "==> Saving model ...\n",
      "L 10 Epoch 154 Loss 0.012 Val Acc 57.600\n",
      "==> Saving model ...\n",
      "L 10 Epoch 155 Loss 0.006 Val Acc 57.640\n",
      "L 10 Epoch 156 Loss 0.012 Val Acc 57.400\n",
      "==> Saving model ...\n",
      "L 10 Epoch 157 Loss 0.003 Val Acc 57.860\n",
      "==> Saving model ...\n",
      "L 10 Epoch 158 Loss 0.008 Val Acc 57.900\n",
      "==> Saving model ...\n",
      "L 10 Epoch 159 Loss 0.014 Val Acc 57.980\n",
      "L 10 Epoch 160 Loss 0.010 Val Acc 57.900\n",
      "L 10 Epoch 161 Loss 0.003 Val Acc 57.920\n",
      "L 10 Epoch 162 Loss 0.013 Val Acc 57.860\n",
      "L 10 Epoch 163 Loss 0.010 Val Acc 57.880\n",
      "==> Saving model ...\n",
      "L 10 Epoch 164 Loss 0.004 Val Acc 58.000\n",
      "==> Saving model ...\n",
      "L 10 Epoch 165 Loss 0.010 Val Acc 58.240\n",
      "L 10 Epoch 166 Loss 0.009 Val Acc 57.900\n",
      "L 10 Epoch 167 Loss 0.007 Val Acc 57.420\n",
      "L 10 Epoch 168 Loss 0.014 Val Acc 57.440\n",
      "L 10 Epoch 169 Loss 0.010 Val Acc 57.600\n",
      "L 10 Epoch 170 Loss 0.007 Val Acc 57.520\n",
      "L 10 Epoch 171 Loss 0.005 Val Acc 57.880\n",
      "L 10 Epoch 172 Loss 0.007 Val Acc 57.460\n",
      "L 10 Epoch 173 Loss 0.009 Val Acc 57.780\n",
      "L 10 Epoch 174 Loss 0.010 Val Acc 57.900\n",
      "L 10 Epoch 175 Loss 0.014 Val Acc 57.540\n",
      "L 10 Epoch 176 Loss 0.002 Val Acc 58.020\n",
      "L 10 Epoch 177 Loss 0.008 Val Acc 57.960\n",
      "L 10 Epoch 178 Loss 0.012 Val Acc 57.340\n",
      "L 10 Epoch 179 Loss 0.007 Val Acc 57.860\n",
      "L 10 Epoch 180 Loss 0.007 Val Acc 57.440\n",
      "L 10 Epoch 181 Loss 0.008 Val Acc 57.280\n",
      "L 10 Epoch 182 Loss 0.005 Val Acc 57.580\n",
      "L 10 Epoch 183 Loss 0.014 Val Acc 57.400\n",
      "L 10 Epoch 184 Loss 0.004 Val Acc 57.940\n",
      "L 10 Epoch 185 Loss 0.006 Val Acc 57.960\n",
      "L 10 Epoch 186 Loss 0.012 Val Acc 57.920\n",
      "L 10 Epoch 187 Loss 0.004 Val Acc 57.820\n",
      "L 10 Epoch 188 Loss 0.005 Val Acc 57.780\n",
      "L 10 Epoch 189 Loss 0.007 Val Acc 57.760\n",
      "L 10 Epoch 190 Loss 0.008 Val Acc 57.580\n",
      "L 10 Epoch 191 Loss 0.009 Val Acc 57.360\n",
      "L 10 Epoch 192 Loss 0.009 Val Acc 57.420\n",
      "L 10 Epoch 193 Loss 0.006 Val Acc 57.460\n",
      "L 10 Epoch 194 Loss 0.015 Val Acc 57.420\n",
      "L 10 Epoch 195 Loss 0.006 Val Acc 57.820\n",
      "L 10 Epoch 196 Loss 0.008 Val Acc 57.960\n",
      "L 10 Epoch 197 Loss 0.012 Val Acc 57.940\n",
      "L 10 Epoch 198 Loss 0.008 Val Acc 57.720\n",
      "L 10 Epoch 199 Loss 0.014 Val Acc 57.680\n",
      "L 4 Epoch 0 Loss 3.920 Val Acc 10.000\n",
      "L 4 Epoch 1 Loss 3.312 Val Acc 10.000\n",
      "L 4 Epoch 2 Loss 3.442 Val Acc 9.520\n",
      "L 4 Epoch 3 Loss 2.713 Val Acc 11.120\n",
      "L 4 Epoch 4 Loss 2.360 Val Acc 17.440\n",
      "L 4 Epoch 5 Loss 2.152 Val Acc 21.300\n",
      "L 4 Epoch 6 Loss 2.040 Val Acc 22.220\n",
      "L 4 Epoch 7 Loss 2.079 Val Acc 27.360\n",
      "L 4 Epoch 8 Loss 1.859 Val Acc 27.160\n",
      "L 4 Epoch 9 Loss 1.815 Val Acc 29.980\n",
      "L 4 Epoch 10 Loss 1.775 Val Acc 33.140\n",
      "L 4 Epoch 11 Loss 1.754 Val Acc 31.940\n",
      "L 4 Epoch 12 Loss 1.695 Val Acc 31.120\n",
      "L 4 Epoch 13 Loss 1.669 Val Acc 34.560\n",
      "L 4 Epoch 14 Loss 1.626 Val Acc 34.340\n",
      "L 4 Epoch 15 Loss 1.620 Val Acc 34.260\n",
      "L 4 Epoch 16 Loss 1.529 Val Acc 37.800\n",
      "L 4 Epoch 17 Loss 1.522 Val Acc 38.780\n",
      "L 4 Epoch 18 Loss 1.486 Val Acc 36.860\n",
      "==> Saving model ...\n",
      "L 4 Epoch 19 Loss 1.498 Val Acc 40.100\n",
      "==> Saving model ...\n",
      "L 4 Epoch 20 Loss 1.438 Val Acc 40.540\n",
      "==> Saving model ...\n",
      "L 4 Epoch 21 Loss 1.446 Val Acc 40.840\n",
      "L 4 Epoch 22 Loss 1.419 Val Acc 40.380\n",
      "==> Saving model ...\n",
      "L 4 Epoch 23 Loss 1.363 Val Acc 41.420\n",
      "L 4 Epoch 24 Loss 1.293 Val Acc 41.420\n",
      "L 4 Epoch 25 Loss 1.297 Val Acc 39.340\n",
      "L 4 Epoch 26 Loss 1.268 Val Acc 39.460\n",
      "L 4 Epoch 27 Loss 1.307 Val Acc 38.620\n",
      "==> Saving model ...\n",
      "L 4 Epoch 28 Loss 1.325 Val Acc 41.680\n",
      "L 4 Epoch 29 Loss 1.263 Val Acc 37.900\n",
      "==> Saving model ...\n",
      "L 4 Epoch 30 Loss 1.202 Val Acc 41.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 31 Loss 1.208 Val Acc 43.720\n",
      "==> Saving model ...\n",
      "L 4 Epoch 32 Loss 1.157 Val Acc 44.860\n",
      "L 4 Epoch 33 Loss 1.074 Val Acc 42.600\n",
      "L 4 Epoch 34 Loss 0.981 Val Acc 42.620\n",
      "L 4 Epoch 35 Loss 0.991 Val Acc 40.800\n",
      "L 4 Epoch 36 Loss 1.007 Val Acc 41.280\n",
      "L 4 Epoch 37 Loss 1.091 Val Acc 42.540\n",
      "L 4 Epoch 38 Loss 1.100 Val Acc 44.280\n",
      "L 4 Epoch 39 Loss 0.977 Val Acc 42.640\n",
      "L 4 Epoch 40 Loss 0.934 Val Acc 44.600\n",
      "L 4 Epoch 41 Loss 0.882 Val Acc 43.340\n",
      "L 4 Epoch 42 Loss 0.775 Val Acc 43.680\n",
      "L 4 Epoch 43 Loss 0.783 Val Acc 44.480\n",
      "==> Saving model ...\n",
      "L 4 Epoch 44 Loss 0.920 Val Acc 46.140\n",
      "==> Saving model ...\n",
      "L 4 Epoch 45 Loss 0.627 Val Acc 47.160\n",
      "L 4 Epoch 46 Loss 0.848 Val Acc 43.200\n",
      "L 4 Epoch 47 Loss 0.829 Val Acc 47.040\n",
      "L 4 Epoch 48 Loss 0.604 Val Acc 45.220\n",
      "L 4 Epoch 49 Loss 0.575 Val Acc 43.140\n",
      "==> Saving model ...\n",
      "L 4 Epoch 50 Loss 0.637 Val Acc 47.700\n",
      "==> Saving model ...\n",
      "L 4 Epoch 51 Loss 0.532 Val Acc 48.200\n",
      "==> Saving model ...\n",
      "L 4 Epoch 52 Loss 0.449 Val Acc 49.360\n",
      "L 4 Epoch 53 Loss 0.500 Val Acc 48.220\n",
      "L 4 Epoch 54 Loss 0.457 Val Acc 48.900\n",
      "L 4 Epoch 55 Loss 0.516 Val Acc 46.740\n",
      "L 4 Epoch 56 Loss 0.547 Val Acc 46.920\n",
      "L 4 Epoch 57 Loss 0.528 Val Acc 45.900\n",
      "L 4 Epoch 58 Loss 0.340 Val Acc 47.360\n",
      "==> Saving model ...\n",
      "L 4 Epoch 59 Loss 0.313 Val Acc 50.600\n",
      "L 4 Epoch 60 Loss 0.417 Val Acc 47.220\n",
      "L 4 Epoch 61 Loss 0.431 Val Acc 44.360\n",
      "L 4 Epoch 62 Loss 0.348 Val Acc 46.780\n",
      "L 4 Epoch 63 Loss 0.488 Val Acc 47.600\n",
      "L 4 Epoch 64 Loss 0.291 Val Acc 48.520\n",
      "L 4 Epoch 65 Loss 0.326 Val Acc 45.620\n",
      "L 4 Epoch 66 Loss 0.315 Val Acc 48.460\n",
      "L 4 Epoch 67 Loss 0.284 Val Acc 49.540\n",
      "L 4 Epoch 68 Loss 0.240 Val Acc 46.740\n",
      "L 4 Epoch 69 Loss 0.219 Val Acc 48.240\n",
      "L 4 Epoch 70 Loss 0.200 Val Acc 49.800\n",
      "==> Saving model ...\n",
      "L 4 Epoch 71 Loss 0.260 Val Acc 51.500\n",
      "L 4 Epoch 72 Loss 0.215 Val Acc 49.560\n",
      "L 4 Epoch 73 Loss 0.229 Val Acc 49.680\n",
      "L 4 Epoch 74 Loss 0.208 Val Acc 49.720\n",
      "L 4 Epoch 75 Loss 0.324 Val Acc 48.020\n",
      "L 4 Epoch 76 Loss 0.208 Val Acc 50.540\n",
      "L 4 Epoch 77 Loss 0.263 Val Acc 46.580\n",
      "L 4 Epoch 78 Loss 0.234 Val Acc 49.940\n",
      "L 4 Epoch 79 Loss 0.194 Val Acc 50.180\n",
      "L 4 Epoch 80 Loss 0.180 Val Acc 50.460\n",
      "L 4 Epoch 81 Loss 0.105 Val Acc 50.100\n",
      "L 4 Epoch 82 Loss 0.165 Val Acc 50.940\n",
      "L 4 Epoch 83 Loss 0.187 Val Acc 50.900\n",
      "==> Saving model ...\n",
      "L 4 Epoch 84 Loss 0.145 Val Acc 52.360\n",
      "L 4 Epoch 85 Loss 0.178 Val Acc 50.880\n",
      "L 4 Epoch 86 Loss 0.181 Val Acc 50.840\n",
      "L 4 Epoch 87 Loss 0.130 Val Acc 51.560\n",
      "L 4 Epoch 88 Loss 0.093 Val Acc 49.540\n",
      "L 4 Epoch 89 Loss 0.087 Val Acc 50.880\n",
      "L 4 Epoch 90 Loss 0.036 Val Acc 51.920\n",
      "L 4 Epoch 91 Loss 0.102 Val Acc 51.440\n",
      "L 4 Epoch 92 Loss 0.096 Val Acc 51.780\n",
      "L 4 Epoch 93 Loss 0.083 Val Acc 51.660\n",
      "==> Saving model ...\n",
      "L 4 Epoch 94 Loss 0.102 Val Acc 52.560\n",
      "L 4 Epoch 95 Loss 0.091 Val Acc 50.640\n",
      "L 4 Epoch 96 Loss 0.082 Val Acc 50.740\n",
      "==> Saving model ...\n",
      "L 4 Epoch 97 Loss 0.091 Val Acc 52.840\n",
      "==> Saving model ...\n",
      "L 4 Epoch 98 Loss 0.058 Val Acc 53.080\n",
      "L 4 Epoch 99 Loss 0.099 Val Acc 52.780\n",
      "L 4 Epoch 100 Loss 0.123 Val Acc 51.260\n",
      "L 4 Epoch 101 Loss 0.053 Val Acc 50.800\n",
      "L 4 Epoch 102 Loss 0.121 Val Acc 50.220\n",
      "L 4 Epoch 103 Loss 0.058 Val Acc 51.800\n",
      "L 4 Epoch 104 Loss 0.096 Val Acc 50.360\n",
      "L 4 Epoch 105 Loss 0.113 Val Acc 51.920\n",
      "L 4 Epoch 106 Loss 0.082 Val Acc 51.800\n",
      "L 4 Epoch 107 Loss 0.069 Val Acc 51.900\n",
      "L 4 Epoch 108 Loss 0.070 Val Acc 52.520\n",
      "L 4 Epoch 109 Loss 0.046 Val Acc 52.400\n",
      "L 4 Epoch 110 Loss 0.068 Val Acc 52.580\n",
      "L 4 Epoch 111 Loss 0.065 Val Acc 51.640\n",
      "L 4 Epoch 112 Loss 0.046 Val Acc 52.460\n",
      "L 4 Epoch 113 Loss 0.075 Val Acc 51.920\n",
      "L 4 Epoch 114 Loss 0.064 Val Acc 51.980\n",
      "L 4 Epoch 115 Loss 0.047 Val Acc 52.840\n",
      "L 4 Epoch 116 Loss 0.055 Val Acc 52.740\n",
      "L 4 Epoch 117 Loss 0.086 Val Acc 52.860\n",
      "L 4 Epoch 118 Loss 0.072 Val Acc 52.180\n",
      "L 4 Epoch 119 Loss 0.065 Val Acc 51.980\n",
      "L 4 Epoch 120 Loss 0.046 Val Acc 52.960\n",
      "==> Saving model ...\n",
      "L 4 Epoch 121 Loss 0.035 Val Acc 53.300\n",
      "==> Saving model ...\n",
      "L 4 Epoch 122 Loss 0.036 Val Acc 53.520\n",
      "L 4 Epoch 123 Loss 0.058 Val Acc 52.280\n",
      "L 4 Epoch 124 Loss 0.026 Val Acc 52.340\n",
      "L 4 Epoch 125 Loss 0.028 Val Acc 53.480\n",
      "L 4 Epoch 126 Loss 0.036 Val Acc 52.440\n",
      "L 4 Epoch 127 Loss 0.031 Val Acc 52.720\n",
      "L 4 Epoch 128 Loss 0.035 Val Acc 52.140\n",
      "L 4 Epoch 129 Loss 0.034 Val Acc 53.000\n",
      "L 4 Epoch 130 Loss 0.043 Val Acc 53.340\n",
      "==> Saving model ...\n",
      "L 4 Epoch 131 Loss 0.024 Val Acc 53.920\n",
      "L 4 Epoch 132 Loss 0.035 Val Acc 53.820\n",
      "L 4 Epoch 133 Loss 0.029 Val Acc 53.080\n",
      "L 4 Epoch 134 Loss 0.033 Val Acc 52.920\n",
      "L 4 Epoch 135 Loss 0.015 Val Acc 53.300\n",
      "L 4 Epoch 136 Loss 0.035 Val Acc 53.300\n",
      "==> Saving model ...\n",
      "L 4 Epoch 137 Loss 0.034 Val Acc 54.000\n",
      "==> Saving model ...\n",
      "L 4 Epoch 138 Loss 0.017 Val Acc 54.400\n",
      "==> Saving model ...\n",
      "L 4 Epoch 139 Loss 0.027 Val Acc 54.600\n",
      "L 4 Epoch 140 Loss 0.047 Val Acc 54.280\n",
      "L 4 Epoch 141 Loss 0.015 Val Acc 53.900\n",
      "L 4 Epoch 142 Loss 0.017 Val Acc 53.500\n",
      "L 4 Epoch 143 Loss 0.033 Val Acc 53.420\n",
      "L 4 Epoch 144 Loss 0.028 Val Acc 53.180\n",
      "L 4 Epoch 145 Loss 0.013 Val Acc 53.680\n",
      "L 4 Epoch 146 Loss 0.015 Val Acc 53.600\n",
      "L 4 Epoch 147 Loss 0.013 Val Acc 54.140\n",
      "L 4 Epoch 148 Loss 0.024 Val Acc 54.260\n",
      "L 4 Epoch 149 Loss 0.013 Val Acc 54.180\n",
      "L 4 Epoch 150 Loss 0.019 Val Acc 54.260\n",
      "L 4 Epoch 151 Loss 0.016 Val Acc 54.520\n",
      "L 4 Epoch 152 Loss 0.028 Val Acc 53.940\n",
      "L 4 Epoch 153 Loss 0.027 Val Acc 53.880\n",
      "L 4 Epoch 154 Loss 0.013 Val Acc 53.920\n",
      "L 4 Epoch 155 Loss 0.016 Val Acc 54.100\n",
      "L 4 Epoch 156 Loss 0.006 Val Acc 54.200\n",
      "L 4 Epoch 157 Loss 0.027 Val Acc 54.500\n",
      "L 4 Epoch 158 Loss 0.017 Val Acc 54.220\n",
      "L 4 Epoch 159 Loss 0.014 Val Acc 54.420\n",
      "L 4 Epoch 160 Loss 0.018 Val Acc 53.720\n",
      "L 4 Epoch 161 Loss 0.015 Val Acc 53.780\n",
      "L 4 Epoch 162 Loss 0.014 Val Acc 54.260\n",
      "L 4 Epoch 163 Loss 0.030 Val Acc 53.960\n",
      "L 4 Epoch 164 Loss 0.015 Val Acc 54.080\n",
      "L 4 Epoch 165 Loss 0.028 Val Acc 53.520\n",
      "L 4 Epoch 166 Loss 0.012 Val Acc 54.020\n",
      "L 4 Epoch 167 Loss 0.005 Val Acc 54.260\n",
      "L 4 Epoch 168 Loss 0.019 Val Acc 53.980\n",
      "L 4 Epoch 169 Loss 0.026 Val Acc 53.680\n",
      "L 4 Epoch 170 Loss 0.033 Val Acc 53.740\n",
      "L 4 Epoch 171 Loss 0.016 Val Acc 53.760\n",
      "L 4 Epoch 172 Loss 0.011 Val Acc 54.120\n",
      "L 4 Epoch 173 Loss 0.008 Val Acc 54.360\n",
      "L 4 Epoch 174 Loss 0.018 Val Acc 54.120\n",
      "L 4 Epoch 175 Loss 0.017 Val Acc 53.680\n",
      "L 4 Epoch 176 Loss 0.011 Val Acc 54.320\n",
      "L 4 Epoch 177 Loss 0.020 Val Acc 54.240\n",
      "L 4 Epoch 178 Loss 0.016 Val Acc 54.540\n",
      "L 4 Epoch 179 Loss 0.015 Val Acc 54.360\n",
      "L 4 Epoch 180 Loss 0.007 Val Acc 54.500\n",
      "L 4 Epoch 181 Loss 0.016 Val Acc 54.400\n",
      "L 4 Epoch 182 Loss 0.016 Val Acc 54.200\n",
      "==> Saving model ...\n",
      "L 4 Epoch 183 Loss 0.010 Val Acc 54.720\n",
      "L 4 Epoch 184 Loss 0.026 Val Acc 54.440\n",
      "==> Saving model ...\n",
      "L 4 Epoch 185 Loss 0.018 Val Acc 54.760\n",
      "L 4 Epoch 186 Loss 0.022 Val Acc 54.020\n",
      "L 4 Epoch 187 Loss 0.016 Val Acc 53.680\n",
      "L 4 Epoch 188 Loss 0.011 Val Acc 54.680\n",
      "==> Saving model ...\n",
      "L 4 Epoch 189 Loss 0.006 Val Acc 54.940\n",
      "L 4 Epoch 190 Loss 0.017 Val Acc 54.440\n",
      "L 4 Epoch 191 Loss 0.014 Val Acc 54.040\n",
      "L 4 Epoch 192 Loss 0.008 Val Acc 54.780\n",
      "L 4 Epoch 193 Loss 0.025 Val Acc 54.700\n",
      "L 4 Epoch 194 Loss 0.012 Val Acc 54.560\n",
      "L 4 Epoch 195 Loss 0.013 Val Acc 54.320\n",
      "L 4 Epoch 196 Loss 0.006 Val Acc 54.840\n",
      "L 4 Epoch 197 Loss 0.014 Val Acc 54.200\n",
      "L 4 Epoch 198 Loss 0.010 Val Acc 54.840\n",
      "L 4 Epoch 199 Loss 0.010 Val Acc 54.080\n",
      "L 6 Epoch 0 Loss 5.370 Val Acc 9.340\n",
      "L 6 Epoch 1 Loss 3.055 Val Acc 10.000\n",
      "L 6 Epoch 2 Loss 2.347 Val Acc 10.600\n",
      "L 6 Epoch 3 Loss 2.181 Val Acc 17.340\n",
      "L 6 Epoch 4 Loss 2.004 Val Acc 16.860\n",
      "L 6 Epoch 5 Loss 1.921 Val Acc 19.320\n",
      "L 6 Epoch 6 Loss 1.879 Val Acc 20.220\n",
      "L 6 Epoch 7 Loss 1.731 Val Acc 24.960\n",
      "L 6 Epoch 8 Loss 1.722 Val Acc 29.060\n",
      "L 6 Epoch 9 Loss 1.707 Val Acc 32.940\n",
      "L 6 Epoch 10 Loss 1.637 Val Acc 35.820\n",
      "L 6 Epoch 11 Loss 1.670 Val Acc 34.480\n",
      "L 6 Epoch 12 Loss 1.646 Val Acc 32.320\n",
      "L 6 Epoch 13 Loss 1.579 Val Acc 35.200\n",
      "L 6 Epoch 14 Loss 1.496 Val Acc 36.200\n",
      "L 6 Epoch 15 Loss 1.528 Val Acc 36.460\n",
      "==> Saving model ...\n",
      "L 6 Epoch 16 Loss 1.464 Val Acc 40.100\n",
      "==> Saving model ...\n",
      "L 6 Epoch 17 Loss 1.441 Val Acc 40.780\n",
      "L 6 Epoch 18 Loss 1.396 Val Acc 38.520\n",
      "L 6 Epoch 19 Loss 1.458 Val Acc 38.160\n",
      "==> Saving model ...\n",
      "L 6 Epoch 20 Loss 1.321 Val Acc 41.560\n",
      "L 6 Epoch 21 Loss 1.252 Val Acc 40.780\n",
      "L 6 Epoch 22 Loss 1.272 Val Acc 37.400\n",
      "L 6 Epoch 23 Loss 1.218 Val Acc 41.220\n",
      "L 6 Epoch 24 Loss 1.150 Val Acc 40.820\n",
      "==> Saving model ...\n",
      "L 6 Epoch 25 Loss 1.108 Val Acc 42.060\n",
      "L 6 Epoch 26 Loss 1.184 Val Acc 40.460\n",
      "==> Saving model ...\n",
      "L 6 Epoch 27 Loss 1.111 Val Acc 42.500\n",
      "L 6 Epoch 28 Loss 1.133 Val Acc 39.600\n",
      "L 6 Epoch 29 Loss 1.152 Val Acc 38.980\n",
      "==> Saving model ...\n",
      "L 6 Epoch 30 Loss 1.046 Val Acc 44.200\n",
      "L 6 Epoch 31 Loss 0.968 Val Acc 43.940\n",
      "L 6 Epoch 32 Loss 0.872 Val Acc 43.720\n",
      "L 6 Epoch 33 Loss 0.814 Val Acc 43.220\n",
      "==> Saving model ...\n",
      "L 6 Epoch 34 Loss 0.908 Val Acc 44.440\n",
      "L 6 Epoch 35 Loss 0.932 Val Acc 41.460\n",
      "==> Saving model ...\n",
      "L 6 Epoch 36 Loss 0.898 Val Acc 44.520\n",
      "L 6 Epoch 37 Loss 0.881 Val Acc 42.900\n",
      "L 6 Epoch 38 Loss 0.839 Val Acc 41.820\n",
      "==> Saving model ...\n",
      "L 6 Epoch 39 Loss 0.813 Val Acc 44.780\n",
      "==> Saving model ...\n",
      "L 6 Epoch 40 Loss 0.777 Val Acc 45.540\n",
      "L 6 Epoch 41 Loss 0.770 Val Acc 44.260\n",
      "==> Saving model ...\n",
      "L 6 Epoch 42 Loss 0.626 Val Acc 46.960\n",
      "L 6 Epoch 43 Loss 0.641 Val Acc 44.940\n",
      "L 6 Epoch 44 Loss 0.644 Val Acc 45.180\n",
      "L 6 Epoch 45 Loss 0.454 Val Acc 43.860\n",
      "L 6 Epoch 46 Loss 0.475 Val Acc 44.240\n",
      "L 6 Epoch 47 Loss 0.601 Val Acc 45.820\n",
      "L 6 Epoch 48 Loss 0.560 Val Acc 44.640\n",
      "L 6 Epoch 49 Loss 0.564 Val Acc 43.360\n",
      "==> Saving model ...\n",
      "L 6 Epoch 50 Loss 0.593 Val Acc 47.420\n",
      "==> Saving model ...\n",
      "L 6 Epoch 51 Loss 0.397 Val Acc 47.440\n",
      "L 6 Epoch 52 Loss 0.617 Val Acc 45.140\n",
      "L 6 Epoch 53 Loss 0.375 Val Acc 45.100\n",
      "L 6 Epoch 54 Loss 0.358 Val Acc 43.880\n",
      "==> Saving model ...\n",
      "L 6 Epoch 55 Loss 0.408 Val Acc 47.580\n",
      "L 6 Epoch 56 Loss 0.442 Val Acc 46.500\n",
      "L 6 Epoch 57 Loss 0.464 Val Acc 43.880\n",
      "==> Saving model ...\n",
      "L 6 Epoch 58 Loss 0.346 Val Acc 49.340\n",
      "L 6 Epoch 59 Loss 0.270 Val Acc 48.080\n",
      "L 6 Epoch 60 Loss 0.186 Val Acc 47.920\n",
      "L 6 Epoch 61 Loss 0.219 Val Acc 49.320\n",
      "L 6 Epoch 62 Loss 0.193 Val Acc 47.640\n",
      "L 6 Epoch 63 Loss 0.217 Val Acc 48.900\n",
      "L 6 Epoch 64 Loss 0.219 Val Acc 48.540\n",
      "L 6 Epoch 65 Loss 0.367 Val Acc 48.760\n",
      "L 6 Epoch 66 Loss 0.285 Val Acc 46.780\n",
      "==> Saving model ...\n",
      "L 6 Epoch 67 Loss 0.263 Val Acc 49.520\n",
      "L 6 Epoch 68 Loss 0.246 Val Acc 48.080\n",
      "L 6 Epoch 69 Loss 0.223 Val Acc 46.800\n",
      "==> Saving model ...\n",
      "L 6 Epoch 70 Loss 0.236 Val Acc 50.380\n",
      "L 6 Epoch 71 Loss 0.173 Val Acc 50.060\n",
      "L 6 Epoch 72 Loss 0.205 Val Acc 47.960\n",
      "L 6 Epoch 73 Loss 0.223 Val Acc 48.200\n",
      "L 6 Epoch 74 Loss 0.249 Val Acc 49.100\n",
      "L 6 Epoch 75 Loss 0.192 Val Acc 48.580\n",
      "L 6 Epoch 76 Loss 0.275 Val Acc 46.320\n",
      "L 6 Epoch 77 Loss 0.229 Val Acc 48.520\n",
      "==> Saving model ...\n",
      "L 6 Epoch 78 Loss 0.210 Val Acc 51.780\n",
      "L 6 Epoch 79 Loss 0.202 Val Acc 49.160\n",
      "L 6 Epoch 80 Loss 0.171 Val Acc 48.420\n",
      "L 6 Epoch 81 Loss 0.112 Val Acc 51.480\n",
      "L 6 Epoch 82 Loss 0.151 Val Acc 49.720\n",
      "L 6 Epoch 83 Loss 0.084 Val Acc 50.040\n",
      "L 6 Epoch 84 Loss 0.080 Val Acc 51.140\n",
      "L 6 Epoch 85 Loss 0.093 Val Acc 49.660\n",
      "L 6 Epoch 86 Loss 0.076 Val Acc 50.340\n",
      "==> Saving model ...\n",
      "L 6 Epoch 87 Loss 0.157 Val Acc 52.520\n",
      "L 6 Epoch 88 Loss 0.152 Val Acc 50.560\n",
      "L 6 Epoch 89 Loss 0.133 Val Acc 48.440\n",
      "L 6 Epoch 90 Loss 0.166 Val Acc 50.680\n",
      "L 6 Epoch 91 Loss 0.130 Val Acc 49.860\n",
      "L 6 Epoch 92 Loss 0.119 Val Acc 49.500\n",
      "L 6 Epoch 93 Loss 0.083 Val Acc 50.080\n",
      "L 6 Epoch 94 Loss 0.106 Val Acc 50.540\n",
      "L 6 Epoch 95 Loss 0.113 Val Acc 51.360\n",
      "L 6 Epoch 96 Loss 0.102 Val Acc 50.980\n",
      "L 6 Epoch 97 Loss 0.089 Val Acc 50.580\n",
      "L 6 Epoch 98 Loss 0.141 Val Acc 49.840\n",
      "L 6 Epoch 99 Loss 0.084 Val Acc 49.640\n",
      "L 6 Epoch 100 Loss 0.093 Val Acc 50.920\n",
      "L 6 Epoch 101 Loss 0.079 Val Acc 50.340\n",
      "L 6 Epoch 102 Loss 0.078 Val Acc 50.400\n",
      "L 6 Epoch 103 Loss 0.106 Val Acc 51.600\n",
      "L 6 Epoch 104 Loss 0.038 Val Acc 50.980\n",
      "L 6 Epoch 105 Loss 0.062 Val Acc 51.540\n",
      "L 6 Epoch 106 Loss 0.075 Val Acc 51.060\n",
      "L 6 Epoch 107 Loss 0.060 Val Acc 51.860\n",
      "L 6 Epoch 108 Loss 0.072 Val Acc 52.020\n",
      "L 6 Epoch 109 Loss 0.045 Val Acc 52.380\n",
      "==> Saving model ...\n",
      "L 6 Epoch 110 Loss 0.091 Val Acc 52.700\n",
      "L 6 Epoch 111 Loss 0.088 Val Acc 51.820\n",
      "L 6 Epoch 112 Loss 0.050 Val Acc 50.780\n",
      "L 6 Epoch 113 Loss 0.058 Val Acc 51.200\n",
      "L 6 Epoch 114 Loss 0.061 Val Acc 52.240\n",
      "L 6 Epoch 115 Loss 0.058 Val Acc 52.080\n",
      "L 6 Epoch 116 Loss 0.042 Val Acc 52.080\n",
      "==> Saving model ...\n",
      "L 6 Epoch 117 Loss 0.056 Val Acc 52.940\n",
      "==> Saving model ...\n",
      "L 6 Epoch 118 Loss 0.060 Val Acc 53.740\n",
      "L 6 Epoch 119 Loss 0.045 Val Acc 53.520\n",
      "L 6 Epoch 120 Loss 0.038 Val Acc 53.180\n",
      "L 6 Epoch 121 Loss 0.048 Val Acc 53.300\n",
      "L 6 Epoch 122 Loss 0.049 Val Acc 52.900\n",
      "L 6 Epoch 123 Loss 0.052 Val Acc 53.360\n",
      "L 6 Epoch 124 Loss 0.063 Val Acc 52.420\n",
      "L 6 Epoch 125 Loss 0.035 Val Acc 52.200\n",
      "L 6 Epoch 126 Loss 0.059 Val Acc 52.360\n",
      "L 6 Epoch 127 Loss 0.050 Val Acc 52.600\n",
      "L 6 Epoch 128 Loss 0.027 Val Acc 52.860\n",
      "L 6 Epoch 129 Loss 0.056 Val Acc 52.400\n",
      "L 6 Epoch 130 Loss 0.047 Val Acc 51.320\n",
      "L 6 Epoch 131 Loss 0.032 Val Acc 52.160\n",
      "L 6 Epoch 132 Loss 0.054 Val Acc 53.000\n",
      "L 6 Epoch 133 Loss 0.024 Val Acc 53.520\n",
      "L 6 Epoch 134 Loss 0.038 Val Acc 53.380\n",
      "L 6 Epoch 135 Loss 0.021 Val Acc 53.040\n",
      "L 6 Epoch 136 Loss 0.014 Val Acc 53.020\n",
      "L 6 Epoch 137 Loss 0.044 Val Acc 53.060\n",
      "L 6 Epoch 138 Loss 0.025 Val Acc 52.400\n",
      "L 6 Epoch 139 Loss 0.036 Val Acc 52.020\n",
      "L 6 Epoch 140 Loss 0.024 Val Acc 52.500\n",
      "L 6 Epoch 141 Loss 0.014 Val Acc 52.260\n",
      "L 6 Epoch 142 Loss 0.031 Val Acc 52.940\n",
      "L 6 Epoch 143 Loss 0.019 Val Acc 52.800\n",
      "L 6 Epoch 144 Loss 0.028 Val Acc 52.840\n",
      "L 6 Epoch 145 Loss 0.032 Val Acc 53.160\n",
      "==> Saving model ...\n",
      "L 6 Epoch 146 Loss 0.006 Val Acc 53.920\n",
      "L 6 Epoch 147 Loss 0.026 Val Acc 53.680\n",
      "L 6 Epoch 148 Loss 0.007 Val Acc 53.480\n",
      "L 6 Epoch 149 Loss 0.035 Val Acc 53.220\n",
      "L 6 Epoch 150 Loss 0.018 Val Acc 53.760\n",
      "L 6 Epoch 151 Loss 0.007 Val Acc 53.840\n",
      "L 6 Epoch 152 Loss 0.030 Val Acc 53.280\n",
      "L 6 Epoch 153 Loss 0.022 Val Acc 53.360\n",
      "L 6 Epoch 154 Loss 0.017 Val Acc 53.180\n",
      "L 6 Epoch 155 Loss 0.042 Val Acc 52.860\n",
      "L 6 Epoch 156 Loss 0.021 Val Acc 52.780\n",
      "L 6 Epoch 157 Loss 0.032 Val Acc 52.780\n",
      "L 6 Epoch 158 Loss 0.031 Val Acc 53.120\n",
      "L 6 Epoch 159 Loss 0.018 Val Acc 53.380\n",
      "==> Saving model ...\n",
      "L 6 Epoch 160 Loss 0.017 Val Acc 53.940\n",
      "L 6 Epoch 161 Loss 0.020 Val Acc 53.220\n",
      "L 6 Epoch 162 Loss 0.025 Val Acc 52.680\n",
      "L 6 Epoch 163 Loss 0.018 Val Acc 52.920\n",
      "L 6 Epoch 164 Loss 0.016 Val Acc 53.020\n",
      "L 6 Epoch 165 Loss 0.021 Val Acc 52.820\n",
      "L 6 Epoch 166 Loss 0.026 Val Acc 53.620\n",
      "L 6 Epoch 167 Loss 0.025 Val Acc 53.520\n",
      "L 6 Epoch 168 Loss 0.005 Val Acc 53.760\n",
      "L 6 Epoch 169 Loss 0.015 Val Acc 53.880\n",
      "L 6 Epoch 170 Loss 0.018 Val Acc 53.680\n",
      "L 6 Epoch 171 Loss 0.010 Val Acc 53.700\n",
      "L 6 Epoch 172 Loss 0.010 Val Acc 53.940\n",
      "==> Saving model ...\n",
      "L 6 Epoch 173 Loss 0.004 Val Acc 54.100\n",
      "L 6 Epoch 174 Loss 0.014 Val Acc 53.940\n",
      "L 6 Epoch 175 Loss 0.008 Val Acc 53.760\n",
      "L 6 Epoch 176 Loss 0.008 Val Acc 53.900\n",
      "L 6 Epoch 177 Loss 0.017 Val Acc 53.820\n",
      "L 6 Epoch 178 Loss 0.009 Val Acc 53.840\n",
      "L 6 Epoch 179 Loss 0.012 Val Acc 53.700\n",
      "L 6 Epoch 180 Loss 0.020 Val Acc 53.340\n",
      "L 6 Epoch 181 Loss 0.011 Val Acc 53.240\n",
      "L 6 Epoch 182 Loss 0.021 Val Acc 53.580\n",
      "L 6 Epoch 183 Loss 0.017 Val Acc 53.680\n",
      "L 6 Epoch 184 Loss 0.012 Val Acc 53.560\n",
      "L 6 Epoch 185 Loss 0.020 Val Acc 53.680\n",
      "L 6 Epoch 186 Loss 0.006 Val Acc 53.780\n",
      "L 6 Epoch 187 Loss 0.014 Val Acc 53.660\n",
      "L 6 Epoch 188 Loss 0.014 Val Acc 53.540\n",
      "L 6 Epoch 189 Loss 0.010 Val Acc 53.680\n",
      "L 6 Epoch 190 Loss 0.019 Val Acc 53.560\n",
      "L 6 Epoch 191 Loss 0.007 Val Acc 53.660\n",
      "L 6 Epoch 192 Loss 0.024 Val Acc 53.220\n",
      "L 6 Epoch 193 Loss 0.017 Val Acc 53.120\n",
      "L 6 Epoch 194 Loss 0.015 Val Acc 53.360\n",
      "L 6 Epoch 195 Loss 0.011 Val Acc 53.580\n",
      "L 6 Epoch 196 Loss 0.011 Val Acc 53.620\n",
      "L 6 Epoch 197 Loss 0.018 Val Acc 53.740\n",
      "L 6 Epoch 198 Loss 0.008 Val Acc 53.540\n",
      "L 6 Epoch 199 Loss 0.021 Val Acc 53.780\n",
      "L 8 Epoch 0 Loss 3.917 Val Acc 10.000\n",
      "L 8 Epoch 1 Loss 2.762 Val Acc 10.060\n",
      "L 8 Epoch 2 Loss 2.069 Val Acc 8.860\n",
      "L 8 Epoch 3 Loss 1.985 Val Acc 15.400\n",
      "L 8 Epoch 4 Loss 2.001 Val Acc 12.580\n",
      "L 8 Epoch 5 Loss 1.889 Val Acc 18.980\n",
      "L 8 Epoch 6 Loss 1.810 Val Acc 21.900\n",
      "L 8 Epoch 7 Loss 1.735 Val Acc 27.100\n",
      "L 8 Epoch 8 Loss 1.673 Val Acc 35.300\n",
      "L 8 Epoch 9 Loss 1.628 Val Acc 36.080\n",
      "L 8 Epoch 10 Loss 1.604 Val Acc 34.380\n",
      "L 8 Epoch 11 Loss 1.608 Val Acc 34.120\n",
      "L 8 Epoch 12 Loss 1.568 Val Acc 33.920\n",
      "L 8 Epoch 13 Loss 1.575 Val Acc 37.420\n",
      "L 8 Epoch 14 Loss 1.483 Val Acc 34.220\n",
      "L 8 Epoch 15 Loss 1.486 Val Acc 37.760\n",
      "L 8 Epoch 16 Loss 1.416 Val Acc 37.800\n",
      "==> Saving model ...\n",
      "L 8 Epoch 17 Loss 1.386 Val Acc 41.380\n",
      "L 8 Epoch 18 Loss 1.379 Val Acc 40.620\n",
      "==> Saving model ...\n",
      "L 8 Epoch 19 Loss 1.268 Val Acc 41.460\n",
      "==> Saving model ...\n",
      "L 8 Epoch 20 Loss 1.218 Val Acc 42.820\n",
      "L 8 Epoch 21 Loss 1.211 Val Acc 39.280\n",
      "L 8 Epoch 22 Loss 1.232 Val Acc 36.800\n",
      "==> Saving model ...\n",
      "L 8 Epoch 23 Loss 1.091 Val Acc 43.280\n",
      "L 8 Epoch 24 Loss 1.134 Val Acc 42.600\n",
      "L 8 Epoch 25 Loss 1.122 Val Acc 43.160\n",
      "L 8 Epoch 26 Loss 1.110 Val Acc 42.820\n",
      "L 8 Epoch 27 Loss 1.138 Val Acc 40.700\n",
      "==> Saving model ...\n",
      "L 8 Epoch 28 Loss 1.032 Val Acc 46.500\n",
      "==> Saving model ...\n",
      "L 8 Epoch 29 Loss 0.984 Val Acc 49.160\n",
      "L 8 Epoch 30 Loss 0.937 Val Acc 41.460\n",
      "L 8 Epoch 31 Loss 0.923 Val Acc 48.340\n",
      "L 8 Epoch 32 Loss 0.831 Val Acc 48.100\n",
      "L 8 Epoch 33 Loss 0.863 Val Acc 48.120\n",
      "==> Saving model ...\n",
      "L 8 Epoch 34 Loss 0.732 Val Acc 50.360\n",
      "L 8 Epoch 35 Loss 0.747 Val Acc 43.820\n",
      "L 8 Epoch 36 Loss 0.754 Val Acc 48.740\n",
      "L 8 Epoch 37 Loss 0.746 Val Acc 48.960\n",
      "L 8 Epoch 38 Loss 0.543 Val Acc 46.040\n",
      "L 8 Epoch 39 Loss 0.682 Val Acc 43.480\n",
      "L 8 Epoch 40 Loss 0.563 Val Acc 49.800\n",
      "L 8 Epoch 41 Loss 0.599 Val Acc 45.580\n",
      "L 8 Epoch 42 Loss 0.448 Val Acc 45.640\n",
      "L 8 Epoch 43 Loss 0.399 Val Acc 49.680\n",
      "L 8 Epoch 44 Loss 0.489 Val Acc 49.120\n",
      "L 8 Epoch 45 Loss 0.433 Val Acc 48.300\n",
      "==> Saving model ...\n",
      "L 8 Epoch 46 Loss 0.394 Val Acc 50.380\n",
      "L 8 Epoch 47 Loss 0.376 Val Acc 45.960\n",
      "==> Saving model ...\n",
      "L 8 Epoch 48 Loss 0.270 Val Acc 51.180\n",
      "L 8 Epoch 49 Loss 0.402 Val Acc 49.660\n",
      "==> Saving model ...\n",
      "L 8 Epoch 50 Loss 0.394 Val Acc 51.720\n",
      "==> Saving model ...\n",
      "L 8 Epoch 51 Loss 0.368 Val Acc 53.320\n",
      "L 8 Epoch 52 Loss 0.399 Val Acc 51.160\n",
      "L 8 Epoch 53 Loss 0.311 Val Acc 52.280\n",
      "L 8 Epoch 54 Loss 0.310 Val Acc 49.640\n",
      "L 8 Epoch 55 Loss 0.292 Val Acc 52.820\n",
      "L 8 Epoch 56 Loss 0.361 Val Acc 50.680\n",
      "L 8 Epoch 57 Loss 0.344 Val Acc 49.140\n",
      "L 8 Epoch 58 Loss 0.341 Val Acc 52.020\n",
      "L 8 Epoch 59 Loss 0.174 Val Acc 52.500\n",
      "L 8 Epoch 60 Loss 0.195 Val Acc 52.740\n",
      "L 8 Epoch 61 Loss 0.205 Val Acc 52.340\n",
      "L 8 Epoch 62 Loss 0.256 Val Acc 52.300\n",
      "==> Saving model ...\n",
      "L 8 Epoch 63 Loss 0.229 Val Acc 55.500\n",
      "L 8 Epoch 64 Loss 0.229 Val Acc 51.580\n",
      "L 8 Epoch 65 Loss 0.156 Val Acc 54.660\n",
      "L 8 Epoch 66 Loss 0.191 Val Acc 54.760\n",
      "L 8 Epoch 67 Loss 0.255 Val Acc 52.420\n",
      "L 8 Epoch 68 Loss 0.205 Val Acc 49.820\n",
      "L 8 Epoch 69 Loss 0.173 Val Acc 54.240\n",
      "L 8 Epoch 70 Loss 0.085 Val Acc 55.140\n",
      "L 8 Epoch 71 Loss 0.175 Val Acc 53.360\n",
      "==> Saving model ...\n",
      "L 8 Epoch 72 Loss 0.117 Val Acc 55.660\n",
      "==> Saving model ...\n",
      "L 8 Epoch 73 Loss 0.127 Val Acc 55.820\n",
      "L 8 Epoch 74 Loss 0.134 Val Acc 54.500\n",
      "L 8 Epoch 75 Loss 0.103 Val Acc 53.660\n",
      "L 8 Epoch 76 Loss 0.097 Val Acc 53.240\n",
      "L 8 Epoch 77 Loss 0.116 Val Acc 53.240\n",
      "L 8 Epoch 78 Loss 0.175 Val Acc 53.740\n",
      "L 8 Epoch 79 Loss 0.076 Val Acc 54.840\n",
      "L 8 Epoch 80 Loss 0.097 Val Acc 55.020\n",
      "L 8 Epoch 81 Loss 0.090 Val Acc 54.580\n",
      "L 8 Epoch 82 Loss 0.081 Val Acc 55.820\n",
      "L 8 Epoch 83 Loss 0.100 Val Acc 53.960\n",
      "==> Saving model ...\n",
      "L 8 Epoch 84 Loss 0.060 Val Acc 56.720\n",
      "L 8 Epoch 85 Loss 0.078 Val Acc 56.200\n",
      "L 8 Epoch 86 Loss 0.098 Val Acc 56.300\n",
      "L 8 Epoch 87 Loss 0.073 Val Acc 56.720\n",
      "L 8 Epoch 88 Loss 0.019 Val Acc 55.960\n",
      "L 8 Epoch 89 Loss 0.094 Val Acc 56.140\n",
      "L 8 Epoch 90 Loss 0.069 Val Acc 55.520\n",
      "L 8 Epoch 91 Loss 0.082 Val Acc 55.260\n",
      "L 8 Epoch 92 Loss 0.087 Val Acc 55.120\n",
      "L 8 Epoch 93 Loss 0.067 Val Acc 54.440\n",
      "==> Saving model ...\n",
      "L 8 Epoch 94 Loss 0.096 Val Acc 57.160\n",
      "L 8 Epoch 95 Loss 0.061 Val Acc 53.480\n",
      "L 8 Epoch 96 Loss 0.111 Val Acc 55.000\n",
      "L 8 Epoch 97 Loss 0.061 Val Acc 54.980\n",
      "L 8 Epoch 98 Loss 0.057 Val Acc 56.780\n",
      "==> Saving model ...\n",
      "L 8 Epoch 99 Loss 0.054 Val Acc 57.240\n",
      "L 8 Epoch 100 Loss 0.061 Val Acc 55.800\n",
      "L 8 Epoch 101 Loss 0.047 Val Acc 56.780\n",
      "L 8 Epoch 102 Loss 0.059 Val Acc 56.240\n",
      "L 8 Epoch 103 Loss 0.077 Val Acc 55.920\n",
      "L 8 Epoch 104 Loss 0.066 Val Acc 56.100\n",
      "L 8 Epoch 105 Loss 0.037 Val Acc 57.020\n",
      "L 8 Epoch 106 Loss 0.072 Val Acc 56.420\n",
      "L 8 Epoch 107 Loss 0.036 Val Acc 56.040\n",
      "L 8 Epoch 108 Loss 0.046 Val Acc 57.200\n",
      "L 8 Epoch 109 Loss 0.048 Val Acc 57.240\n",
      "L 8 Epoch 110 Loss 0.035 Val Acc 56.420\n",
      "L 8 Epoch 111 Loss 0.047 Val Acc 56.560\n",
      "==> Saving model ...\n",
      "L 8 Epoch 112 Loss 0.047 Val Acc 57.600\n",
      "L 8 Epoch 113 Loss 0.043 Val Acc 55.980\n",
      "L 8 Epoch 114 Loss 0.019 Val Acc 57.300\n",
      "L 8 Epoch 115 Loss 0.030 Val Acc 57.540\n",
      "==> Saving model ...\n",
      "L 8 Epoch 116 Loss 0.046 Val Acc 58.000\n",
      "L 8 Epoch 117 Loss 0.051 Val Acc 57.980\n",
      "==> Saving model ...\n",
      "L 8 Epoch 118 Loss 0.030 Val Acc 58.640\n",
      "L 8 Epoch 119 Loss 0.020 Val Acc 58.100\n",
      "L 8 Epoch 120 Loss 0.033 Val Acc 58.020\n",
      "L 8 Epoch 121 Loss 0.018 Val Acc 58.200\n",
      "L 8 Epoch 122 Loss 0.024 Val Acc 58.640\n",
      "L 8 Epoch 123 Loss 0.022 Val Acc 58.540\n",
      "L 8 Epoch 124 Loss 0.017 Val Acc 58.620\n",
      "L 8 Epoch 125 Loss 0.019 Val Acc 58.580\n",
      "L 8 Epoch 126 Loss 0.039 Val Acc 57.820\n",
      "L 8 Epoch 127 Loss 0.038 Val Acc 57.980\n",
      "L 8 Epoch 128 Loss 0.034 Val Acc 56.760\n",
      "L 8 Epoch 129 Loss 0.021 Val Acc 57.240\n",
      "L 8 Epoch 130 Loss 0.028 Val Acc 58.040\n",
      "L 8 Epoch 131 Loss 0.039 Val Acc 57.560\n",
      "L 8 Epoch 132 Loss 0.028 Val Acc 57.360\n",
      "L 8 Epoch 133 Loss 0.021 Val Acc 57.660\n",
      "L 8 Epoch 134 Loss 0.049 Val Acc 58.000\n",
      "L 8 Epoch 135 Loss 0.021 Val Acc 58.000\n",
      "L 8 Epoch 136 Loss 0.033 Val Acc 57.860\n",
      "L 8 Epoch 137 Loss 0.023 Val Acc 57.620\n",
      "L 8 Epoch 138 Loss 0.023 Val Acc 57.820\n",
      "L 8 Epoch 139 Loss 0.042 Val Acc 58.220\n",
      "L 8 Epoch 140 Loss 0.035 Val Acc 58.540\n",
      "==> Saving model ...\n",
      "L 8 Epoch 141 Loss 0.020 Val Acc 59.580\n",
      "L 8 Epoch 142 Loss 0.021 Val Acc 59.580\n",
      "==> Saving model ...\n",
      "L 8 Epoch 143 Loss 0.018 Val Acc 59.780\n",
      "L 8 Epoch 144 Loss 0.027 Val Acc 58.520\n",
      "L 8 Epoch 145 Loss 0.009 Val Acc 58.520\n",
      "L 8 Epoch 146 Loss 0.006 Val Acc 59.460\n",
      "L 8 Epoch 147 Loss 0.027 Val Acc 58.940\n",
      "L 8 Epoch 148 Loss 0.025 Val Acc 58.420\n",
      "L 8 Epoch 149 Loss 0.019 Val Acc 57.980\n",
      "L 8 Epoch 150 Loss 0.040 Val Acc 57.980\n",
      "L 8 Epoch 151 Loss 0.006 Val Acc 58.680\n",
      "L 8 Epoch 152 Loss 0.014 Val Acc 59.000\n",
      "L 8 Epoch 153 Loss 0.026 Val Acc 58.200\n",
      "L 8 Epoch 154 Loss 0.021 Val Acc 58.520\n",
      "L 8 Epoch 155 Loss 0.015 Val Acc 59.320\n",
      "L 8 Epoch 156 Loss 0.015 Val Acc 59.300\n",
      "L 8 Epoch 157 Loss 0.023 Val Acc 58.920\n",
      "L 8 Epoch 158 Loss 0.008 Val Acc 59.240\n",
      "L 8 Epoch 159 Loss 0.008 Val Acc 59.420\n",
      "L 8 Epoch 160 Loss 0.019 Val Acc 59.300\n",
      "L 8 Epoch 161 Loss 0.016 Val Acc 58.920\n",
      "L 8 Epoch 162 Loss 0.009 Val Acc 59.080\n",
      "L 8 Epoch 163 Loss 0.012 Val Acc 58.960\n",
      "L 8 Epoch 164 Loss 0.019 Val Acc 58.580\n",
      "L 8 Epoch 165 Loss 0.011 Val Acc 58.660\n",
      "L 8 Epoch 166 Loss 0.012 Val Acc 59.380\n",
      "L 8 Epoch 167 Loss 0.010 Val Acc 59.100\n",
      "L 8 Epoch 168 Loss 0.011 Val Acc 59.240\n",
      "L 8 Epoch 169 Loss 0.009 Val Acc 58.900\n",
      "L 8 Epoch 170 Loss 0.018 Val Acc 58.820\n",
      "L 8 Epoch 171 Loss 0.016 Val Acc 58.380\n",
      "L 8 Epoch 172 Loss 0.007 Val Acc 59.080\n",
      "L 8 Epoch 173 Loss 0.017 Val Acc 58.540\n",
      "L 8 Epoch 174 Loss 0.017 Val Acc 58.580\n",
      "L 8 Epoch 175 Loss 0.014 Val Acc 58.880\n",
      "L 8 Epoch 176 Loss 0.005 Val Acc 59.120\n",
      "L 8 Epoch 177 Loss 0.002 Val Acc 59.720\n",
      "L 8 Epoch 178 Loss 0.011 Val Acc 59.380\n",
      "L 8 Epoch 179 Loss 0.011 Val Acc 58.980\n",
      "L 8 Epoch 180 Loss 0.011 Val Acc 58.020\n",
      "L 8 Epoch 181 Loss 0.012 Val Acc 58.940\n",
      "L 8 Epoch 182 Loss 0.011 Val Acc 58.960\n",
      "L 8 Epoch 183 Loss 0.007 Val Acc 59.220\n",
      "L 8 Epoch 184 Loss 0.012 Val Acc 59.000\n",
      "L 8 Epoch 185 Loss 0.027 Val Acc 58.060\n",
      "L 8 Epoch 186 Loss 0.014 Val Acc 58.760\n",
      "L 8 Epoch 187 Loss 0.005 Val Acc 59.420\n",
      "L 8 Epoch 188 Loss 0.010 Val Acc 59.080\n",
      "L 8 Epoch 189 Loss 0.005 Val Acc 59.520\n",
      "L 8 Epoch 190 Loss 0.012 Val Acc 59.180\n",
      "L 8 Epoch 191 Loss 0.009 Val Acc 59.380\n",
      "L 8 Epoch 192 Loss 0.009 Val Acc 58.620\n",
      "L 8 Epoch 193 Loss 0.004 Val Acc 59.200\n",
      "L 8 Epoch 194 Loss 0.009 Val Acc 59.340\n",
      "L 8 Epoch 195 Loss 0.007 Val Acc 59.180\n",
      "L 8 Epoch 196 Loss 0.006 Val Acc 59.420\n",
      "L 8 Epoch 197 Loss 0.006 Val Acc 59.640\n",
      "L 8 Epoch 198 Loss 0.006 Val Acc 59.680\n",
      "L 8 Epoch 199 Loss 0.019 Val Acc 59.340\n",
      "L 10 Epoch 0 Loss 6.083 Val Acc 10.000\n",
      "L 10 Epoch 1 Loss 3.896 Val Acc 10.080\n",
      "L 10 Epoch 2 Loss 2.707 Val Acc 11.680\n",
      "L 10 Epoch 3 Loss 2.175 Val Acc 15.420\n",
      "L 10 Epoch 4 Loss 2.043 Val Acc 18.820\n",
      "L 10 Epoch 5 Loss 1.887 Val Acc 23.820\n",
      "L 10 Epoch 6 Loss 1.803 Val Acc 30.140\n",
      "L 10 Epoch 7 Loss 1.719 Val Acc 30.760\n",
      "L 10 Epoch 8 Loss 1.717 Val Acc 32.580\n",
      "L 10 Epoch 9 Loss 1.659 Val Acc 35.380\n",
      "L 10 Epoch 10 Loss 1.596 Val Acc 34.740\n",
      "L 10 Epoch 11 Loss 1.580 Val Acc 35.460\n",
      "L 10 Epoch 12 Loss 1.591 Val Acc 36.720\n",
      "L 10 Epoch 13 Loss 1.527 Val Acc 38.160\n",
      "L 10 Epoch 14 Loss 1.477 Val Acc 39.040\n",
      "==> Saving model ...\n",
      "L 10 Epoch 15 Loss 1.483 Val Acc 40.080\n",
      "L 10 Epoch 16 Loss 1.473 Val Acc 39.800\n",
      "==> Saving model ...\n",
      "L 10 Epoch 17 Loss 1.387 Val Acc 40.440\n",
      "L 10 Epoch 18 Loss 1.390 Val Acc 40.060\n",
      "==> Saving model ...\n",
      "L 10 Epoch 19 Loss 1.393 Val Acc 40.840\n",
      "==> Saving model ...\n",
      "L 10 Epoch 20 Loss 1.362 Val Acc 41.220\n",
      "==> Saving model ...\n",
      "L 10 Epoch 21 Loss 1.304 Val Acc 41.620\n",
      "L 10 Epoch 22 Loss 1.270 Val Acc 40.740\n",
      "L 10 Epoch 23 Loss 1.306 Val Acc 40.180\n",
      "L 10 Epoch 24 Loss 1.228 Val Acc 41.160\n",
      "L 10 Epoch 25 Loss 1.193 Val Acc 39.320\n",
      "==> Saving model ...\n",
      "L 10 Epoch 26 Loss 1.161 Val Acc 43.720\n",
      "L 10 Epoch 27 Loss 1.178 Val Acc 41.800\n",
      "==> Saving model ...\n",
      "L 10 Epoch 28 Loss 1.147 Val Acc 44.740\n",
      "L 10 Epoch 29 Loss 1.114 Val Acc 40.700\n",
      "==> Saving model ...\n",
      "L 10 Epoch 30 Loss 1.118 Val Acc 45.400\n",
      "L 10 Epoch 31 Loss 1.029 Val Acc 43.920\n",
      "L 10 Epoch 32 Loss 0.981 Val Acc 40.780\n",
      "L 10 Epoch 33 Loss 1.050 Val Acc 42.000\n",
      "L 10 Epoch 34 Loss 0.974 Val Acc 41.440\n",
      "L 10 Epoch 35 Loss 0.930 Val Acc 44.860\n",
      "L 10 Epoch 36 Loss 0.933 Val Acc 43.940\n",
      "L 10 Epoch 37 Loss 0.750 Val Acc 45.360\n",
      "L 10 Epoch 38 Loss 0.814 Val Acc 40.440\n",
      "==> Saving model ...\n",
      "L 10 Epoch 39 Loss 0.725 Val Acc 45.660\n",
      "L 10 Epoch 40 Loss 0.800 Val Acc 44.840\n",
      "L 10 Epoch 41 Loss 0.778 Val Acc 43.960\n",
      "L 10 Epoch 42 Loss 0.750 Val Acc 43.200\n",
      "==> Saving model ...\n",
      "L 10 Epoch 43 Loss 0.570 Val Acc 47.500\n",
      "L 10 Epoch 44 Loss 0.651 Val Acc 43.920\n",
      "L 10 Epoch 45 Loss 0.630 Val Acc 46.500\n",
      "L 10 Epoch 46 Loss 0.521 Val Acc 46.400\n",
      "L 10 Epoch 47 Loss 0.458 Val Acc 46.760\n",
      "L 10 Epoch 48 Loss 0.717 Val Acc 46.920\n",
      "==> Saving model ...\n",
      "L 10 Epoch 49 Loss 0.622 Val Acc 47.740\n",
      "==> Saving model ...\n",
      "L 10 Epoch 50 Loss 0.476 Val Acc 48.520\n",
      "L 10 Epoch 51 Loss 0.379 Val Acc 47.120\n",
      "L 10 Epoch 52 Loss 0.374 Val Acc 45.500\n",
      "==> Saving model ...\n",
      "L 10 Epoch 53 Loss 0.311 Val Acc 48.560\n",
      "L 10 Epoch 54 Loss 0.398 Val Acc 46.680\n",
      "L 10 Epoch 55 Loss 0.385 Val Acc 47.720\n",
      "==> Saving model ...\n",
      "L 10 Epoch 56 Loss 0.368 Val Acc 48.820\n",
      "L 10 Epoch 57 Loss 0.345 Val Acc 47.100\n",
      "L 10 Epoch 58 Loss 0.487 Val Acc 45.680\n",
      "L 10 Epoch 59 Loss 0.414 Val Acc 48.060\n",
      "==> Saving model ...\n",
      "L 10 Epoch 60 Loss 0.332 Val Acc 48.900\n",
      "L 10 Epoch 61 Loss 0.296 Val Acc 48.840\n",
      "==> Saving model ...\n",
      "L 10 Epoch 62 Loss 0.171 Val Acc 51.080\n",
      "L 10 Epoch 63 Loss 0.353 Val Acc 47.720\n",
      "L 10 Epoch 64 Loss 0.292 Val Acc 48.060\n",
      "L 10 Epoch 65 Loss 0.289 Val Acc 47.940\n",
      "L 10 Epoch 66 Loss 0.302 Val Acc 47.400\n",
      "L 10 Epoch 67 Loss 0.271 Val Acc 47.580\n",
      "L 10 Epoch 68 Loss 0.212 Val Acc 48.880\n",
      "L 10 Epoch 69 Loss 0.172 Val Acc 50.720\n",
      "L 10 Epoch 70 Loss 0.202 Val Acc 49.700\n",
      "L 10 Epoch 71 Loss 0.134 Val Acc 49.820\n",
      "L 10 Epoch 72 Loss 0.219 Val Acc 49.140\n",
      "L 10 Epoch 73 Loss 0.191 Val Acc 50.280\n",
      "L 10 Epoch 74 Loss 0.128 Val Acc 47.480\n",
      "L 10 Epoch 75 Loss 0.195 Val Acc 50.980\n",
      "==> Saving model ...\n",
      "L 10 Epoch 76 Loss 0.168 Val Acc 51.400\n",
      "L 10 Epoch 77 Loss 0.171 Val Acc 51.380\n",
      "L 10 Epoch 78 Loss 0.144 Val Acc 50.040\n",
      "L 10 Epoch 79 Loss 0.144 Val Acc 50.980\n",
      "L 10 Epoch 80 Loss 0.145 Val Acc 50.400\n",
      "L 10 Epoch 81 Loss 0.106 Val Acc 50.440\n",
      "L 10 Epoch 82 Loss 0.129 Val Acc 51.220\n",
      "L 10 Epoch 83 Loss 0.093 Val Acc 51.180\n",
      "L 10 Epoch 84 Loss 0.175 Val Acc 51.380\n",
      "L 10 Epoch 85 Loss 0.147 Val Acc 50.700\n",
      "==> Saving model ...\n",
      "L 10 Epoch 86 Loss 0.108 Val Acc 51.580\n",
      "L 10 Epoch 87 Loss 0.115 Val Acc 50.720\n",
      "L 10 Epoch 88 Loss 0.132 Val Acc 49.260\n",
      "==> Saving model ...\n",
      "L 10 Epoch 89 Loss 0.118 Val Acc 52.040\n",
      "L 10 Epoch 90 Loss 0.105 Val Acc 51.380\n",
      "==> Saving model ...\n",
      "L 10 Epoch 91 Loss 0.176 Val Acc 52.180\n",
      "L 10 Epoch 92 Loss 0.096 Val Acc 51.360\n",
      "L 10 Epoch 93 Loss 0.148 Val Acc 50.360\n",
      "L 10 Epoch 94 Loss 0.080 Val Acc 50.880\n",
      "L 10 Epoch 95 Loss 0.047 Val Acc 52.040\n",
      "L 10 Epoch 96 Loss 0.107 Val Acc 51.380\n",
      "L 10 Epoch 97 Loss 0.117 Val Acc 51.560\n",
      "L 10 Epoch 98 Loss 0.093 Val Acc 50.860\n",
      "==> Saving model ...\n",
      "L 10 Epoch 99 Loss 0.068 Val Acc 53.060\n",
      "L 10 Epoch 100 Loss 0.138 Val Acc 52.020\n",
      "L 10 Epoch 101 Loss 0.066 Val Acc 52.500\n",
      "L 10 Epoch 102 Loss 0.094 Val Acc 52.760\n",
      "L 10 Epoch 103 Loss 0.114 Val Acc 52.040\n",
      "L 10 Epoch 104 Loss 0.073 Val Acc 52.480\n",
      "L 10 Epoch 105 Loss 0.079 Val Acc 52.720\n",
      "L 10 Epoch 106 Loss 0.057 Val Acc 52.460\n",
      "L 10 Epoch 107 Loss 0.064 Val Acc 51.820\n",
      "L 10 Epoch 108 Loss 0.062 Val Acc 50.920\n",
      "L 10 Epoch 109 Loss 0.101 Val Acc 52.740\n",
      "L 10 Epoch 110 Loss 0.081 Val Acc 52.400\n",
      "==> Saving model ...\n",
      "L 10 Epoch 111 Loss 0.070 Val Acc 53.840\n",
      "L 10 Epoch 112 Loss 0.061 Val Acc 53.640\n",
      "L 10 Epoch 113 Loss 0.077 Val Acc 52.960\n",
      "L 10 Epoch 114 Loss 0.049 Val Acc 52.400\n",
      "L 10 Epoch 115 Loss 0.097 Val Acc 53.280\n",
      "L 10 Epoch 116 Loss 0.069 Val Acc 53.000\n",
      "L 10 Epoch 117 Loss 0.048 Val Acc 52.980\n",
      "L 10 Epoch 118 Loss 0.054 Val Acc 53.220\n",
      "L 10 Epoch 119 Loss 0.030 Val Acc 52.200\n",
      "L 10 Epoch 120 Loss 0.051 Val Acc 52.520\n",
      "L 10 Epoch 121 Loss 0.069 Val Acc 53.200\n",
      "L 10 Epoch 122 Loss 0.046 Val Acc 53.020\n",
      "L 10 Epoch 123 Loss 0.054 Val Acc 52.840\n",
      "L 10 Epoch 124 Loss 0.046 Val Acc 52.660\n",
      "L 10 Epoch 125 Loss 0.034 Val Acc 53.040\n",
      "==> Saving model ...\n",
      "L 10 Epoch 126 Loss 0.027 Val Acc 53.860\n",
      "L 10 Epoch 127 Loss 0.029 Val Acc 53.860\n",
      "L 10 Epoch 128 Loss 0.030 Val Acc 53.460\n",
      "L 10 Epoch 129 Loss 0.030 Val Acc 53.700\n",
      "==> Saving model ...\n",
      "L 10 Epoch 130 Loss 0.030 Val Acc 54.160\n",
      "==> Saving model ...\n",
      "L 10 Epoch 131 Loss 0.013 Val Acc 54.440\n",
      "L 10 Epoch 132 Loss 0.027 Val Acc 54.280\n",
      "==> Saving model ...\n",
      "L 10 Epoch 133 Loss 0.023 Val Acc 54.600\n",
      "L 10 Epoch 134 Loss 0.018 Val Acc 54.060\n",
      "L 10 Epoch 135 Loss 0.028 Val Acc 53.580\n",
      "L 10 Epoch 136 Loss 0.024 Val Acc 53.540\n",
      "L 10 Epoch 137 Loss 0.034 Val Acc 53.660\n",
      "L 10 Epoch 138 Loss 0.019 Val Acc 53.740\n",
      "L 10 Epoch 139 Loss 0.024 Val Acc 53.760\n",
      "L 10 Epoch 140 Loss 0.024 Val Acc 53.920\n",
      "L 10 Epoch 141 Loss 0.024 Val Acc 53.580\n",
      "L 10 Epoch 142 Loss 0.017 Val Acc 53.660\n",
      "L 10 Epoch 143 Loss 0.027 Val Acc 53.140\n",
      "L 10 Epoch 144 Loss 0.027 Val Acc 53.140\n",
      "L 10 Epoch 145 Loss 0.017 Val Acc 53.360\n",
      "L 10 Epoch 146 Loss 0.020 Val Acc 53.580\n",
      "L 10 Epoch 147 Loss 0.020 Val Acc 53.780\n",
      "L 10 Epoch 148 Loss 0.016 Val Acc 53.880\n",
      "L 10 Epoch 149 Loss 0.029 Val Acc 53.380\n",
      "L 10 Epoch 150 Loss 0.022 Val Acc 53.740\n",
      "L 10 Epoch 151 Loss 0.035 Val Acc 53.720\n",
      "L 10 Epoch 152 Loss 0.016 Val Acc 54.340\n",
      "L 10 Epoch 153 Loss 0.022 Val Acc 53.940\n",
      "L 10 Epoch 154 Loss 0.019 Val Acc 53.900\n",
      "L 10 Epoch 155 Loss 0.025 Val Acc 54.260\n",
      "==> Saving model ...\n",
      "L 10 Epoch 156 Loss 0.025 Val Acc 54.960\n",
      "L 10 Epoch 157 Loss 0.022 Val Acc 54.960\n",
      "L 10 Epoch 158 Loss 0.026 Val Acc 54.660\n",
      "L 10 Epoch 159 Loss 0.023 Val Acc 54.840\n",
      "L 10 Epoch 160 Loss 0.020 Val Acc 54.520\n",
      "L 10 Epoch 161 Loss 0.009 Val Acc 54.920\n",
      "L 10 Epoch 162 Loss 0.022 Val Acc 54.920\n",
      "L 10 Epoch 163 Loss 0.017 Val Acc 54.780\n",
      "L 10 Epoch 164 Loss 0.032 Val Acc 54.300\n",
      "L 10 Epoch 165 Loss 0.013 Val Acc 54.640\n",
      "L 10 Epoch 166 Loss 0.022 Val Acc 54.260\n",
      "L 10 Epoch 167 Loss 0.011 Val Acc 54.720\n",
      "L 10 Epoch 168 Loss 0.014 Val Acc 54.600\n",
      "L 10 Epoch 169 Loss 0.023 Val Acc 54.560\n",
      "L 10 Epoch 170 Loss 0.016 Val Acc 54.680\n",
      "L 10 Epoch 171 Loss 0.008 Val Acc 54.920\n",
      "L 10 Epoch 172 Loss 0.010 Val Acc 54.960\n",
      "==> Saving model ...\n",
      "L 10 Epoch 173 Loss 0.004 Val Acc 55.020\n",
      "L 10 Epoch 174 Loss 0.014 Val Acc 54.740\n",
      "L 10 Epoch 175 Loss 0.022 Val Acc 54.720\n",
      "L 10 Epoch 176 Loss 0.017 Val Acc 54.760\n",
      "==> Saving model ...\n",
      "L 10 Epoch 177 Loss 0.012 Val Acc 55.060\n",
      "L 10 Epoch 178 Loss 0.028 Val Acc 54.660\n",
      "==> Saving model ...\n",
      "L 10 Epoch 179 Loss 0.007 Val Acc 55.220\n",
      "==> Saving model ...\n",
      "L 10 Epoch 180 Loss 0.009 Val Acc 55.340\n",
      "L 10 Epoch 181 Loss 0.018 Val Acc 55.320\n",
      "L 10 Epoch 182 Loss 0.025 Val Acc 54.340\n",
      "L 10 Epoch 183 Loss 0.020 Val Acc 54.460\n",
      "L 10 Epoch 184 Loss 0.016 Val Acc 54.680\n",
      "L 10 Epoch 185 Loss 0.012 Val Acc 54.760\n",
      "L 10 Epoch 186 Loss 0.013 Val Acc 54.900\n",
      "L 10 Epoch 187 Loss 0.011 Val Acc 55.120\n",
      "L 10 Epoch 188 Loss 0.011 Val Acc 54.740\n",
      "L 10 Epoch 189 Loss 0.023 Val Acc 54.260\n",
      "L 10 Epoch 190 Loss 0.004 Val Acc 55.060\n",
      "L 10 Epoch 191 Loss 0.008 Val Acc 55.080\n",
      "L 10 Epoch 192 Loss 0.013 Val Acc 55.100\n",
      "L 10 Epoch 193 Loss 0.007 Val Acc 55.020\n",
      "L 10 Epoch 194 Loss 0.007 Val Acc 55.100\n",
      "L 10 Epoch 195 Loss 0.023 Val Acc 54.960\n",
      "L 10 Epoch 196 Loss 0.010 Val Acc 54.860\n",
      "L 10 Epoch 197 Loss 0.015 Val Acc 54.500\n",
      "L 10 Epoch 198 Loss 0.012 Val Acc 54.900\n",
      "L 10 Epoch 199 Loss 0.019 Val Acc 54.920\n",
      "L 4 Epoch 0 Loss 4.283 Val Acc 10.000\n",
      "L 4 Epoch 1 Loss 3.944 Val Acc 10.000\n",
      "L 4 Epoch 2 Loss 2.767 Val Acc 10.360\n",
      "L 4 Epoch 3 Loss 2.423 Val Acc 12.440\n",
      "L 4 Epoch 4 Loss 2.672 Val Acc 16.720\n",
      "L 4 Epoch 5 Loss 2.504 Val Acc 19.100\n",
      "L 4 Epoch 6 Loss 2.279 Val Acc 21.100\n",
      "L 4 Epoch 7 Loss 2.158 Val Acc 20.400\n",
      "L 4 Epoch 8 Loss 1.997 Val Acc 25.180\n",
      "L 4 Epoch 9 Loss 1.877 Val Acc 28.480\n",
      "L 4 Epoch 10 Loss 1.824 Val Acc 31.860\n",
      "L 4 Epoch 11 Loss 1.735 Val Acc 27.720\n",
      "L 4 Epoch 12 Loss 1.699 Val Acc 33.760\n",
      "L 4 Epoch 13 Loss 1.677 Val Acc 35.220\n",
      "L 4 Epoch 14 Loss 1.633 Val Acc 35.380\n",
      "L 4 Epoch 15 Loss 1.565 Val Acc 37.000\n",
      "L 4 Epoch 16 Loss 1.563 Val Acc 39.340\n",
      "L 4 Epoch 17 Loss 1.506 Val Acc 38.120\n",
      "L 4 Epoch 18 Loss 1.477 Val Acc 38.520\n",
      "L 4 Epoch 19 Loss 1.477 Val Acc 39.640\n",
      "==> Saving model ...\n",
      "L 4 Epoch 20 Loss 1.417 Val Acc 40.580\n",
      "L 4 Epoch 21 Loss 1.364 Val Acc 39.920\n",
      "==> Saving model ...\n",
      "L 4 Epoch 22 Loss 1.307 Val Acc 41.980\n",
      "L 4 Epoch 23 Loss 1.303 Val Acc 39.380\n",
      "==> Saving model ...\n",
      "L 4 Epoch 24 Loss 1.256 Val Acc 43.700\n",
      "L 4 Epoch 25 Loss 1.267 Val Acc 41.960\n",
      "==> Saving model ...\n",
      "L 4 Epoch 26 Loss 1.220 Val Acc 45.380\n",
      "L 4 Epoch 27 Loss 1.146 Val Acc 42.080\n",
      "==> Saving model ...\n",
      "L 4 Epoch 28 Loss 1.032 Val Acc 46.080\n",
      "L 4 Epoch 29 Loss 1.136 Val Acc 41.640\n",
      "L 4 Epoch 30 Loss 1.068 Val Acc 39.520\n",
      "L 4 Epoch 31 Loss 0.959 Val Acc 45.920\n",
      "L 4 Epoch 32 Loss 0.992 Val Acc 41.320\n",
      "L 4 Epoch 33 Loss 0.974 Val Acc 44.120\n",
      "L 4 Epoch 34 Loss 0.993 Val Acc 44.980\n",
      "L 4 Epoch 35 Loss 0.910 Val Acc 44.620\n",
      "L 4 Epoch 36 Loss 0.860 Val Acc 44.960\n",
      "==> Saving model ...\n",
      "L 4 Epoch 37 Loss 0.898 Val Acc 46.100\n",
      "==> Saving model ...\n",
      "L 4 Epoch 38 Loss 0.875 Val Acc 48.960\n",
      "L 4 Epoch 39 Loss 0.733 Val Acc 48.060\n",
      "L 4 Epoch 40 Loss 0.658 Val Acc 47.600\n",
      "L 4 Epoch 41 Loss 0.632 Val Acc 45.440\n",
      "L 4 Epoch 42 Loss 0.677 Val Acc 44.600\n",
      "L 4 Epoch 43 Loss 0.713 Val Acc 48.640\n",
      "L 4 Epoch 44 Loss 0.668 Val Acc 43.360\n",
      "L 4 Epoch 45 Loss 0.584 Val Acc 48.120\n",
      "L 4 Epoch 46 Loss 0.584 Val Acc 47.160\n",
      "L 4 Epoch 47 Loss 0.521 Val Acc 47.040\n",
      "L 4 Epoch 48 Loss 0.524 Val Acc 44.820\n",
      "L 4 Epoch 49 Loss 0.460 Val Acc 46.480\n",
      "L 4 Epoch 50 Loss 0.386 Val Acc 48.600\n",
      "==> Saving model ...\n",
      "L 4 Epoch 51 Loss 0.411 Val Acc 49.940\n",
      "L 4 Epoch 52 Loss 0.427 Val Acc 45.180\n",
      "L 4 Epoch 53 Loss 0.396 Val Acc 49.540\n",
      "L 4 Epoch 54 Loss 0.329 Val Acc 46.380\n",
      "L 4 Epoch 55 Loss 0.361 Val Acc 49.580\n",
      "L 4 Epoch 56 Loss 0.321 Val Acc 49.420\n",
      "==> Saving model ...\n",
      "L 4 Epoch 57 Loss 0.254 Val Acc 50.020\n",
      "==> Saving model ...\n",
      "L 4 Epoch 58 Loss 0.356 Val Acc 50.740\n",
      "L 4 Epoch 59 Loss 0.258 Val Acc 49.740\n",
      "L 4 Epoch 60 Loss 0.286 Val Acc 49.560\n",
      "L 4 Epoch 61 Loss 0.144 Val Acc 50.280\n",
      "L 4 Epoch 62 Loss 0.194 Val Acc 49.820\n",
      "==> Saving model ...\n",
      "L 4 Epoch 63 Loss 0.177 Val Acc 51.540\n",
      "L 4 Epoch 64 Loss 0.223 Val Acc 49.960\n",
      "L 4 Epoch 65 Loss 0.152 Val Acc 51.000\n",
      "L 4 Epoch 66 Loss 0.186 Val Acc 50.760\n",
      "L 4 Epoch 67 Loss 0.170 Val Acc 50.500\n",
      "L 4 Epoch 68 Loss 0.148 Val Acc 50.420\n",
      "L 4 Epoch 69 Loss 0.145 Val Acc 49.940\n",
      "L 4 Epoch 70 Loss 0.218 Val Acc 49.280\n",
      "L 4 Epoch 71 Loss 0.267 Val Acc 50.380\n",
      "L 4 Epoch 72 Loss 0.256 Val Acc 47.600\n",
      "L 4 Epoch 73 Loss 0.271 Val Acc 46.500\n",
      "L 4 Epoch 74 Loss 0.205 Val Acc 48.820\n",
      "L 4 Epoch 75 Loss 0.194 Val Acc 49.920\n",
      "L 4 Epoch 76 Loss 0.169 Val Acc 49.780\n",
      "==> Saving model ...\n",
      "L 4 Epoch 77 Loss 0.133 Val Acc 51.600\n",
      "==> Saving model ...\n",
      "L 4 Epoch 78 Loss 0.077 Val Acc 51.620\n",
      "==> Saving model ...\n",
      "L 4 Epoch 79 Loss 0.086 Val Acc 51.800\n",
      "L 4 Epoch 80 Loss 0.161 Val Acc 50.980\n",
      "L 4 Epoch 81 Loss 0.100 Val Acc 51.400\n",
      "==> Saving model ...\n",
      "L 4 Epoch 82 Loss 0.142 Val Acc 52.320\n",
      "L 4 Epoch 83 Loss 0.165 Val Acc 51.620\n",
      "L 4 Epoch 84 Loss 0.106 Val Acc 52.200\n",
      "==> Saving model ...\n",
      "L 4 Epoch 85 Loss 0.079 Val Acc 52.400\n",
      "L 4 Epoch 86 Loss 0.074 Val Acc 52.180\n",
      "L 4 Epoch 87 Loss 0.149 Val Acc 50.200\n",
      "L 4 Epoch 88 Loss 0.118 Val Acc 49.640\n",
      "L 4 Epoch 89 Loss 0.134 Val Acc 52.260\n",
      "L 4 Epoch 90 Loss 0.082 Val Acc 52.080\n",
      "L 4 Epoch 91 Loss 0.063 Val Acc 51.580\n",
      "L 4 Epoch 92 Loss 0.100 Val Acc 51.400\n",
      "==> Saving model ...\n",
      "L 4 Epoch 93 Loss 0.055 Val Acc 53.140\n",
      "L 4 Epoch 94 Loss 0.049 Val Acc 50.840\n",
      "L 4 Epoch 95 Loss 0.054 Val Acc 52.760\n",
      "==> Saving model ...\n",
      "L 4 Epoch 96 Loss 0.076 Val Acc 53.280\n",
      "L 4 Epoch 97 Loss 0.103 Val Acc 51.920\n",
      "L 4 Epoch 98 Loss 0.067 Val Acc 51.040\n",
      "L 4 Epoch 99 Loss 0.100 Val Acc 51.560\n",
      "L 4 Epoch 100 Loss 0.092 Val Acc 52.360\n",
      "L 4 Epoch 101 Loss 0.096 Val Acc 50.520\n",
      "L 4 Epoch 102 Loss 0.040 Val Acc 50.920\n",
      "L 4 Epoch 103 Loss 0.055 Val Acc 51.760\n",
      "L 4 Epoch 104 Loss 0.065 Val Acc 52.440\n",
      "L 4 Epoch 105 Loss 0.042 Val Acc 52.940\n",
      "L 4 Epoch 106 Loss 0.041 Val Acc 52.860\n",
      "L 4 Epoch 107 Loss 0.063 Val Acc 52.380\n",
      "==> Saving model ...\n",
      "L 4 Epoch 108 Loss 0.066 Val Acc 53.920\n",
      "L 4 Epoch 109 Loss 0.050 Val Acc 53.480\n",
      "L 4 Epoch 110 Loss 0.075 Val Acc 52.220\n",
      "L 4 Epoch 111 Loss 0.037 Val Acc 52.720\n",
      "L 4 Epoch 112 Loss 0.029 Val Acc 53.100\n",
      "L 4 Epoch 113 Loss 0.063 Val Acc 53.140\n",
      "L 4 Epoch 114 Loss 0.020 Val Acc 53.560\n",
      "==> Saving model ...\n",
      "L 4 Epoch 115 Loss 0.049 Val Acc 54.720\n",
      "L 4 Epoch 116 Loss 0.030 Val Acc 54.680\n",
      "L 4 Epoch 117 Loss 0.066 Val Acc 53.060\n",
      "L 4 Epoch 118 Loss 0.046 Val Acc 51.500\n",
      "L 4 Epoch 119 Loss 0.019 Val Acc 52.720\n",
      "L 4 Epoch 120 Loss 0.044 Val Acc 53.160\n",
      "L 4 Epoch 121 Loss 0.027 Val Acc 53.240\n",
      "L 4 Epoch 122 Loss 0.021 Val Acc 53.340\n",
      "L 4 Epoch 123 Loss 0.068 Val Acc 52.940\n",
      "L 4 Epoch 124 Loss 0.018 Val Acc 52.860\n",
      "L 4 Epoch 125 Loss 0.039 Val Acc 53.760\n",
      "L 4 Epoch 126 Loss 0.042 Val Acc 53.240\n",
      "L 4 Epoch 127 Loss 0.019 Val Acc 53.380\n",
      "L 4 Epoch 128 Loss 0.019 Val Acc 53.500\n",
      "L 4 Epoch 129 Loss 0.032 Val Acc 53.080\n",
      "L 4 Epoch 130 Loss 0.029 Val Acc 52.960\n",
      "L 4 Epoch 131 Loss 0.015 Val Acc 52.760\n",
      "L 4 Epoch 132 Loss 0.025 Val Acc 52.880\n",
      "L 4 Epoch 133 Loss 0.021 Val Acc 53.800\n",
      "L 4 Epoch 134 Loss 0.022 Val Acc 53.680\n",
      "L 4 Epoch 135 Loss 0.034 Val Acc 53.280\n",
      "L 4 Epoch 136 Loss 0.027 Val Acc 52.340\n",
      "L 4 Epoch 137 Loss 0.025 Val Acc 53.060\n",
      "L 4 Epoch 138 Loss 0.017 Val Acc 53.820\n",
      "L 4 Epoch 139 Loss 0.028 Val Acc 53.740\n",
      "L 4 Epoch 140 Loss 0.032 Val Acc 52.840\n",
      "L 4 Epoch 141 Loss 0.019 Val Acc 53.520\n",
      "L 4 Epoch 142 Loss 0.026 Val Acc 53.620\n",
      "L 4 Epoch 143 Loss 0.013 Val Acc 53.720\n",
      "L 4 Epoch 144 Loss 0.013 Val Acc 53.960\n",
      "L 4 Epoch 145 Loss 0.016 Val Acc 54.000\n",
      "L 4 Epoch 146 Loss 0.020 Val Acc 54.040\n",
      "L 4 Epoch 147 Loss 0.012 Val Acc 54.160\n",
      "L 4 Epoch 148 Loss 0.004 Val Acc 54.220\n",
      "L 4 Epoch 149 Loss 0.016 Val Acc 54.420\n",
      "L 4 Epoch 150 Loss 0.011 Val Acc 54.120\n",
      "L 4 Epoch 151 Loss 0.017 Val Acc 54.140\n",
      "L 4 Epoch 152 Loss 0.017 Val Acc 54.360\n",
      "L 4 Epoch 153 Loss 0.013 Val Acc 54.540\n",
      "L 4 Epoch 154 Loss 0.006 Val Acc 54.600\n",
      "L 4 Epoch 155 Loss 0.021 Val Acc 54.640\n",
      "==> Saving model ...\n",
      "L 4 Epoch 156 Loss 0.009 Val Acc 54.760\n",
      "L 4 Epoch 157 Loss 0.020 Val Acc 54.280\n",
      "L 4 Epoch 158 Loss 0.029 Val Acc 53.940\n",
      "L 4 Epoch 159 Loss 0.027 Val Acc 53.780\n",
      "L 4 Epoch 160 Loss 0.010 Val Acc 54.660\n",
      "L 4 Epoch 161 Loss 0.015 Val Acc 54.620\n",
      "L 4 Epoch 162 Loss 0.012 Val Acc 53.780\n",
      "L 4 Epoch 163 Loss 0.010 Val Acc 53.720\n",
      "L 4 Epoch 164 Loss 0.012 Val Acc 53.740\n",
      "L 4 Epoch 165 Loss 0.012 Val Acc 53.820\n",
      "L 4 Epoch 166 Loss 0.010 Val Acc 54.040\n",
      "L 4 Epoch 167 Loss 0.024 Val Acc 53.540\n",
      "L 4 Epoch 168 Loss 0.011 Val Acc 54.320\n",
      "L 4 Epoch 169 Loss 0.030 Val Acc 53.940\n",
      "L 4 Epoch 170 Loss 0.004 Val Acc 54.180\n",
      "L 4 Epoch 171 Loss 0.013 Val Acc 54.020\n",
      "L 4 Epoch 172 Loss 0.009 Val Acc 54.220\n",
      "L 4 Epoch 173 Loss 0.017 Val Acc 54.400\n",
      "L 4 Epoch 174 Loss 0.017 Val Acc 54.100\n",
      "L 4 Epoch 175 Loss 0.025 Val Acc 53.800\n",
      "L 4 Epoch 176 Loss 0.025 Val Acc 52.740\n",
      "L 4 Epoch 177 Loss 0.017 Val Acc 54.000\n",
      "L 4 Epoch 178 Loss 0.012 Val Acc 53.780\n",
      "L 4 Epoch 179 Loss 0.003 Val Acc 54.300\n",
      "L 4 Epoch 180 Loss 0.014 Val Acc 54.020\n",
      "L 4 Epoch 181 Loss 0.009 Val Acc 54.040\n",
      "L 4 Epoch 182 Loss 0.026 Val Acc 54.080\n",
      "L 4 Epoch 183 Loss 0.026 Val Acc 53.840\n",
      "L 4 Epoch 184 Loss 0.012 Val Acc 54.280\n",
      "L 4 Epoch 185 Loss 0.021 Val Acc 54.180\n",
      "L 4 Epoch 186 Loss 0.015 Val Acc 54.260\n",
      "L 4 Epoch 187 Loss 0.008 Val Acc 54.320\n",
      "L 4 Epoch 188 Loss 0.007 Val Acc 54.400\n",
      "L 4 Epoch 189 Loss 0.006 Val Acc 54.400\n",
      "L 4 Epoch 190 Loss 0.009 Val Acc 54.100\n",
      "L 4 Epoch 191 Loss 0.020 Val Acc 54.180\n",
      "L 4 Epoch 192 Loss 0.002 Val Acc 54.480\n",
      "L 4 Epoch 193 Loss 0.012 Val Acc 54.440\n",
      "L 4 Epoch 194 Loss 0.013 Val Acc 54.260\n",
      "L 4 Epoch 195 Loss 0.007 Val Acc 54.260\n",
      "L 4 Epoch 196 Loss 0.008 Val Acc 54.400\n",
      "L 4 Epoch 197 Loss 0.005 Val Acc 54.300\n",
      "L 4 Epoch 198 Loss 0.008 Val Acc 54.260\n",
      "L 4 Epoch 199 Loss 0.008 Val Acc 54.240\n",
      "L 6 Epoch 0 Loss 5.019 Val Acc 10.000\n",
      "L 6 Epoch 1 Loss 6.023 Val Acc 10.000\n",
      "L 6 Epoch 2 Loss 3.486 Val Acc 10.000\n",
      "L 6 Epoch 3 Loss 2.664 Val Acc 10.180\n",
      "L 6 Epoch 4 Loss 2.335 Val Acc 12.900\n",
      "L 6 Epoch 5 Loss 2.256 Val Acc 15.440\n",
      "L 6 Epoch 6 Loss 2.178 Val Acc 17.380\n",
      "L 6 Epoch 7 Loss 2.174 Val Acc 18.640\n",
      "L 6 Epoch 8 Loss 2.083 Val Acc 21.300\n",
      "L 6 Epoch 9 Loss 2.053 Val Acc 21.360\n",
      "L 6 Epoch 10 Loss 1.990 Val Acc 23.200\n",
      "L 6 Epoch 11 Loss 1.968 Val Acc 24.840\n",
      "L 6 Epoch 12 Loss 1.911 Val Acc 23.720\n",
      "L 6 Epoch 13 Loss 1.863 Val Acc 26.920\n",
      "L 6 Epoch 14 Loss 1.818 Val Acc 28.600\n",
      "L 6 Epoch 15 Loss 1.805 Val Acc 27.220\n",
      "L 6 Epoch 16 Loss 1.767 Val Acc 28.380\n",
      "L 6 Epoch 17 Loss 1.706 Val Acc 33.200\n",
      "L 6 Epoch 18 Loss 1.666 Val Acc 26.400\n",
      "L 6 Epoch 19 Loss 1.560 Val Acc 31.520\n",
      "L 6 Epoch 20 Loss 1.555 Val Acc 34.540\n",
      "L 6 Epoch 21 Loss 1.555 Val Acc 35.560\n",
      "L 6 Epoch 22 Loss 1.500 Val Acc 35.280\n",
      "L 6 Epoch 23 Loss 1.516 Val Acc 39.060\n",
      "L 6 Epoch 24 Loss 1.473 Val Acc 38.760\n",
      "L 6 Epoch 25 Loss 1.435 Val Acc 38.520\n",
      "L 6 Epoch 26 Loss 1.427 Val Acc 36.600\n",
      "==> Saving model ...\n",
      "L 6 Epoch 27 Loss 1.442 Val Acc 40.080\n",
      "L 6 Epoch 28 Loss 1.409 Val Acc 39.500\n",
      "==> Saving model ...\n",
      "L 6 Epoch 29 Loss 1.371 Val Acc 43.000\n",
      "L 6 Epoch 30 Loss 1.299 Val Acc 38.260\n",
      "L 6 Epoch 31 Loss 1.279 Val Acc 38.320\n",
      "L 6 Epoch 32 Loss 1.225 Val Acc 40.140\n",
      "L 6 Epoch 33 Loss 1.271 Val Acc 40.620\n",
      "L 6 Epoch 34 Loss 1.208 Val Acc 39.900\n",
      "==> Saving model ...\n",
      "L 6 Epoch 35 Loss 1.162 Val Acc 43.680\n",
      "L 6 Epoch 36 Loss 1.052 Val Acc 42.300\n",
      "L 6 Epoch 37 Loss 1.146 Val Acc 41.560\n",
      "L 6 Epoch 38 Loss 1.082 Val Acc 41.540\n",
      "==> Saving model ...\n",
      "L 6 Epoch 39 Loss 1.056 Val Acc 44.400\n",
      "L 6 Epoch 40 Loss 1.017 Val Acc 42.820\n",
      "L 6 Epoch 41 Loss 1.021 Val Acc 42.560\n",
      "L 6 Epoch 42 Loss 0.967 Val Acc 43.040\n",
      "L 6 Epoch 43 Loss 0.988 Val Acc 43.140\n",
      "L 6 Epoch 44 Loss 0.885 Val Acc 42.760\n",
      "==> Saving model ...\n",
      "L 6 Epoch 45 Loss 0.854 Val Acc 45.300\n",
      "L 6 Epoch 46 Loss 0.752 Val Acc 45.060\n",
      "L 6 Epoch 47 Loss 0.756 Val Acc 42.260\n",
      "L 6 Epoch 48 Loss 0.722 Val Acc 44.400\n",
      "L 6 Epoch 49 Loss 0.720 Val Acc 43.640\n",
      "L 6 Epoch 50 Loss 0.674 Val Acc 43.680\n",
      "L 6 Epoch 51 Loss 0.639 Val Acc 43.420\n",
      "L 6 Epoch 52 Loss 0.511 Val Acc 45.280\n",
      "L 6 Epoch 53 Loss 0.725 Val Acc 44.740\n",
      "L 6 Epoch 54 Loss 0.660 Val Acc 44.080\n",
      "L 6 Epoch 55 Loss 0.745 Val Acc 42.760\n",
      "L 6 Epoch 56 Loss 0.632 Val Acc 43.560\n",
      "L 6 Epoch 57 Loss 0.615 Val Acc 44.280\n",
      "L 6 Epoch 58 Loss 0.642 Val Acc 41.680\n",
      "==> Saving model ...\n",
      "L 6 Epoch 59 Loss 0.440 Val Acc 45.580\n",
      "==> Saving model ...\n",
      "L 6 Epoch 60 Loss 0.417 Val Acc 46.020\n",
      "L 6 Epoch 61 Loss 0.449 Val Acc 45.060\n",
      "L 6 Epoch 62 Loss 0.323 Val Acc 46.000\n",
      "==> Saving model ...\n",
      "L 6 Epoch 63 Loss 0.371 Val Acc 46.580\n",
      "L 6 Epoch 64 Loss 0.272 Val Acc 44.860\n",
      "L 6 Epoch 65 Loss 0.273 Val Acc 45.780\n",
      "==> Saving model ...\n",
      "L 6 Epoch 66 Loss 0.302 Val Acc 46.600\n",
      "L 6 Epoch 67 Loss 0.310 Val Acc 46.420\n",
      "L 6 Epoch 68 Loss 0.346 Val Acc 45.480\n",
      "L 6 Epoch 69 Loss 0.336 Val Acc 44.980\n",
      "L 6 Epoch 70 Loss 0.318 Val Acc 44.320\n",
      "L 6 Epoch 71 Loss 0.302 Val Acc 45.480\n",
      "L 6 Epoch 72 Loss 0.346 Val Acc 46.120\n",
      "L 6 Epoch 73 Loss 0.346 Val Acc 45.580\n",
      "L 6 Epoch 74 Loss 0.367 Val Acc 45.920\n",
      "==> Saving model ...\n",
      "L 6 Epoch 75 Loss 0.243 Val Acc 47.640\n",
      "==> Saving model ...\n",
      "L 6 Epoch 76 Loss 0.274 Val Acc 48.220\n",
      "L 6 Epoch 77 Loss 0.283 Val Acc 46.320\n",
      "==> Saving model ...\n",
      "L 6 Epoch 78 Loss 0.186 Val Acc 48.260\n",
      "==> Saving model ...\n",
      "L 6 Epoch 79 Loss 0.206 Val Acc 48.480\n",
      "L 6 Epoch 80 Loss 0.205 Val Acc 48.300\n",
      "L 6 Epoch 81 Loss 0.210 Val Acc 46.260\n",
      "L 6 Epoch 82 Loss 0.256 Val Acc 45.420\n",
      "L 6 Epoch 83 Loss 0.182 Val Acc 45.640\n",
      "L 6 Epoch 84 Loss 0.194 Val Acc 47.600\n",
      "==> Saving model ...\n",
      "L 6 Epoch 85 Loss 0.191 Val Acc 48.700\n",
      "==> Saving model ...\n",
      "L 6 Epoch 86 Loss 0.139 Val Acc 48.760\n",
      "L 6 Epoch 87 Loss 0.139 Val Acc 48.660\n",
      "L 6 Epoch 88 Loss 0.146 Val Acc 48.460\n",
      "L 6 Epoch 89 Loss 0.084 Val Acc 47.880\n",
      "L 6 Epoch 90 Loss 0.162 Val Acc 47.240\n",
      "L 6 Epoch 91 Loss 0.087 Val Acc 48.680\n",
      "L 6 Epoch 92 Loss 0.118 Val Acc 48.200\n",
      "==> Saving model ...\n",
      "L 6 Epoch 93 Loss 0.118 Val Acc 48.860\n",
      "==> Saving model ...\n",
      "L 6 Epoch 94 Loss 0.147 Val Acc 48.880\n",
      "L 6 Epoch 95 Loss 0.106 Val Acc 48.400\n",
      "==> Saving model ...\n",
      "L 6 Epoch 96 Loss 0.157 Val Acc 49.880\n",
      "L 6 Epoch 97 Loss 0.117 Val Acc 48.660\n",
      "L 6 Epoch 98 Loss 0.082 Val Acc 48.260\n",
      "L 6 Epoch 99 Loss 0.102 Val Acc 48.760\n",
      "L 6 Epoch 100 Loss 0.089 Val Acc 48.640\n",
      "L 6 Epoch 101 Loss 0.106 Val Acc 49.220\n",
      "L 6 Epoch 102 Loss 0.112 Val Acc 48.800\n",
      "L 6 Epoch 103 Loss 0.107 Val Acc 49.340\n",
      "L 6 Epoch 104 Loss 0.100 Val Acc 47.900\n",
      "L 6 Epoch 105 Loss 0.068 Val Acc 48.760\n",
      "==> Saving model ...\n",
      "L 6 Epoch 106 Loss 0.066 Val Acc 50.800\n",
      "==> Saving model ...\n",
      "L 6 Epoch 107 Loss 0.061 Val Acc 50.840\n",
      "L 6 Epoch 108 Loss 0.056 Val Acc 49.920\n",
      "L 6 Epoch 109 Loss 0.074 Val Acc 49.180\n",
      "L 6 Epoch 110 Loss 0.041 Val Acc 50.000\n",
      "L 6 Epoch 111 Loss 0.061 Val Acc 50.660\n",
      "L 6 Epoch 112 Loss 0.090 Val Acc 47.820\n",
      "L 6 Epoch 113 Loss 0.083 Val Acc 50.380\n",
      "L 6 Epoch 114 Loss 0.078 Val Acc 49.200\n",
      "L 6 Epoch 115 Loss 0.071 Val Acc 50.020\n",
      "L 6 Epoch 116 Loss 0.073 Val Acc 49.300\n",
      "L 6 Epoch 117 Loss 0.074 Val Acc 49.800\n",
      "L 6 Epoch 118 Loss 0.058 Val Acc 49.980\n",
      "L 6 Epoch 119 Loss 0.033 Val Acc 50.080\n",
      "L 6 Epoch 120 Loss 0.030 Val Acc 49.820\n",
      "L 6 Epoch 121 Loss 0.071 Val Acc 50.440\n",
      "L 6 Epoch 122 Loss 0.049 Val Acc 50.500\n",
      "L 6 Epoch 123 Loss 0.048 Val Acc 49.860\n",
      "L 6 Epoch 124 Loss 0.053 Val Acc 49.120\n",
      "L 6 Epoch 125 Loss 0.047 Val Acc 50.780\n",
      "L 6 Epoch 126 Loss 0.038 Val Acc 49.660\n",
      "==> Saving model ...\n",
      "L 6 Epoch 127 Loss 0.058 Val Acc 51.120\n",
      "L 6 Epoch 128 Loss 0.045 Val Acc 50.240\n",
      "L 6 Epoch 129 Loss 0.056 Val Acc 49.360\n",
      "L 6 Epoch 130 Loss 0.047 Val Acc 49.900\n",
      "L 6 Epoch 131 Loss 0.039 Val Acc 50.600\n",
      "L 6 Epoch 132 Loss 0.053 Val Acc 51.100\n",
      "L 6 Epoch 133 Loss 0.034 Val Acc 50.800\n",
      "==> Saving model ...\n",
      "L 6 Epoch 134 Loss 0.019 Val Acc 51.260\n",
      "==> Saving model ...\n",
      "L 6 Epoch 135 Loss 0.055 Val Acc 51.520\n",
      "L 6 Epoch 136 Loss 0.041 Val Acc 51.200\n",
      "L 6 Epoch 137 Loss 0.022 Val Acc 51.420\n",
      "L 6 Epoch 138 Loss 0.035 Val Acc 51.420\n",
      "L 6 Epoch 139 Loss 0.024 Val Acc 51.320\n",
      "==> Saving model ...\n",
      "L 6 Epoch 140 Loss 0.039 Val Acc 52.020\n",
      "==> Saving model ...\n",
      "L 6 Epoch 141 Loss 0.044 Val Acc 52.500\n",
      "L 6 Epoch 142 Loss 0.078 Val Acc 51.420\n",
      "L 6 Epoch 143 Loss 0.026 Val Acc 50.980\n",
      "L 6 Epoch 144 Loss 0.018 Val Acc 51.300\n",
      "L 6 Epoch 145 Loss 0.021 Val Acc 51.320\n",
      "L 6 Epoch 146 Loss 0.038 Val Acc 51.220\n",
      "L 6 Epoch 147 Loss 0.036 Val Acc 50.580\n",
      "L 6 Epoch 148 Loss 0.023 Val Acc 51.500\n",
      "L 6 Epoch 149 Loss 0.019 Val Acc 51.680\n",
      "L 6 Epoch 150 Loss 0.047 Val Acc 52.200\n",
      "L 6 Epoch 151 Loss 0.028 Val Acc 52.220\n",
      "L 6 Epoch 152 Loss 0.025 Val Acc 52.000\n",
      "L 6 Epoch 153 Loss 0.017 Val Acc 51.820\n",
      "L 6 Epoch 154 Loss 0.028 Val Acc 51.620\n",
      "L 6 Epoch 155 Loss 0.015 Val Acc 51.440\n",
      "L 6 Epoch 156 Loss 0.011 Val Acc 51.320\n",
      "L 6 Epoch 157 Loss 0.024 Val Acc 51.340\n",
      "L 6 Epoch 158 Loss 0.036 Val Acc 51.520\n",
      "L 6 Epoch 159 Loss 0.044 Val Acc 51.580\n",
      "L 6 Epoch 160 Loss 0.019 Val Acc 51.580\n",
      "L 6 Epoch 161 Loss 0.009 Val Acc 51.540\n",
      "L 6 Epoch 162 Loss 0.025 Val Acc 51.580\n",
      "L 6 Epoch 163 Loss 0.017 Val Acc 51.820\n",
      "L 6 Epoch 164 Loss 0.022 Val Acc 51.800\n",
      "L 6 Epoch 165 Loss 0.022 Val Acc 51.980\n",
      "L 6 Epoch 166 Loss 0.019 Val Acc 51.940\n",
      "L 6 Epoch 167 Loss 0.032 Val Acc 52.260\n",
      "L 6 Epoch 168 Loss 0.015 Val Acc 52.320\n",
      "L 6 Epoch 169 Loss 0.019 Val Acc 52.420\n",
      "L 6 Epoch 170 Loss 0.023 Val Acc 52.280\n",
      "L 6 Epoch 171 Loss 0.021 Val Acc 52.280\n",
      "L 6 Epoch 172 Loss 0.021 Val Acc 51.900\n",
      "L 6 Epoch 173 Loss 0.021 Val Acc 51.700\n",
      "L 6 Epoch 174 Loss 0.015 Val Acc 51.920\n",
      "L 6 Epoch 175 Loss 0.031 Val Acc 51.840\n",
      "L 6 Epoch 176 Loss 0.017 Val Acc 51.880\n",
      "L 6 Epoch 177 Loss 0.012 Val Acc 51.960\n",
      "L 6 Epoch 178 Loss 0.017 Val Acc 51.820\n",
      "L 6 Epoch 179 Loss 0.009 Val Acc 52.220\n",
      "L 6 Epoch 180 Loss 0.012 Val Acc 52.000\n",
      "L 6 Epoch 181 Loss 0.020 Val Acc 52.100\n",
      "L 6 Epoch 182 Loss 0.006 Val Acc 52.080\n",
      "L 6 Epoch 183 Loss 0.017 Val Acc 52.040\n",
      "L 6 Epoch 184 Loss 0.023 Val Acc 51.980\n",
      "L 6 Epoch 185 Loss 0.018 Val Acc 52.020\n",
      "L 6 Epoch 186 Loss 0.014 Val Acc 52.200\n",
      "L 6 Epoch 187 Loss 0.015 Val Acc 52.120\n",
      "L 6 Epoch 188 Loss 0.016 Val Acc 52.060\n",
      "L 6 Epoch 189 Loss 0.015 Val Acc 52.120\n",
      "L 6 Epoch 190 Loss 0.011 Val Acc 52.040\n",
      "L 6 Epoch 191 Loss 0.013 Val Acc 51.980\n",
      "L 6 Epoch 192 Loss 0.018 Val Acc 52.080\n",
      "L 6 Epoch 193 Loss 0.020 Val Acc 52.040\n",
      "L 6 Epoch 194 Loss 0.015 Val Acc 51.980\n",
      "L 6 Epoch 195 Loss 0.020 Val Acc 51.980\n",
      "L 6 Epoch 196 Loss 0.010 Val Acc 52.020\n",
      "L 6 Epoch 197 Loss 0.022 Val Acc 52.080\n",
      "L 6 Epoch 198 Loss 0.023 Val Acc 52.100\n",
      "L 6 Epoch 199 Loss 0.035 Val Acc 51.880\n",
      "L 8 Epoch 0 Loss 3.671 Val Acc 8.920\n",
      "L 8 Epoch 1 Loss 3.912 Val Acc 9.780\n",
      "L 8 Epoch 2 Loss 2.872 Val Acc 11.980\n",
      "L 8 Epoch 3 Loss 2.231 Val Acc 14.760\n",
      "L 8 Epoch 4 Loss 2.010 Val Acc 18.740\n",
      "L 8 Epoch 5 Loss 1.944 Val Acc 22.680\n",
      "L 8 Epoch 6 Loss 1.836 Val Acc 26.240\n",
      "L 8 Epoch 7 Loss 1.765 Val Acc 30.660\n",
      "L 8 Epoch 8 Loss 1.698 Val Acc 34.080\n",
      "L 8 Epoch 9 Loss 1.661 Val Acc 35.480\n",
      "L 8 Epoch 10 Loss 1.585 Val Acc 34.480\n",
      "L 8 Epoch 11 Loss 1.534 Val Acc 37.880\n",
      "L 8 Epoch 12 Loss 1.542 Val Acc 38.160\n",
      "L 8 Epoch 13 Loss 1.485 Val Acc 37.100\n",
      "L 8 Epoch 14 Loss 1.507 Val Acc 35.380\n",
      "L 8 Epoch 15 Loss 1.498 Val Acc 39.740\n",
      "L 8 Epoch 16 Loss 1.493 Val Acc 35.460\n",
      "L 8 Epoch 17 Loss 1.439 Val Acc 37.940\n",
      "==> Saving model ...\n",
      "L 8 Epoch 18 Loss 1.385 Val Acc 40.140\n",
      "==> Saving model ...\n",
      "L 8 Epoch 19 Loss 1.334 Val Acc 43.700\n",
      "L 8 Epoch 20 Loss 1.326 Val Acc 38.720\n",
      "L 8 Epoch 21 Loss 1.266 Val Acc 40.380\n",
      "L 8 Epoch 22 Loss 1.206 Val Acc 43.260\n",
      "L 8 Epoch 23 Loss 1.150 Val Acc 43.680\n",
      "L 8 Epoch 24 Loss 1.225 Val Acc 43.180\n",
      "==> Saving model ...\n",
      "L 8 Epoch 25 Loss 1.144 Val Acc 46.840\n",
      "==> Saving model ...\n",
      "L 8 Epoch 26 Loss 1.010 Val Acc 47.500\n",
      "L 8 Epoch 27 Loss 0.954 Val Acc 46.580\n",
      "L 8 Epoch 28 Loss 0.967 Val Acc 45.240\n",
      "L 8 Epoch 29 Loss 0.985 Val Acc 47.240\n",
      "==> Saving model ...\n",
      "L 8 Epoch 30 Loss 0.874 Val Acc 48.020\n",
      "L 8 Epoch 31 Loss 0.847 Val Acc 46.180\n",
      "L 8 Epoch 32 Loss 0.792 Val Acc 45.120\n",
      "L 8 Epoch 33 Loss 0.923 Val Acc 41.860\n",
      "L 8 Epoch 34 Loss 0.921 Val Acc 47.540\n",
      "L 8 Epoch 35 Loss 0.778 Val Acc 46.220\n",
      "L 8 Epoch 36 Loss 0.704 Val Acc 45.740\n",
      "L 8 Epoch 37 Loss 0.710 Val Acc 47.920\n",
      "L 8 Epoch 38 Loss 0.922 Val Acc 45.680\n",
      "L 8 Epoch 39 Loss 0.727 Val Acc 46.580\n",
      "==> Saving model ...\n",
      "L 8 Epoch 40 Loss 0.628 Val Acc 48.600\n",
      "==> Saving model ...\n",
      "L 8 Epoch 41 Loss 0.633 Val Acc 49.120\n",
      "==> Saving model ...\n",
      "L 8 Epoch 42 Loss 0.611 Val Acc 49.480\n",
      "L 8 Epoch 43 Loss 0.604 Val Acc 45.380\n",
      "L 8 Epoch 44 Loss 0.675 Val Acc 49.320\n",
      "==> Saving model ...\n",
      "L 8 Epoch 45 Loss 0.565 Val Acc 50.240\n",
      "L 8 Epoch 46 Loss 0.501 Val Acc 49.200\n",
      "==> Saving model ...\n",
      "L 8 Epoch 47 Loss 0.458 Val Acc 51.740\n",
      "L 8 Epoch 48 Loss 0.442 Val Acc 48.580\n",
      "L 8 Epoch 49 Loss 0.397 Val Acc 50.720\n",
      "==> Saving model ...\n",
      "L 8 Epoch 50 Loss 0.337 Val Acc 51.800\n",
      "L 8 Epoch 51 Loss 0.410 Val Acc 49.820\n",
      "L 8 Epoch 52 Loss 0.471 Val Acc 49.900\n",
      "L 8 Epoch 53 Loss 0.426 Val Acc 49.560\n",
      "==> Saving model ...\n",
      "L 8 Epoch 54 Loss 0.376 Val Acc 54.080\n",
      "L 8 Epoch 55 Loss 0.303 Val Acc 49.840\n",
      "L 8 Epoch 56 Loss 0.277 Val Acc 51.180\n",
      "L 8 Epoch 57 Loss 0.279 Val Acc 51.540\n",
      "L 8 Epoch 58 Loss 0.305 Val Acc 48.640\n",
      "L 8 Epoch 59 Loss 0.217 Val Acc 49.980\n",
      "L 8 Epoch 60 Loss 0.261 Val Acc 51.140\n",
      "L 8 Epoch 61 Loss 0.268 Val Acc 52.460\n",
      "L 8 Epoch 62 Loss 0.265 Val Acc 51.620\n",
      "L 8 Epoch 63 Loss 0.246 Val Acc 50.900\n",
      "L 8 Epoch 64 Loss 0.299 Val Acc 50.560\n",
      "==> Saving model ...\n",
      "L 8 Epoch 65 Loss 0.259 Val Acc 54.160\n",
      "L 8 Epoch 66 Loss 0.298 Val Acc 50.520\n",
      "L 8 Epoch 67 Loss 0.269 Val Acc 48.720\n",
      "L 8 Epoch 68 Loss 0.174 Val Acc 50.400\n",
      "L 8 Epoch 69 Loss 0.151 Val Acc 52.160\n",
      "L 8 Epoch 70 Loss 0.206 Val Acc 48.520\n",
      "L 8 Epoch 71 Loss 0.156 Val Acc 51.600\n",
      "L 8 Epoch 72 Loss 0.180 Val Acc 53.900\n",
      "==> Saving model ...\n",
      "L 8 Epoch 73 Loss 0.182 Val Acc 54.540\n",
      "L 8 Epoch 74 Loss 0.096 Val Acc 53.240\n",
      "L 8 Epoch 75 Loss 0.148 Val Acc 53.000\n",
      "L 8 Epoch 76 Loss 0.092 Val Acc 54.520\n",
      "L 8 Epoch 77 Loss 0.084 Val Acc 53.520\n",
      "L 8 Epoch 78 Loss 0.130 Val Acc 53.820\n",
      "L 8 Epoch 79 Loss 0.111 Val Acc 53.220\n",
      "L 8 Epoch 80 Loss 0.181 Val Acc 52.600\n",
      "L 8 Epoch 81 Loss 0.200 Val Acc 54.140\n",
      "L 8 Epoch 82 Loss 0.119 Val Acc 53.140\n",
      "L 8 Epoch 83 Loss 0.095 Val Acc 52.980\n",
      "L 8 Epoch 84 Loss 0.089 Val Acc 54.180\n",
      "L 8 Epoch 85 Loss 0.030 Val Acc 53.920\n",
      "==> Saving model ...\n",
      "L 8 Epoch 86 Loss 0.045 Val Acc 55.720\n",
      "L 8 Epoch 87 Loss 0.109 Val Acc 54.940\n",
      "L 8 Epoch 88 Loss 0.163 Val Acc 53.720\n",
      "L 8 Epoch 89 Loss 0.098 Val Acc 53.340\n",
      "L 8 Epoch 90 Loss 0.087 Val Acc 54.440\n",
      "L 8 Epoch 91 Loss 0.144 Val Acc 51.820\n",
      "L 8 Epoch 92 Loss 0.066 Val Acc 54.160\n",
      "L 8 Epoch 93 Loss 0.104 Val Acc 55.560\n",
      "L 8 Epoch 94 Loss 0.081 Val Acc 54.380\n",
      "L 8 Epoch 95 Loss 0.079 Val Acc 54.200\n",
      "L 8 Epoch 96 Loss 0.064 Val Acc 55.700\n",
      "L 8 Epoch 97 Loss 0.071 Val Acc 55.680\n",
      "==> Saving model ...\n",
      "L 8 Epoch 98 Loss 0.062 Val Acc 55.920\n",
      "L 8 Epoch 99 Loss 0.092 Val Acc 54.720\n",
      "L 8 Epoch 100 Loss 0.042 Val Acc 54.140\n",
      "L 8 Epoch 101 Loss 0.033 Val Acc 54.720\n",
      "L 8 Epoch 102 Loss 0.046 Val Acc 55.660\n",
      "L 8 Epoch 103 Loss 0.082 Val Acc 55.420\n",
      "L 8 Epoch 104 Loss 0.025 Val Acc 53.320\n",
      "==> Saving model ...\n",
      "L 8 Epoch 105 Loss 0.063 Val Acc 56.160\n",
      "L 8 Epoch 106 Loss 0.048 Val Acc 55.560\n",
      "L 8 Epoch 107 Loss 0.055 Val Acc 55.020\n",
      "L 8 Epoch 108 Loss 0.053 Val Acc 55.800\n",
      "L 8 Epoch 109 Loss 0.034 Val Acc 56.080\n",
      "L 8 Epoch 110 Loss 0.023 Val Acc 55.980\n",
      "L 8 Epoch 111 Loss 0.043 Val Acc 55.140\n",
      "L 8 Epoch 112 Loss 0.055 Val Acc 55.780\n",
      "L 8 Epoch 113 Loss 0.062 Val Acc 54.320\n",
      "L 8 Epoch 114 Loss 0.060 Val Acc 55.700\n",
      "==> Saving model ...\n",
      "L 8 Epoch 115 Loss 0.032 Val Acc 56.480\n",
      "L 8 Epoch 116 Loss 0.042 Val Acc 55.440\n",
      "L 8 Epoch 117 Loss 0.068 Val Acc 56.160\n",
      "L 8 Epoch 118 Loss 0.042 Val Acc 56.080\n",
      "L 8 Epoch 119 Loss 0.040 Val Acc 56.100\n",
      "L 8 Epoch 120 Loss 0.018 Val Acc 55.620\n",
      "L 8 Epoch 121 Loss 0.023 Val Acc 56.080\n",
      "L 8 Epoch 122 Loss 0.054 Val Acc 56.480\n",
      "L 8 Epoch 123 Loss 0.016 Val Acc 56.240\n",
      "==> Saving model ...\n",
      "L 8 Epoch 124 Loss 0.026 Val Acc 56.740\n",
      "L 8 Epoch 125 Loss 0.031 Val Acc 56.120\n",
      "L 8 Epoch 126 Loss 0.023 Val Acc 55.660\n",
      "L 8 Epoch 127 Loss 0.057 Val Acc 55.700\n",
      "L 8 Epoch 128 Loss 0.040 Val Acc 56.460\n",
      "==> Saving model ...\n",
      "L 8 Epoch 129 Loss 0.030 Val Acc 56.760\n",
      "==> Saving model ...\n",
      "L 8 Epoch 130 Loss 0.030 Val Acc 57.160\n",
      "==> Saving model ...\n",
      "L 8 Epoch 131 Loss 0.020 Val Acc 57.200\n",
      "L 8 Epoch 132 Loss 0.012 Val Acc 57.180\n",
      "L 8 Epoch 133 Loss 0.034 Val Acc 57.200\n",
      "L 8 Epoch 134 Loss 0.011 Val Acc 57.200\n",
      "L 8 Epoch 135 Loss 0.016 Val Acc 56.920\n",
      "L 8 Epoch 136 Loss 0.010 Val Acc 57.040\n",
      "L 8 Epoch 137 Loss 0.027 Val Acc 57.180\n",
      "==> Saving model ...\n",
      "L 8 Epoch 138 Loss 0.018 Val Acc 57.480\n",
      "L 8 Epoch 139 Loss 0.010 Val Acc 57.160\n",
      "L 8 Epoch 140 Loss 0.012 Val Acc 56.620\n",
      "L 8 Epoch 141 Loss 0.019 Val Acc 56.920\n",
      "L 8 Epoch 142 Loss 0.012 Val Acc 56.580\n",
      "L 8 Epoch 143 Loss 0.010 Val Acc 56.580\n",
      "L 8 Epoch 144 Loss 0.017 Val Acc 56.800\n",
      "L 8 Epoch 145 Loss 0.010 Val Acc 56.900\n",
      "L 8 Epoch 146 Loss 0.029 Val Acc 56.560\n",
      "L 8 Epoch 147 Loss 0.014 Val Acc 56.780\n",
      "L 8 Epoch 148 Loss 0.019 Val Acc 56.740\n",
      "L 8 Epoch 149 Loss 0.005 Val Acc 57.280\n",
      "L 8 Epoch 150 Loss 0.013 Val Acc 56.960\n",
      "L 8 Epoch 151 Loss 0.003 Val Acc 57.240\n",
      "L 8 Epoch 152 Loss 0.010 Val Acc 57.380\n",
      "==> Saving model ...\n",
      "L 8 Epoch 153 Loss 0.007 Val Acc 57.780\n",
      "L 8 Epoch 154 Loss 0.008 Val Acc 57.600\n",
      "L 8 Epoch 155 Loss 0.018 Val Acc 57.520\n",
      "L 8 Epoch 156 Loss 0.024 Val Acc 56.800\n",
      "L 8 Epoch 157 Loss 0.017 Val Acc 57.000\n",
      "L 8 Epoch 158 Loss 0.020 Val Acc 57.060\n",
      "L 8 Epoch 159 Loss 0.014 Val Acc 57.260\n",
      "L 8 Epoch 160 Loss 0.018 Val Acc 57.580\n",
      "L 8 Epoch 161 Loss 0.008 Val Acc 57.700\n",
      "L 8 Epoch 162 Loss 0.014 Val Acc 57.640\n",
      "==> Saving model ...\n",
      "L 8 Epoch 163 Loss 0.007 Val Acc 57.920\n",
      "L 8 Epoch 164 Loss 0.006 Val Acc 57.800\n",
      "L 8 Epoch 165 Loss 0.022 Val Acc 57.640\n",
      "==> Saving model ...\n",
      "L 8 Epoch 166 Loss 0.006 Val Acc 57.940\n",
      "L 8 Epoch 167 Loss 0.014 Val Acc 57.760\n",
      "==> Saving model ...\n",
      "L 8 Epoch 168 Loss 0.009 Val Acc 57.980\n",
      "L 8 Epoch 169 Loss 0.005 Val Acc 57.820\n",
      "L 8 Epoch 170 Loss 0.013 Val Acc 57.740\n",
      "L 8 Epoch 171 Loss 0.010 Val Acc 57.880\n",
      "L 8 Epoch 172 Loss 0.008 Val Acc 57.780\n",
      "L 8 Epoch 173 Loss 0.013 Val Acc 57.900\n",
      "L 8 Epoch 174 Loss 0.019 Val Acc 57.560\n",
      "L 8 Epoch 175 Loss 0.006 Val Acc 57.740\n",
      "==> Saving model ...\n",
      "L 8 Epoch 176 Loss 0.011 Val Acc 58.120\n",
      "L 8 Epoch 177 Loss 0.016 Val Acc 57.900\n",
      "L 8 Epoch 178 Loss 0.009 Val Acc 57.760\n",
      "L 8 Epoch 179 Loss 0.005 Val Acc 58.020\n",
      "L 8 Epoch 180 Loss 0.018 Val Acc 57.920\n",
      "L 8 Epoch 181 Loss 0.018 Val Acc 57.600\n",
      "L 8 Epoch 182 Loss 0.006 Val Acc 57.680\n",
      "L 8 Epoch 183 Loss 0.008 Val Acc 57.560\n",
      "L 8 Epoch 184 Loss 0.014 Val Acc 57.100\n",
      "L 8 Epoch 185 Loss 0.013 Val Acc 57.660\n",
      "L 8 Epoch 186 Loss 0.022 Val Acc 57.600\n",
      "L 8 Epoch 187 Loss 0.015 Val Acc 57.820\n",
      "L 8 Epoch 188 Loss 0.009 Val Acc 57.700\n",
      "L 8 Epoch 189 Loss 0.016 Val Acc 57.540\n",
      "L 8 Epoch 190 Loss 0.003 Val Acc 58.020\n",
      "L 8 Epoch 191 Loss 0.011 Val Acc 57.900\n",
      "L 8 Epoch 192 Loss 0.016 Val Acc 57.840\n",
      "L 8 Epoch 193 Loss 0.017 Val Acc 57.500\n",
      "L 8 Epoch 194 Loss 0.009 Val Acc 57.780\n",
      "L 8 Epoch 195 Loss 0.012 Val Acc 57.880\n",
      "L 8 Epoch 196 Loss 0.009 Val Acc 57.760\n",
      "L 8 Epoch 197 Loss 0.008 Val Acc 57.680\n",
      "L 8 Epoch 198 Loss 0.010 Val Acc 57.380\n",
      "L 8 Epoch 199 Loss 0.008 Val Acc 57.220\n",
      "L 10 Epoch 0 Loss 4.859 Val Acc 10.000\n",
      "L 10 Epoch 1 Loss 3.751 Val Acc 10.040\n",
      "L 10 Epoch 2 Loss 2.808 Val Acc 10.520\n",
      "L 10 Epoch 3 Loss 2.676 Val Acc 18.040\n",
      "L 10 Epoch 4 Loss 2.269 Val Acc 16.300\n",
      "L 10 Epoch 5 Loss 2.460 Val Acc 16.160\n",
      "L 10 Epoch 6 Loss 2.257 Val Acc 14.040\n",
      "L 10 Epoch 7 Loss 2.211 Val Acc 16.120\n",
      "L 10 Epoch 8 Loss 1.919 Val Acc 25.060\n",
      "L 10 Epoch 9 Loss 1.842 Val Acc 24.640\n",
      "L 10 Epoch 10 Loss 1.777 Val Acc 28.860\n",
      "L 10 Epoch 11 Loss 1.693 Val Acc 30.660\n",
      "L 10 Epoch 12 Loss 1.644 Val Acc 31.980\n",
      "L 10 Epoch 13 Loss 1.654 Val Acc 32.300\n",
      "L 10 Epoch 14 Loss 1.635 Val Acc 34.220\n",
      "L 10 Epoch 15 Loss 1.580 Val Acc 37.180\n",
      "L 10 Epoch 16 Loss 1.535 Val Acc 36.280\n",
      "L 10 Epoch 17 Loss 1.542 Val Acc 38.420\n",
      "L 10 Epoch 18 Loss 1.565 Val Acc 39.240\n",
      "L 10 Epoch 19 Loss 1.584 Val Acc 38.460\n",
      "L 10 Epoch 20 Loss 1.442 Val Acc 38.380\n",
      "L 10 Epoch 21 Loss 1.478 Val Acc 39.720\n",
      "==> Saving model ...\n",
      "L 10 Epoch 22 Loss 1.410 Val Acc 41.200\n",
      "==> Saving model ...\n",
      "L 10 Epoch 23 Loss 1.363 Val Acc 43.920\n",
      "==> Saving model ...\n",
      "L 10 Epoch 24 Loss 1.299 Val Acc 44.480\n",
      "L 10 Epoch 25 Loss 1.283 Val Acc 43.680\n",
      "L 10 Epoch 26 Loss 1.277 Val Acc 40.160\n",
      "L 10 Epoch 27 Loss 1.193 Val Acc 42.480\n",
      "==> Saving model ...\n",
      "L 10 Epoch 28 Loss 1.243 Val Acc 45.020\n",
      "L 10 Epoch 29 Loss 1.153 Val Acc 43.400\n",
      "L 10 Epoch 30 Loss 1.207 Val Acc 35.640\n",
      "L 10 Epoch 31 Loss 1.184 Val Acc 44.100\n",
      "L 10 Epoch 32 Loss 1.114 Val Acc 40.600\n",
      "L 10 Epoch 33 Loss 1.070 Val Acc 43.840\n",
      "==> Saving model ...\n",
      "L 10 Epoch 34 Loss 0.988 Val Acc 45.960\n",
      "L 10 Epoch 35 Loss 1.025 Val Acc 45.480\n",
      "L 10 Epoch 36 Loss 0.965 Val Acc 40.980\n",
      "L 10 Epoch 37 Loss 1.039 Val Acc 45.540\n",
      "L 10 Epoch 38 Loss 1.004 Val Acc 44.340\n",
      "==> Saving model ...\n",
      "L 10 Epoch 39 Loss 0.883 Val Acc 46.180\n",
      "==> Saving model ...\n",
      "L 10 Epoch 40 Loss 0.825 Val Acc 48.280\n",
      "==> Saving model ...\n",
      "L 10 Epoch 41 Loss 0.852 Val Acc 48.300\n",
      "L 10 Epoch 42 Loss 0.865 Val Acc 45.900\n",
      "==> Saving model ...\n",
      "L 10 Epoch 43 Loss 0.745 Val Acc 49.760\n",
      "==> Saving model ...\n",
      "L 10 Epoch 44 Loss 0.570 Val Acc 49.880\n",
      "L 10 Epoch 45 Loss 0.520 Val Acc 46.540\n",
      "L 10 Epoch 46 Loss 0.550 Val Acc 46.240\n",
      "L 10 Epoch 47 Loss 0.689 Val Acc 47.000\n",
      "L 10 Epoch 48 Loss 0.573 Val Acc 49.260\n",
      "L 10 Epoch 49 Loss 0.604 Val Acc 49.460\n",
      "L 10 Epoch 50 Loss 0.735 Val Acc 46.200\n",
      "==> Saving model ...\n",
      "L 10 Epoch 51 Loss 0.566 Val Acc 51.400\n",
      "L 10 Epoch 52 Loss 0.564 Val Acc 47.640\n",
      "==> Saving model ...\n",
      "L 10 Epoch 53 Loss 0.471 Val Acc 52.180\n",
      "L 10 Epoch 54 Loss 0.481 Val Acc 50.140\n",
      "L 10 Epoch 55 Loss 0.472 Val Acc 50.160\n",
      "L 10 Epoch 56 Loss 0.502 Val Acc 47.060\n",
      "L 10 Epoch 57 Loss 0.342 Val Acc 50.220\n",
      "L 10 Epoch 58 Loss 0.447 Val Acc 50.600\n",
      "L 10 Epoch 59 Loss 0.393 Val Acc 49.180\n",
      "L 10 Epoch 60 Loss 0.462 Val Acc 49.080\n",
      "==> Saving model ...\n",
      "L 10 Epoch 61 Loss 0.286 Val Acc 52.500\n",
      "L 10 Epoch 62 Loss 0.368 Val Acc 51.160\n",
      "==> Saving model ...\n",
      "L 10 Epoch 63 Loss 0.295 Val Acc 53.000\n",
      "L 10 Epoch 64 Loss 0.284 Val Acc 50.700\n",
      "L 10 Epoch 65 Loss 0.278 Val Acc 52.340\n",
      "L 10 Epoch 66 Loss 0.264 Val Acc 50.420\n",
      "L 10 Epoch 67 Loss 0.303 Val Acc 51.980\n",
      "==> Saving model ...\n",
      "L 10 Epoch 68 Loss 0.166 Val Acc 53.180\n",
      "L 10 Epoch 69 Loss 0.200 Val Acc 52.820\n",
      "L 10 Epoch 70 Loss 0.141 Val Acc 53.120\n",
      "L 10 Epoch 71 Loss 0.270 Val Acc 51.340\n",
      "L 10 Epoch 72 Loss 0.182 Val Acc 51.000\n",
      "L 10 Epoch 73 Loss 0.159 Val Acc 53.120\n",
      "==> Saving model ...\n",
      "L 10 Epoch 74 Loss 0.242 Val Acc 53.740\n",
      "==> Saving model ...\n",
      "L 10 Epoch 75 Loss 0.120 Val Acc 54.080\n",
      "L 10 Epoch 76 Loss 0.153 Val Acc 53.760\n",
      "L 10 Epoch 77 Loss 0.173 Val Acc 52.540\n",
      "L 10 Epoch 78 Loss 0.129 Val Acc 51.940\n",
      "L 10 Epoch 79 Loss 0.143 Val Acc 53.380\n",
      "L 10 Epoch 80 Loss 0.215 Val Acc 51.900\n",
      "L 10 Epoch 81 Loss 0.177 Val Acc 52.920\n",
      "==> Saving model ...\n",
      "L 10 Epoch 82 Loss 0.124 Val Acc 54.640\n",
      "==> Saving model ...\n",
      "L 10 Epoch 83 Loss 0.145 Val Acc 55.700\n",
      "L 10 Epoch 84 Loss 0.103 Val Acc 53.700\n",
      "L 10 Epoch 85 Loss 0.174 Val Acc 52.880\n",
      "L 10 Epoch 86 Loss 0.098 Val Acc 53.700\n",
      "L 10 Epoch 87 Loss 0.139 Val Acc 51.600\n",
      "L 10 Epoch 88 Loss 0.133 Val Acc 52.320\n",
      "L 10 Epoch 89 Loss 0.114 Val Acc 51.160\n",
      "L 10 Epoch 90 Loss 0.111 Val Acc 54.500\n",
      "L 10 Epoch 91 Loss 0.093 Val Acc 53.600\n",
      "L 10 Epoch 92 Loss 0.104 Val Acc 54.780\n",
      "L 10 Epoch 93 Loss 0.139 Val Acc 53.100\n",
      "L 10 Epoch 94 Loss 0.079 Val Acc 53.440\n",
      "L 10 Epoch 95 Loss 0.136 Val Acc 54.160\n",
      "L 10 Epoch 96 Loss 0.102 Val Acc 54.700\n",
      "L 10 Epoch 97 Loss 0.118 Val Acc 54.180\n",
      "L 10 Epoch 98 Loss 0.075 Val Acc 54.120\n",
      "L 10 Epoch 99 Loss 0.123 Val Acc 54.020\n",
      "L 10 Epoch 100 Loss 0.124 Val Acc 52.620\n",
      "L 10 Epoch 101 Loss 0.065 Val Acc 52.880\n",
      "L 10 Epoch 102 Loss 0.131 Val Acc 55.560\n",
      "==> Saving model ...\n",
      "L 10 Epoch 103 Loss 0.087 Val Acc 56.020\n",
      "L 10 Epoch 104 Loss 0.091 Val Acc 53.180\n",
      "L 10 Epoch 105 Loss 0.086 Val Acc 55.420\n",
      "L 10 Epoch 106 Loss 0.079 Val Acc 55.540\n",
      "L 10 Epoch 107 Loss 0.060 Val Acc 55.720\n",
      "L 10 Epoch 108 Loss 0.066 Val Acc 55.660\n",
      "L 10 Epoch 109 Loss 0.066 Val Acc 55.380\n",
      "==> Saving model ...\n",
      "L 10 Epoch 110 Loss 0.051 Val Acc 56.180\n",
      "L 10 Epoch 111 Loss 0.076 Val Acc 55.540\n",
      "L 10 Epoch 112 Loss 0.062 Val Acc 54.980\n",
      "L 10 Epoch 113 Loss 0.060 Val Acc 55.500\n",
      "L 10 Epoch 114 Loss 0.080 Val Acc 55.640\n",
      "==> Saving model ...\n",
      "L 10 Epoch 115 Loss 0.066 Val Acc 56.440\n",
      "L 10 Epoch 116 Loss 0.047 Val Acc 55.340\n",
      "L 10 Epoch 117 Loss 0.086 Val Acc 55.340\n",
      "L 10 Epoch 118 Loss 0.061 Val Acc 55.520\n",
      "L 10 Epoch 119 Loss 0.033 Val Acc 56.000\n",
      "L 10 Epoch 120 Loss 0.059 Val Acc 55.300\n",
      "L 10 Epoch 121 Loss 0.022 Val Acc 55.920\n",
      "==> Saving model ...\n",
      "L 10 Epoch 122 Loss 0.019 Val Acc 56.860\n",
      "L 10 Epoch 123 Loss 0.039 Val Acc 56.360\n",
      "L 10 Epoch 124 Loss 0.028 Val Acc 56.860\n",
      "==> Saving model ...\n",
      "L 10 Epoch 125 Loss 0.030 Val Acc 57.000\n",
      "L 10 Epoch 126 Loss 0.029 Val Acc 56.320\n",
      "L 10 Epoch 127 Loss 0.022 Val Acc 55.880\n",
      "L 10 Epoch 128 Loss 0.026 Val Acc 55.780\n",
      "L 10 Epoch 129 Loss 0.055 Val Acc 55.620\n",
      "L 10 Epoch 130 Loss 0.021 Val Acc 56.500\n",
      "L 10 Epoch 131 Loss 0.025 Val Acc 56.580\n",
      "L 10 Epoch 132 Loss 0.032 Val Acc 56.440\n",
      "L 10 Epoch 133 Loss 0.027 Val Acc 56.640\n",
      "L 10 Epoch 134 Loss 0.026 Val Acc 56.380\n",
      "L 10 Epoch 135 Loss 0.024 Val Acc 56.080\n",
      "L 10 Epoch 136 Loss 0.039 Val Acc 55.920\n",
      "L 10 Epoch 137 Loss 0.022 Val Acc 56.020\n",
      "L 10 Epoch 138 Loss 0.043 Val Acc 56.440\n",
      "L 10 Epoch 139 Loss 0.022 Val Acc 56.260\n",
      "L 10 Epoch 140 Loss 0.028 Val Acc 56.200\n",
      "L 10 Epoch 141 Loss 0.020 Val Acc 56.680\n",
      "L 10 Epoch 142 Loss 0.033 Val Acc 56.140\n",
      "L 10 Epoch 143 Loss 0.018 Val Acc 56.440\n",
      "L 10 Epoch 144 Loss 0.025 Val Acc 56.740\n",
      "L 10 Epoch 145 Loss 0.023 Val Acc 56.160\n",
      "L 10 Epoch 146 Loss 0.011 Val Acc 56.680\n",
      "L 10 Epoch 147 Loss 0.034 Val Acc 56.800\n",
      "L 10 Epoch 148 Loss 0.008 Val Acc 56.860\n",
      "==> Saving model ...\n",
      "L 10 Epoch 149 Loss 0.017 Val Acc 57.180\n",
      "==> Saving model ...\n",
      "L 10 Epoch 150 Loss 0.014 Val Acc 57.420\n",
      "L 10 Epoch 151 Loss 0.011 Val Acc 57.340\n",
      "L 10 Epoch 152 Loss 0.020 Val Acc 57.280\n",
      "L 10 Epoch 153 Loss 0.021 Val Acc 56.820\n",
      "L 10 Epoch 154 Loss 0.015 Val Acc 57.000\n",
      "L 10 Epoch 155 Loss 0.024 Val Acc 56.740\n",
      "L 10 Epoch 156 Loss 0.020 Val Acc 56.700\n",
      "L 10 Epoch 157 Loss 0.029 Val Acc 56.660\n",
      "L 10 Epoch 158 Loss 0.004 Val Acc 56.820\n",
      "L 10 Epoch 159 Loss 0.012 Val Acc 56.800\n",
      "L 10 Epoch 160 Loss 0.016 Val Acc 56.680\n",
      "L 10 Epoch 161 Loss 0.019 Val Acc 56.360\n",
      "L 10 Epoch 162 Loss 0.016 Val Acc 56.920\n",
      "L 10 Epoch 163 Loss 0.019 Val Acc 56.840\n",
      "L 10 Epoch 164 Loss 0.027 Val Acc 56.620\n",
      "L 10 Epoch 165 Loss 0.018 Val Acc 56.740\n",
      "L 10 Epoch 166 Loss 0.011 Val Acc 56.860\n",
      "L 10 Epoch 167 Loss 0.013 Val Acc 56.660\n",
      "L 10 Epoch 168 Loss 0.006 Val Acc 56.920\n",
      "L 10 Epoch 169 Loss 0.010 Val Acc 56.960\n",
      "L 10 Epoch 170 Loss 0.018 Val Acc 56.880\n",
      "L 10 Epoch 171 Loss 0.006 Val Acc 57.360\n",
      "L 10 Epoch 172 Loss 0.027 Val Acc 56.860\n",
      "L 10 Epoch 173 Loss 0.017 Val Acc 56.760\n",
      "L 10 Epoch 174 Loss 0.014 Val Acc 56.700\n",
      "L 10 Epoch 175 Loss 0.007 Val Acc 57.260\n",
      "==> Saving model ...\n",
      "L 10 Epoch 176 Loss 0.002 Val Acc 57.520\n",
      "L 10 Epoch 177 Loss 0.018 Val Acc 57.200\n",
      "L 10 Epoch 178 Loss 0.010 Val Acc 56.960\n",
      "L 10 Epoch 179 Loss 0.019 Val Acc 56.860\n",
      "L 10 Epoch 180 Loss 0.015 Val Acc 56.860\n",
      "L 10 Epoch 181 Loss 0.009 Val Acc 56.960\n",
      "L 10 Epoch 182 Loss 0.009 Val Acc 56.960\n",
      "L 10 Epoch 183 Loss 0.010 Val Acc 57.080\n",
      "L 10 Epoch 184 Loss 0.006 Val Acc 57.340\n",
      "L 10 Epoch 185 Loss 0.016 Val Acc 56.840\n",
      "L 10 Epoch 186 Loss 0.008 Val Acc 57.200\n",
      "L 10 Epoch 187 Loss 0.009 Val Acc 57.180\n",
      "L 10 Epoch 188 Loss 0.017 Val Acc 57.000\n",
      "L 10 Epoch 189 Loss 0.011 Val Acc 56.640\n",
      "L 10 Epoch 190 Loss 0.017 Val Acc 56.840\n",
      "L 10 Epoch 191 Loss 0.013 Val Acc 56.720\n",
      "L 10 Epoch 192 Loss 0.006 Val Acc 56.800\n",
      "L 10 Epoch 193 Loss 0.010 Val Acc 57.000\n",
      "L 10 Epoch 194 Loss 0.013 Val Acc 56.960\n",
      "L 10 Epoch 195 Loss 0.003 Val Acc 57.340\n",
      "L 10 Epoch 196 Loss 0.020 Val Acc 56.780\n",
      "L 10 Epoch 197 Loss 0.015 Val Acc 56.840\n",
      "L 10 Epoch 198 Loss 0.034 Val Acc 56.720\n",
      "L 10 Epoch 199 Loss 0.023 Val Acc 56.720\n",
      "L 4 Epoch 0 Loss 4.011 Val Acc 10.000\n",
      "L 4 Epoch 1 Loss 4.049 Val Acc 9.980\n",
      "L 4 Epoch 2 Loss 2.890 Val Acc 9.700\n",
      "L 4 Epoch 3 Loss 2.693 Val Acc 14.420\n",
      "L 4 Epoch 4 Loss 2.227 Val Acc 16.320\n",
      "L 4 Epoch 5 Loss 2.086 Val Acc 18.640\n",
      "L 4 Epoch 6 Loss 2.013 Val Acc 20.160\n",
      "L 4 Epoch 7 Loss 1.908 Val Acc 24.940\n",
      "L 4 Epoch 8 Loss 1.821 Val Acc 29.660\n",
      "L 4 Epoch 9 Loss 1.737 Val Acc 33.640\n",
      "L 4 Epoch 10 Loss 1.712 Val Acc 32.880\n",
      "L 4 Epoch 11 Loss 1.670 Val Acc 35.280\n",
      "L 4 Epoch 12 Loss 1.662 Val Acc 33.180\n",
      "L 4 Epoch 13 Loss 1.557 Val Acc 35.980\n",
      "L 4 Epoch 14 Loss 1.585 Val Acc 36.080\n",
      "L 4 Epoch 15 Loss 1.565 Val Acc 33.060\n",
      "L 4 Epoch 16 Loss 1.526 Val Acc 35.820\n",
      "L 4 Epoch 17 Loss 1.465 Val Acc 36.700\n",
      "L 4 Epoch 18 Loss 1.429 Val Acc 38.180\n",
      "L 4 Epoch 19 Loss 1.334 Val Acc 37.180\n",
      "L 4 Epoch 20 Loss 1.377 Val Acc 39.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 21 Loss 1.297 Val Acc 40.200\n",
      "==> Saving model ...\n",
      "L 4 Epoch 22 Loss 1.288 Val Acc 40.400\n",
      "==> Saving model ...\n",
      "L 4 Epoch 23 Loss 1.273 Val Acc 41.460\n",
      "L 4 Epoch 24 Loss 1.354 Val Acc 40.620\n",
      "==> Saving model ...\n",
      "L 4 Epoch 25 Loss 1.296 Val Acc 43.560\n",
      "L 4 Epoch 26 Loss 1.160 Val Acc 42.920\n",
      "L 4 Epoch 27 Loss 1.106 Val Acc 39.000\n",
      "L 4 Epoch 28 Loss 1.089 Val Acc 40.300\n",
      "L 4 Epoch 29 Loss 1.030 Val Acc 42.140\n",
      "L 4 Epoch 30 Loss 1.072 Val Acc 43.040\n",
      "L 4 Epoch 31 Loss 1.054 Val Acc 43.440\n",
      "L 4 Epoch 32 Loss 1.014 Val Acc 41.520\n",
      "L 4 Epoch 33 Loss 1.082 Val Acc 43.060\n",
      "==> Saving model ...\n",
      "L 4 Epoch 34 Loss 1.047 Val Acc 44.340\n",
      "L 4 Epoch 35 Loss 1.101 Val Acc 43.620\n",
      "L 4 Epoch 36 Loss 0.915 Val Acc 42.800\n",
      "L 4 Epoch 37 Loss 0.993 Val Acc 42.700\n",
      "==> Saving model ...\n",
      "L 4 Epoch 38 Loss 0.868 Val Acc 44.800\n",
      "==> Saving model ...\n",
      "L 4 Epoch 39 Loss 0.734 Val Acc 47.180\n",
      "L 4 Epoch 40 Loss 0.781 Val Acc 45.600\n",
      "L 4 Epoch 41 Loss 0.797 Val Acc 42.600\n",
      "==> Saving model ...\n",
      "L 4 Epoch 42 Loss 0.815 Val Acc 47.300\n",
      "==> Saving model ...\n",
      "L 4 Epoch 43 Loss 0.677 Val Acc 47.700\n",
      "L 4 Epoch 44 Loss 0.565 Val Acc 45.360\n",
      "L 4 Epoch 45 Loss 0.534 Val Acc 46.880\n",
      "L 4 Epoch 46 Loss 0.521 Val Acc 47.300\n",
      "L 4 Epoch 47 Loss 0.598 Val Acc 46.340\n",
      "L 4 Epoch 48 Loss 0.560 Val Acc 44.460\n",
      "L 4 Epoch 49 Loss 0.570 Val Acc 46.340\n",
      "==> Saving model ...\n",
      "L 4 Epoch 50 Loss 0.511 Val Acc 48.620\n",
      "L 4 Epoch 51 Loss 0.442 Val Acc 47.480\n",
      "L 4 Epoch 52 Loss 0.490 Val Acc 48.240\n",
      "==> Saving model ...\n",
      "L 4 Epoch 53 Loss 0.476 Val Acc 48.760\n",
      "L 4 Epoch 54 Loss 0.466 Val Acc 47.280\n",
      "L 4 Epoch 55 Loss 0.463 Val Acc 47.660\n",
      "==> Saving model ...\n",
      "L 4 Epoch 56 Loss 0.364 Val Acc 49.000\n",
      "L 4 Epoch 57 Loss 0.361 Val Acc 47.120\n",
      "L 4 Epoch 58 Loss 0.265 Val Acc 48.600\n",
      "==> Saving model ...\n",
      "L 4 Epoch 59 Loss 0.352 Val Acc 49.760\n",
      "L 4 Epoch 60 Loss 0.236 Val Acc 47.240\n",
      "L 4 Epoch 61 Loss 0.369 Val Acc 42.480\n",
      "L 4 Epoch 62 Loss 0.296 Val Acc 48.840\n",
      "L 4 Epoch 63 Loss 0.371 Val Acc 48.180\n",
      "==> Saving model ...\n",
      "L 4 Epoch 64 Loss 0.202 Val Acc 50.060\n",
      "L 4 Epoch 65 Loss 0.234 Val Acc 48.440\n",
      "L 4 Epoch 66 Loss 0.282 Val Acc 47.000\n",
      "L 4 Epoch 67 Loss 0.227 Val Acc 50.040\n",
      "==> Saving model ...\n",
      "L 4 Epoch 68 Loss 0.154 Val Acc 50.360\n",
      "==> Saving model ...\n",
      "L 4 Epoch 69 Loss 0.275 Val Acc 50.740\n",
      "L 4 Epoch 70 Loss 0.302 Val Acc 48.240\n",
      "L 4 Epoch 71 Loss 0.164 Val Acc 49.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 72 Loss 0.144 Val Acc 51.580\n",
      "L 4 Epoch 73 Loss 0.197 Val Acc 50.300\n",
      "L 4 Epoch 74 Loss 0.231 Val Acc 50.640\n",
      "L 4 Epoch 75 Loss 0.150 Val Acc 50.620\n",
      "L 4 Epoch 76 Loss 0.162 Val Acc 51.140\n",
      "L 4 Epoch 77 Loss 0.130 Val Acc 51.420\n",
      "L 4 Epoch 78 Loss 0.131 Val Acc 50.900\n",
      "==> Saving model ...\n",
      "L 4 Epoch 79 Loss 0.175 Val Acc 51.620\n",
      "L 4 Epoch 80 Loss 0.134 Val Acc 50.640\n",
      "L 4 Epoch 81 Loss 0.172 Val Acc 51.460\n",
      "L 4 Epoch 82 Loss 0.173 Val Acc 51.100\n",
      "L 4 Epoch 83 Loss 0.123 Val Acc 51.320\n",
      "L 4 Epoch 84 Loss 0.124 Val Acc 51.260\n",
      "L 4 Epoch 85 Loss 0.192 Val Acc 51.160\n",
      "L 4 Epoch 86 Loss 0.150 Val Acc 51.040\n",
      "L 4 Epoch 87 Loss 0.180 Val Acc 50.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 88 Loss 0.155 Val Acc 52.200\n",
      "L 4 Epoch 89 Loss 0.137 Val Acc 49.340\n",
      "L 4 Epoch 90 Loss 0.073 Val Acc 51.420\n",
      "==> Saving model ...\n",
      "L 4 Epoch 91 Loss 0.090 Val Acc 54.060\n",
      "L 4 Epoch 92 Loss 0.099 Val Acc 51.900\n",
      "L 4 Epoch 93 Loss 0.100 Val Acc 53.840\n",
      "L 4 Epoch 94 Loss 0.129 Val Acc 52.520\n",
      "L 4 Epoch 95 Loss 0.130 Val Acc 51.800\n",
      "L 4 Epoch 96 Loss 0.098 Val Acc 52.720\n",
      "L 4 Epoch 97 Loss 0.094 Val Acc 52.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 98 Loss 0.100 Val Acc 54.700\n",
      "L 4 Epoch 99 Loss 0.093 Val Acc 54.060\n",
      "L 4 Epoch 100 Loss 0.068 Val Acc 53.980\n",
      "L 4 Epoch 101 Loss 0.086 Val Acc 53.700\n",
      "L 4 Epoch 102 Loss 0.045 Val Acc 52.960\n",
      "L 4 Epoch 103 Loss 0.058 Val Acc 52.960\n",
      "L 4 Epoch 104 Loss 0.071 Val Acc 53.460\n",
      "L 4 Epoch 105 Loss 0.082 Val Acc 54.100\n",
      "L 4 Epoch 106 Loss 0.036 Val Acc 53.780\n",
      "L 4 Epoch 107 Loss 0.066 Val Acc 53.640\n",
      "L 4 Epoch 108 Loss 0.058 Val Acc 53.000\n",
      "L 4 Epoch 109 Loss 0.058 Val Acc 53.140\n",
      "L 4 Epoch 110 Loss 0.073 Val Acc 53.700\n",
      "L 4 Epoch 111 Loss 0.047 Val Acc 54.640\n",
      "L 4 Epoch 112 Loss 0.048 Val Acc 54.340\n",
      "L 4 Epoch 113 Loss 0.060 Val Acc 54.580\n",
      "L 4 Epoch 114 Loss 0.061 Val Acc 52.340\n",
      "L 4 Epoch 115 Loss 0.046 Val Acc 53.020\n",
      "L 4 Epoch 116 Loss 0.052 Val Acc 54.260\n",
      "L 4 Epoch 117 Loss 0.044 Val Acc 53.460\n",
      "L 4 Epoch 118 Loss 0.035 Val Acc 53.840\n",
      "L 4 Epoch 119 Loss 0.044 Val Acc 54.040\n",
      "L 4 Epoch 120 Loss 0.054 Val Acc 53.800\n",
      "L 4 Epoch 121 Loss 0.052 Val Acc 54.120\n",
      "L 4 Epoch 122 Loss 0.030 Val Acc 54.560\n",
      "L 4 Epoch 123 Loss 0.038 Val Acc 53.920\n",
      "L 4 Epoch 124 Loss 0.044 Val Acc 53.320\n",
      "L 4 Epoch 125 Loss 0.024 Val Acc 53.720\n",
      "L 4 Epoch 126 Loss 0.030 Val Acc 54.360\n",
      "==> Saving model ...\n",
      "L 4 Epoch 127 Loss 0.024 Val Acc 54.940\n",
      "L 4 Epoch 128 Loss 0.030 Val Acc 54.780\n",
      "==> Saving model ...\n",
      "L 4 Epoch 129 Loss 0.059 Val Acc 55.080\n",
      "L 4 Epoch 130 Loss 0.043 Val Acc 54.080\n",
      "L 4 Epoch 131 Loss 0.036 Val Acc 53.640\n",
      "L 4 Epoch 132 Loss 0.018 Val Acc 53.920\n",
      "L 4 Epoch 133 Loss 0.045 Val Acc 54.340\n",
      "L 4 Epoch 134 Loss 0.051 Val Acc 54.340\n",
      "L 4 Epoch 135 Loss 0.021 Val Acc 54.420\n",
      "L 4 Epoch 136 Loss 0.059 Val Acc 54.580\n",
      "L 4 Epoch 137 Loss 0.008 Val Acc 54.240\n",
      "L 4 Epoch 138 Loss 0.027 Val Acc 54.340\n",
      "L 4 Epoch 139 Loss 0.037 Val Acc 54.580\n",
      "L 4 Epoch 140 Loss 0.032 Val Acc 54.580\n",
      "L 4 Epoch 141 Loss 0.032 Val Acc 54.420\n",
      "L 4 Epoch 142 Loss 0.017 Val Acc 54.400\n",
      "L 4 Epoch 143 Loss 0.010 Val Acc 54.480\n",
      "L 4 Epoch 144 Loss 0.017 Val Acc 54.800\n",
      "==> Saving model ...\n",
      "L 4 Epoch 145 Loss 0.012 Val Acc 55.300\n",
      "==> Saving model ...\n",
      "L 4 Epoch 146 Loss 0.014 Val Acc 55.680\n",
      "L 4 Epoch 147 Loss 0.033 Val Acc 55.140\n",
      "L 4 Epoch 148 Loss 0.022 Val Acc 55.080\n",
      "L 4 Epoch 149 Loss 0.029 Val Acc 54.740\n",
      "L 4 Epoch 150 Loss 0.023 Val Acc 54.700\n",
      "L 4 Epoch 151 Loss 0.026 Val Acc 54.860\n",
      "L 4 Epoch 152 Loss 0.010 Val Acc 54.940\n",
      "L 4 Epoch 153 Loss 0.017 Val Acc 54.940\n",
      "L 4 Epoch 154 Loss 0.027 Val Acc 55.120\n",
      "L 4 Epoch 155 Loss 0.017 Val Acc 55.140\n",
      "L 4 Epoch 156 Loss 0.025 Val Acc 55.400\n",
      "L 4 Epoch 157 Loss 0.024 Val Acc 55.200\n",
      "L 4 Epoch 158 Loss 0.023 Val Acc 55.320\n",
      "L 4 Epoch 159 Loss 0.021 Val Acc 55.320\n",
      "L 4 Epoch 160 Loss 0.007 Val Acc 55.300\n",
      "L 4 Epoch 161 Loss 0.020 Val Acc 55.200\n",
      "L 4 Epoch 162 Loss 0.010 Val Acc 55.340\n",
      "L 4 Epoch 163 Loss 0.021 Val Acc 55.340\n",
      "L 4 Epoch 164 Loss 0.018 Val Acc 55.080\n",
      "L 4 Epoch 165 Loss 0.011 Val Acc 55.240\n",
      "L 4 Epoch 166 Loss 0.012 Val Acc 55.360\n",
      "L 4 Epoch 167 Loss 0.015 Val Acc 55.480\n",
      "L 4 Epoch 168 Loss 0.013 Val Acc 55.220\n",
      "L 4 Epoch 169 Loss 0.015 Val Acc 55.300\n",
      "L 4 Epoch 170 Loss 0.005 Val Acc 55.440\n",
      "==> Saving model ...\n",
      "L 4 Epoch 171 Loss 0.008 Val Acc 55.720\n",
      "==> Saving model ...\n",
      "L 4 Epoch 172 Loss 0.007 Val Acc 55.780\n",
      "==> Saving model ...\n",
      "L 4 Epoch 173 Loss 0.006 Val Acc 55.840\n",
      "L 4 Epoch 174 Loss 0.018 Val Acc 55.840\n",
      "L 4 Epoch 175 Loss 0.012 Val Acc 55.760\n",
      "L 4 Epoch 176 Loss 0.006 Val Acc 55.680\n",
      "L 4 Epoch 177 Loss 0.024 Val Acc 55.180\n",
      "L 4 Epoch 178 Loss 0.015 Val Acc 55.380\n",
      "L 4 Epoch 179 Loss 0.011 Val Acc 55.380\n",
      "==> Saving model ...\n",
      "L 4 Epoch 180 Loss 0.005 Val Acc 55.900\n",
      "L 4 Epoch 181 Loss 0.014 Val Acc 55.620\n",
      "L 4 Epoch 182 Loss 0.011 Val Acc 55.700\n",
      "L 4 Epoch 183 Loss 0.023 Val Acc 55.140\n",
      "L 4 Epoch 184 Loss 0.013 Val Acc 55.700\n",
      "L 4 Epoch 185 Loss 0.019 Val Acc 55.260\n",
      "L 4 Epoch 186 Loss 0.008 Val Acc 55.580\n",
      "L 4 Epoch 187 Loss 0.009 Val Acc 55.780\n",
      "L 4 Epoch 188 Loss 0.016 Val Acc 55.600\n",
      "L 4 Epoch 189 Loss 0.013 Val Acc 55.620\n",
      "L 4 Epoch 190 Loss 0.014 Val Acc 55.000\n",
      "L 4 Epoch 191 Loss 0.005 Val Acc 55.700\n",
      "L 4 Epoch 192 Loss 0.009 Val Acc 55.740\n",
      "L 4 Epoch 193 Loss 0.022 Val Acc 55.600\n",
      "L 4 Epoch 194 Loss 0.016 Val Acc 55.540\n",
      "L 4 Epoch 195 Loss 0.015 Val Acc 55.780\n",
      "L 4 Epoch 196 Loss 0.022 Val Acc 55.620\n",
      "L 4 Epoch 197 Loss 0.015 Val Acc 55.480\n",
      "L 4 Epoch 198 Loss 0.009 Val Acc 55.560\n",
      "L 4 Epoch 199 Loss 0.006 Val Acc 55.740\n",
      "L 6 Epoch 0 Loss 4.163 Val Acc 10.000\n",
      "L 6 Epoch 1 Loss 3.660 Val Acc 10.000\n",
      "L 6 Epoch 2 Loss 3.004 Val Acc 10.940\n",
      "L 6 Epoch 3 Loss 2.416 Val Acc 10.140\n",
      "L 6 Epoch 4 Loss 2.241 Val Acc 16.920\n",
      "L 6 Epoch 5 Loss 2.059 Val Acc 19.300\n",
      "L 6 Epoch 6 Loss 1.990 Val Acc 22.940\n",
      "L 6 Epoch 7 Loss 1.884 Val Acc 28.660\n",
      "L 6 Epoch 8 Loss 1.821 Val Acc 28.720\n",
      "L 6 Epoch 9 Loss 1.703 Val Acc 32.260\n",
      "L 6 Epoch 10 Loss 1.758 Val Acc 29.160\n",
      "L 6 Epoch 11 Loss 1.738 Val Acc 35.580\n",
      "L 6 Epoch 12 Loss 1.656 Val Acc 34.120\n",
      "L 6 Epoch 13 Loss 1.616 Val Acc 35.060\n",
      "L 6 Epoch 14 Loss 1.594 Val Acc 36.700\n",
      "L 6 Epoch 15 Loss 1.561 Val Acc 34.920\n",
      "L 6 Epoch 16 Loss 1.515 Val Acc 39.720\n",
      "L 6 Epoch 17 Loss 1.509 Val Acc 37.200\n",
      "L 6 Epoch 18 Loss 1.462 Val Acc 38.960\n",
      "L 6 Epoch 19 Loss 1.424 Val Acc 39.160\n",
      "==> Saving model ...\n",
      "L 6 Epoch 20 Loss 1.460 Val Acc 40.140\n",
      "==> Saving model ...\n",
      "L 6 Epoch 21 Loss 1.416 Val Acc 40.300\n",
      "L 6 Epoch 22 Loss 1.365 Val Acc 39.820\n",
      "L 6 Epoch 23 Loss 1.354 Val Acc 39.060\n",
      "==> Saving model ...\n",
      "L 6 Epoch 24 Loss 1.269 Val Acc 41.160\n",
      "==> Saving model ...\n",
      "L 6 Epoch 25 Loss 1.273 Val Acc 43.540\n",
      "L 6 Epoch 26 Loss 1.249 Val Acc 38.460\n",
      "L 6 Epoch 27 Loss 1.324 Val Acc 41.100\n",
      "L 6 Epoch 28 Loss 1.221 Val Acc 41.180\n",
      "L 6 Epoch 29 Loss 1.225 Val Acc 41.080\n",
      "L 6 Epoch 30 Loss 1.174 Val Acc 43.080\n",
      "==> Saving model ...\n",
      "L 6 Epoch 31 Loss 1.134 Val Acc 44.100\n",
      "==> Saving model ...\n",
      "L 6 Epoch 32 Loss 1.048 Val Acc 44.120\n",
      "L 6 Epoch 33 Loss 1.093 Val Acc 42.640\n",
      "L 6 Epoch 34 Loss 1.086 Val Acc 43.260\n",
      "L 6 Epoch 35 Loss 0.971 Val Acc 42.520\n",
      "L 6 Epoch 36 Loss 0.906 Val Acc 43.380\n",
      "==> Saving model ...\n",
      "L 6 Epoch 37 Loss 0.817 Val Acc 45.520\n",
      "L 6 Epoch 38 Loss 0.870 Val Acc 44.740\n",
      "L 6 Epoch 39 Loss 0.989 Val Acc 41.360\n",
      "L 6 Epoch 40 Loss 0.928 Val Acc 43.380\n",
      "==> Saving model ...\n",
      "L 6 Epoch 41 Loss 0.770 Val Acc 45.560\n",
      "==> Saving model ...\n",
      "L 6 Epoch 42 Loss 0.614 Val Acc 46.620\n",
      "L 6 Epoch 43 Loss 0.698 Val Acc 45.840\n",
      "L 6 Epoch 44 Loss 0.636 Val Acc 40.300\n",
      "L 6 Epoch 45 Loss 0.746 Val Acc 39.320\n",
      "L 6 Epoch 46 Loss 0.702 Val Acc 45.200\n",
      "==> Saving model ...\n",
      "L 6 Epoch 47 Loss 0.531 Val Acc 46.980\n",
      "==> Saving model ...\n",
      "L 6 Epoch 48 Loss 0.601 Val Acc 47.840\n",
      "L 6 Epoch 49 Loss 0.423 Val Acc 47.000\n",
      "L 6 Epoch 50 Loss 0.491 Val Acc 45.740\n",
      "L 6 Epoch 51 Loss 0.538 Val Acc 45.620\n",
      "==> Saving model ...\n",
      "L 6 Epoch 52 Loss 0.457 Val Acc 48.020\n",
      "L 6 Epoch 53 Loss 0.492 Val Acc 46.460\n",
      "L 6 Epoch 54 Loss 0.389 Val Acc 46.720\n",
      "==> Saving model ...\n",
      "L 6 Epoch 55 Loss 0.373 Val Acc 48.320\n",
      "==> Saving model ...\n",
      "L 6 Epoch 56 Loss 0.474 Val Acc 48.820\n",
      "L 6 Epoch 57 Loss 0.399 Val Acc 48.080\n",
      "==> Saving model ...\n",
      "L 6 Epoch 58 Loss 0.404 Val Acc 49.600\n",
      "==> Saving model ...\n",
      "L 6 Epoch 59 Loss 0.306 Val Acc 50.140\n",
      "L 6 Epoch 60 Loss 0.317 Val Acc 49.520\n",
      "==> Saving model ...\n",
      "L 6 Epoch 61 Loss 0.295 Val Acc 50.700\n",
      "L 6 Epoch 62 Loss 0.317 Val Acc 47.780\n",
      "L 6 Epoch 63 Loss 0.293 Val Acc 47.880\n",
      "L 6 Epoch 64 Loss 0.330 Val Acc 48.860\n",
      "L 6 Epoch 65 Loss 0.336 Val Acc 50.240\n",
      "L 6 Epoch 66 Loss 0.333 Val Acc 50.620\n",
      "L 6 Epoch 67 Loss 0.226 Val Acc 48.180\n",
      "L 6 Epoch 68 Loss 0.228 Val Acc 48.960\n",
      "L 6 Epoch 69 Loss 0.166 Val Acc 50.320\n",
      "L 6 Epoch 70 Loss 0.193 Val Acc 50.000\n",
      "L 6 Epoch 71 Loss 0.166 Val Acc 50.000\n",
      "L 6 Epoch 72 Loss 0.103 Val Acc 49.500\n",
      "L 6 Epoch 73 Loss 0.185 Val Acc 50.080\n",
      "==> Saving model ...\n",
      "L 6 Epoch 74 Loss 0.096 Val Acc 50.780\n",
      "L 6 Epoch 75 Loss 0.159 Val Acc 49.580\n",
      "L 6 Epoch 76 Loss 0.130 Val Acc 49.320\n",
      "==> Saving model ...\n",
      "L 6 Epoch 77 Loss 0.131 Val Acc 51.520\n",
      "==> Saving model ...\n",
      "L 6 Epoch 78 Loss 0.121 Val Acc 51.780\n",
      "L 6 Epoch 79 Loss 0.114 Val Acc 51.400\n",
      "L 6 Epoch 80 Loss 0.137 Val Acc 50.160\n",
      "L 6 Epoch 81 Loss 0.122 Val Acc 46.440\n",
      "L 6 Epoch 82 Loss 0.078 Val Acc 49.120\n",
      "L 6 Epoch 83 Loss 0.124 Val Acc 51.320\n",
      "==> Saving model ...\n",
      "L 6 Epoch 84 Loss 0.139 Val Acc 52.000\n",
      "L 6 Epoch 85 Loss 0.085 Val Acc 50.540\n",
      "L 6 Epoch 86 Loss 0.127 Val Acc 51.180\n",
      "L 6 Epoch 87 Loss 0.108 Val Acc 51.340\n",
      "L 6 Epoch 88 Loss 0.125 Val Acc 51.880\n",
      "L 6 Epoch 89 Loss 0.145 Val Acc 50.820\n",
      "L 6 Epoch 90 Loss 0.196 Val Acc 49.620\n",
      "L 6 Epoch 91 Loss 0.119 Val Acc 51.140\n",
      "==> Saving model ...\n",
      "L 6 Epoch 92 Loss 0.104 Val Acc 52.060\n",
      "L 6 Epoch 93 Loss 0.080 Val Acc 51.540\n",
      "L 6 Epoch 94 Loss 0.137 Val Acc 50.860\n",
      "L 6 Epoch 95 Loss 0.118 Val Acc 50.400\n",
      "L 6 Epoch 96 Loss 0.092 Val Acc 50.320\n",
      "L 6 Epoch 97 Loss 0.087 Val Acc 51.480\n",
      "==> Saving model ...\n",
      "L 6 Epoch 98 Loss 0.173 Val Acc 53.600\n",
      "L 6 Epoch 99 Loss 0.182 Val Acc 49.960\n",
      "L 6 Epoch 100 Loss 0.123 Val Acc 50.520\n",
      "L 6 Epoch 101 Loss 0.101 Val Acc 52.540\n",
      "L 6 Epoch 102 Loss 0.102 Val Acc 51.140\n",
      "L 6 Epoch 103 Loss 0.057 Val Acc 52.540\n",
      "L 6 Epoch 104 Loss 0.090 Val Acc 52.080\n",
      "L 6 Epoch 105 Loss 0.040 Val Acc 51.180\n",
      "L 6 Epoch 106 Loss 0.099 Val Acc 53.120\n",
      "L 6 Epoch 107 Loss 0.050 Val Acc 53.100\n",
      "L 6 Epoch 108 Loss 0.075 Val Acc 52.100\n",
      "L 6 Epoch 109 Loss 0.059 Val Acc 51.560\n",
      "L 6 Epoch 110 Loss 0.071 Val Acc 52.960\n",
      "L 6 Epoch 111 Loss 0.048 Val Acc 52.720\n",
      "L 6 Epoch 112 Loss 0.076 Val Acc 52.500\n",
      "L 6 Epoch 113 Loss 0.044 Val Acc 53.520\n",
      "L 6 Epoch 114 Loss 0.062 Val Acc 53.040\n",
      "L 6 Epoch 115 Loss 0.051 Val Acc 53.140\n",
      "L 6 Epoch 116 Loss 0.044 Val Acc 52.400\n",
      "L 6 Epoch 117 Loss 0.074 Val Acc 52.780\n",
      "L 6 Epoch 118 Loss 0.072 Val Acc 53.400\n",
      "L 6 Epoch 119 Loss 0.055 Val Acc 52.600\n",
      "L 6 Epoch 120 Loss 0.065 Val Acc 52.840\n",
      "L 6 Epoch 121 Loss 0.046 Val Acc 53.440\n",
      "L 6 Epoch 122 Loss 0.034 Val Acc 53.480\n",
      "==> Saving model ...\n",
      "L 6 Epoch 123 Loss 0.039 Val Acc 54.460\n",
      "L 6 Epoch 124 Loss 0.048 Val Acc 54.240\n",
      "L 6 Epoch 125 Loss 0.048 Val Acc 53.840\n",
      "L 6 Epoch 126 Loss 0.016 Val Acc 53.480\n",
      "L 6 Epoch 127 Loss 0.035 Val Acc 53.680\n",
      "L 6 Epoch 128 Loss 0.041 Val Acc 52.700\n",
      "L 6 Epoch 129 Loss 0.035 Val Acc 54.040\n",
      "L 6 Epoch 130 Loss 0.029 Val Acc 54.240\n",
      "L 6 Epoch 131 Loss 0.050 Val Acc 53.660\n",
      "L 6 Epoch 132 Loss 0.034 Val Acc 52.740\n",
      "L 6 Epoch 133 Loss 0.021 Val Acc 54.080\n",
      "L 6 Epoch 134 Loss 0.011 Val Acc 54.100\n",
      "L 6 Epoch 135 Loss 0.029 Val Acc 53.660\n",
      "L 6 Epoch 136 Loss 0.025 Val Acc 54.200\n",
      "L 6 Epoch 137 Loss 0.024 Val Acc 53.940\n",
      "L 6 Epoch 138 Loss 0.023 Val Acc 54.240\n",
      "L 6 Epoch 139 Loss 0.025 Val Acc 54.040\n",
      "L 6 Epoch 140 Loss 0.027 Val Acc 54.000\n",
      "L 6 Epoch 141 Loss 0.019 Val Acc 53.600\n",
      "L 6 Epoch 142 Loss 0.024 Val Acc 53.820\n",
      "L 6 Epoch 143 Loss 0.026 Val Acc 54.320\n",
      "==> Saving model ...\n",
      "L 6 Epoch 144 Loss 0.018 Val Acc 54.580\n",
      "L 6 Epoch 145 Loss 0.014 Val Acc 54.460\n",
      "L 6 Epoch 146 Loss 0.017 Val Acc 54.500\n",
      "L 6 Epoch 147 Loss 0.022 Val Acc 54.440\n",
      "L 6 Epoch 148 Loss 0.022 Val Acc 54.340\n",
      "L 6 Epoch 149 Loss 0.028 Val Acc 54.140\n",
      "L 6 Epoch 150 Loss 0.017 Val Acc 54.400\n",
      "L 6 Epoch 151 Loss 0.028 Val Acc 54.280\n",
      "L 6 Epoch 152 Loss 0.018 Val Acc 54.320\n",
      "L 6 Epoch 153 Loss 0.030 Val Acc 54.040\n",
      "L 6 Epoch 154 Loss 0.015 Val Acc 54.140\n",
      "L 6 Epoch 155 Loss 0.019 Val Acc 54.380\n",
      "L 6 Epoch 156 Loss 0.023 Val Acc 54.160\n",
      "L 6 Epoch 157 Loss 0.005 Val Acc 54.540\n",
      "L 6 Epoch 158 Loss 0.019 Val Acc 54.160\n",
      "L 6 Epoch 159 Loss 0.029 Val Acc 53.860\n",
      "L 6 Epoch 160 Loss 0.006 Val Acc 54.440\n",
      "L 6 Epoch 161 Loss 0.031 Val Acc 53.860\n",
      "L 6 Epoch 162 Loss 0.021 Val Acc 54.220\n",
      "==> Saving model ...\n",
      "L 6 Epoch 163 Loss 0.008 Val Acc 54.740\n",
      "==> Saving model ...\n",
      "L 6 Epoch 164 Loss 0.021 Val Acc 54.840\n",
      "==> Saving model ...\n",
      "L 6 Epoch 165 Loss 0.002 Val Acc 55.240\n",
      "L 6 Epoch 166 Loss 0.010 Val Acc 55.040\n",
      "L 6 Epoch 167 Loss 0.020 Val Acc 54.800\n",
      "L 6 Epoch 168 Loss 0.013 Val Acc 54.920\n",
      "L 6 Epoch 169 Loss 0.015 Val Acc 54.860\n",
      "L 6 Epoch 170 Loss 0.033 Val Acc 54.580\n",
      "L 6 Epoch 171 Loss 0.016 Val Acc 54.380\n",
      "L 6 Epoch 172 Loss 0.019 Val Acc 54.520\n",
      "L 6 Epoch 173 Loss 0.019 Val Acc 54.560\n",
      "L 6 Epoch 174 Loss 0.007 Val Acc 54.680\n",
      "L 6 Epoch 175 Loss 0.008 Val Acc 55.000\n",
      "L 6 Epoch 176 Loss 0.025 Val Acc 54.780\n",
      "L 6 Epoch 177 Loss 0.012 Val Acc 54.840\n",
      "L 6 Epoch 178 Loss 0.008 Val Acc 55.100\n",
      "L 6 Epoch 179 Loss 0.023 Val Acc 54.760\n",
      "L 6 Epoch 180 Loss 0.015 Val Acc 54.900\n",
      "L 6 Epoch 181 Loss 0.019 Val Acc 54.780\n",
      "L 6 Epoch 182 Loss 0.009 Val Acc 55.080\n",
      "L 6 Epoch 183 Loss 0.019 Val Acc 54.900\n",
      "L 6 Epoch 184 Loss 0.014 Val Acc 54.880\n",
      "L 6 Epoch 185 Loss 0.020 Val Acc 55.160\n",
      "L 6 Epoch 186 Loss 0.021 Val Acc 54.980\n",
      "L 6 Epoch 187 Loss 0.010 Val Acc 54.920\n",
      "==> Saving model ...\n",
      "L 6 Epoch 188 Loss 0.009 Val Acc 55.320\n",
      "L 6 Epoch 189 Loss 0.027 Val Acc 54.940\n",
      "L 6 Epoch 190 Loss 0.014 Val Acc 54.960\n",
      "L 6 Epoch 191 Loss 0.011 Val Acc 55.260\n",
      "L 6 Epoch 192 Loss 0.017 Val Acc 55.240\n",
      "L 6 Epoch 193 Loss 0.013 Val Acc 55.220\n",
      "L 6 Epoch 194 Loss 0.015 Val Acc 55.100\n",
      "L 6 Epoch 195 Loss 0.005 Val Acc 55.160\n",
      "L 6 Epoch 196 Loss 0.010 Val Acc 54.960\n",
      "L 6 Epoch 197 Loss 0.011 Val Acc 55.160\n",
      "L 6 Epoch 198 Loss 0.006 Val Acc 55.240\n",
      "L 6 Epoch 199 Loss 0.012 Val Acc 55.040\n",
      "L 8 Epoch 0 Loss 4.177 Val Acc 10.000\n",
      "L 8 Epoch 1 Loss 3.066 Val Acc 10.200\n",
      "L 8 Epoch 2 Loss 2.357 Val Acc 10.220\n",
      "L 8 Epoch 3 Loss 2.249 Val Acc 14.360\n",
      "L 8 Epoch 4 Loss 2.069 Val Acc 17.680\n",
      "L 8 Epoch 5 Loss 1.959 Val Acc 22.160\n",
      "L 8 Epoch 6 Loss 1.875 Val Acc 25.080\n",
      "L 8 Epoch 7 Loss 1.803 Val Acc 26.060\n",
      "L 8 Epoch 8 Loss 1.738 Val Acc 32.320\n",
      "L 8 Epoch 9 Loss 1.744 Val Acc 29.660\n",
      "L 8 Epoch 10 Loss 1.647 Val Acc 33.160\n",
      "L 8 Epoch 11 Loss 1.621 Val Acc 34.640\n",
      "L 8 Epoch 12 Loss 1.619 Val Acc 33.940\n",
      "L 8 Epoch 13 Loss 1.647 Val Acc 34.340\n",
      "L 8 Epoch 14 Loss 1.664 Val Acc 35.240\n",
      "L 8 Epoch 15 Loss 1.515 Val Acc 36.860\n",
      "L 8 Epoch 16 Loss 1.485 Val Acc 35.960\n",
      "L 8 Epoch 17 Loss 1.463 Val Acc 37.400\n",
      "L 8 Epoch 18 Loss 1.430 Val Acc 38.680\n",
      "L 8 Epoch 19 Loss 1.423 Val Acc 39.840\n",
      "==> Saving model ...\n",
      "L 8 Epoch 20 Loss 1.301 Val Acc 40.540\n",
      "L 8 Epoch 21 Loss 1.303 Val Acc 36.860\n",
      "L 8 Epoch 22 Loss 1.368 Val Acc 38.720\n",
      "L 8 Epoch 23 Loss 1.294 Val Acc 40.120\n",
      "==> Saving model ...\n",
      "L 8 Epoch 24 Loss 1.238 Val Acc 42.160\n",
      "L 8 Epoch 25 Loss 1.244 Val Acc 41.960\n",
      "==> Saving model ...\n",
      "L 8 Epoch 26 Loss 1.152 Val Acc 42.300\n",
      "==> Saving model ...\n",
      "L 8 Epoch 27 Loss 1.190 Val Acc 44.780\n",
      "L 8 Epoch 28 Loss 1.169 Val Acc 40.900\n",
      "L 8 Epoch 29 Loss 1.120 Val Acc 40.700\n",
      "L 8 Epoch 30 Loss 1.038 Val Acc 43.380\n",
      "L 8 Epoch 31 Loss 1.007 Val Acc 43.100\n",
      "==> Saving model ...\n",
      "L 8 Epoch 32 Loss 0.948 Val Acc 44.820\n",
      "L 8 Epoch 33 Loss 0.863 Val Acc 44.040\n",
      "==> Saving model ...\n",
      "L 8 Epoch 34 Loss 0.922 Val Acc 45.480\n",
      "==> Saving model ...\n",
      "L 8 Epoch 35 Loss 0.873 Val Acc 47.480\n",
      "L 8 Epoch 36 Loss 0.908 Val Acc 46.660\n",
      "L 8 Epoch 37 Loss 0.804 Val Acc 45.700\n",
      "L 8 Epoch 38 Loss 0.701 Val Acc 41.980\n",
      "L 8 Epoch 39 Loss 0.641 Val Acc 45.460\n",
      "L 8 Epoch 40 Loss 0.773 Val Acc 46.600\n",
      "L 8 Epoch 41 Loss 0.733 Val Acc 45.920\n",
      "==> Saving model ...\n",
      "L 8 Epoch 42 Loss 0.648 Val Acc 47.680\n",
      "==> Saving model ...\n",
      "L 8 Epoch 43 Loss 0.583 Val Acc 47.820\n",
      "L 8 Epoch 44 Loss 0.527 Val Acc 47.520\n",
      "==> Saving model ...\n",
      "L 8 Epoch 45 Loss 0.535 Val Acc 48.420\n",
      "L 8 Epoch 46 Loss 0.471 Val Acc 47.860\n",
      "L 8 Epoch 47 Loss 0.442 Val Acc 47.940\n",
      "L 8 Epoch 48 Loss 0.368 Val Acc 47.680\n",
      "L 8 Epoch 49 Loss 0.382 Val Acc 46.300\n",
      "L 8 Epoch 50 Loss 0.538 Val Acc 46.680\n",
      "==> Saving model ...\n",
      "L 8 Epoch 51 Loss 0.534 Val Acc 48.500\n",
      "==> Saving model ...\n",
      "L 8 Epoch 52 Loss 0.350 Val Acc 50.760\n",
      "==> Saving model ...\n",
      "L 8 Epoch 53 Loss 0.359 Val Acc 50.800\n",
      "L 8 Epoch 54 Loss 0.442 Val Acc 47.700\n",
      "L 8 Epoch 55 Loss 0.536 Val Acc 49.180\n",
      "L 8 Epoch 56 Loss 0.483 Val Acc 48.960\n",
      "==> Saving model ...\n",
      "L 8 Epoch 57 Loss 0.328 Val Acc 51.000\n",
      "==> Saving model ...\n",
      "L 8 Epoch 58 Loss 0.352 Val Acc 52.580\n",
      "L 8 Epoch 59 Loss 0.317 Val Acc 49.740\n",
      "L 8 Epoch 60 Loss 0.271 Val Acc 49.800\n",
      "L 8 Epoch 61 Loss 0.279 Val Acc 50.300\n",
      "L 8 Epoch 62 Loss 0.297 Val Acc 49.040\n",
      "L 8 Epoch 63 Loss 0.259 Val Acc 50.560\n",
      "L 8 Epoch 64 Loss 0.157 Val Acc 51.000\n",
      "L 8 Epoch 65 Loss 0.172 Val Acc 52.020\n",
      "L 8 Epoch 66 Loss 0.153 Val Acc 51.840\n",
      "==> Saving model ...\n",
      "L 8 Epoch 67 Loss 0.200 Val Acc 52.600\n",
      "L 8 Epoch 68 Loss 0.126 Val Acc 52.040\n",
      "==> Saving model ...\n",
      "L 8 Epoch 69 Loss 0.194 Val Acc 53.080\n",
      "L 8 Epoch 70 Loss 0.155 Val Acc 51.080\n",
      "==> Saving model ...\n",
      "L 8 Epoch 71 Loss 0.198 Val Acc 53.100\n",
      "L 8 Epoch 72 Loss 0.145 Val Acc 52.200\n",
      "L 8 Epoch 73 Loss 0.142 Val Acc 52.900\n",
      "L 8 Epoch 74 Loss 0.209 Val Acc 52.180\n",
      "L 8 Epoch 75 Loss 0.060 Val Acc 51.940\n",
      "L 8 Epoch 76 Loss 0.160 Val Acc 52.160\n",
      "L 8 Epoch 77 Loss 0.143 Val Acc 52.560\n",
      "L 8 Epoch 78 Loss 0.136 Val Acc 52.560\n",
      "==> Saving model ...\n",
      "L 8 Epoch 79 Loss 0.123 Val Acc 53.500\n",
      "==> Saving model ...\n",
      "L 8 Epoch 80 Loss 0.106 Val Acc 54.380\n",
      "L 8 Epoch 81 Loss 0.113 Val Acc 53.520\n",
      "L 8 Epoch 82 Loss 0.146 Val Acc 53.180\n",
      "L 8 Epoch 83 Loss 0.134 Val Acc 51.020\n",
      "L 8 Epoch 84 Loss 0.188 Val Acc 52.960\n",
      "L 8 Epoch 85 Loss 0.154 Val Acc 53.360\n",
      "L 8 Epoch 86 Loss 0.156 Val Acc 49.880\n",
      "L 8 Epoch 87 Loss 0.098 Val Acc 54.020\n",
      "==> Saving model ...\n",
      "L 8 Epoch 88 Loss 0.138 Val Acc 55.560\n",
      "L 8 Epoch 89 Loss 0.104 Val Acc 52.180\n",
      "L 8 Epoch 90 Loss 0.102 Val Acc 54.000\n",
      "L 8 Epoch 91 Loss 0.053 Val Acc 54.540\n",
      "L 8 Epoch 92 Loss 0.121 Val Acc 53.660\n",
      "L 8 Epoch 93 Loss 0.118 Val Acc 53.080\n",
      "L 8 Epoch 94 Loss 0.105 Val Acc 53.400\n",
      "L 8 Epoch 95 Loss 0.141 Val Acc 52.360\n",
      "L 8 Epoch 96 Loss 0.134 Val Acc 54.620\n",
      "L 8 Epoch 97 Loss 0.091 Val Acc 54.100\n",
      "L 8 Epoch 98 Loss 0.043 Val Acc 54.620\n",
      "L 8 Epoch 99 Loss 0.113 Val Acc 54.360\n",
      "L 8 Epoch 100 Loss 0.059 Val Acc 53.420\n",
      "L 8 Epoch 101 Loss 0.052 Val Acc 54.160\n",
      "L 8 Epoch 102 Loss 0.049 Val Acc 54.460\n",
      "L 8 Epoch 103 Loss 0.050 Val Acc 54.460\n",
      "L 8 Epoch 104 Loss 0.084 Val Acc 54.760\n",
      "L 8 Epoch 105 Loss 0.050 Val Acc 54.580\n",
      "L 8 Epoch 106 Loss 0.078 Val Acc 55.040\n",
      "L 8 Epoch 107 Loss 0.074 Val Acc 54.060\n",
      "L 8 Epoch 108 Loss 0.082 Val Acc 54.400\n",
      "L 8 Epoch 109 Loss 0.056 Val Acc 55.100\n",
      "==> Saving model ...\n",
      "L 8 Epoch 110 Loss 0.042 Val Acc 56.620\n",
      "L 8 Epoch 111 Loss 0.031 Val Acc 55.340\n",
      "L 8 Epoch 112 Loss 0.027 Val Acc 56.200\n",
      "L 8 Epoch 113 Loss 0.067 Val Acc 56.420\n",
      "L 8 Epoch 114 Loss 0.047 Val Acc 56.140\n",
      "L 8 Epoch 115 Loss 0.041 Val Acc 56.020\n",
      "==> Saving model ...\n",
      "L 8 Epoch 116 Loss 0.056 Val Acc 56.740\n",
      "L 8 Epoch 117 Loss 0.052 Val Acc 55.840\n",
      "L 8 Epoch 118 Loss 0.019 Val Acc 56.060\n",
      "L 8 Epoch 119 Loss 0.046 Val Acc 56.160\n",
      "L 8 Epoch 120 Loss 0.040 Val Acc 55.300\n",
      "L 8 Epoch 121 Loss 0.019 Val Acc 55.740\n",
      "L 8 Epoch 122 Loss 0.021 Val Acc 55.900\n",
      "L 8 Epoch 123 Loss 0.034 Val Acc 56.680\n",
      "L 8 Epoch 124 Loss 0.012 Val Acc 56.140\n",
      "==> Saving model ...\n",
      "L 8 Epoch 125 Loss 0.028 Val Acc 56.820\n",
      "L 8 Epoch 126 Loss 0.019 Val Acc 56.500\n",
      "L 8 Epoch 127 Loss 0.014 Val Acc 56.500\n",
      "L 8 Epoch 128 Loss 0.037 Val Acc 56.780\n",
      "L 8 Epoch 129 Loss 0.025 Val Acc 56.060\n",
      "L 8 Epoch 130 Loss 0.050 Val Acc 56.480\n",
      "L 8 Epoch 131 Loss 0.064 Val Acc 56.240\n",
      "L 8 Epoch 132 Loss 0.034 Val Acc 56.240\n",
      "L 8 Epoch 133 Loss 0.009 Val Acc 55.720\n",
      "L 8 Epoch 134 Loss 0.024 Val Acc 55.560\n",
      "L 8 Epoch 135 Loss 0.020 Val Acc 55.760\n",
      "L 8 Epoch 136 Loss 0.031 Val Acc 56.100\n",
      "L 8 Epoch 137 Loss 0.015 Val Acc 55.900\n",
      "L 8 Epoch 138 Loss 0.030 Val Acc 56.700\n",
      "==> Saving model ...\n",
      "L 8 Epoch 139 Loss 0.016 Val Acc 56.900\n",
      "L 8 Epoch 140 Loss 0.021 Val Acc 56.720\n",
      "L 8 Epoch 141 Loss 0.026 Val Acc 56.080\n",
      "L 8 Epoch 142 Loss 0.019 Val Acc 56.520\n",
      "L 8 Epoch 143 Loss 0.027 Val Acc 56.280\n",
      "L 8 Epoch 144 Loss 0.026 Val Acc 56.040\n",
      "L 8 Epoch 145 Loss 0.033 Val Acc 55.940\n",
      "L 8 Epoch 146 Loss 0.014 Val Acc 56.300\n",
      "L 8 Epoch 147 Loss 0.017 Val Acc 56.220\n",
      "L 8 Epoch 148 Loss 0.024 Val Acc 56.620\n",
      "==> Saving model ...\n",
      "L 8 Epoch 149 Loss 0.016 Val Acc 57.140\n",
      "L 8 Epoch 150 Loss 0.019 Val Acc 57.100\n",
      "==> Saving model ...\n",
      "L 8 Epoch 151 Loss 0.010 Val Acc 57.380\n",
      "L 8 Epoch 152 Loss 0.026 Val Acc 56.760\n",
      "L 8 Epoch 153 Loss 0.009 Val Acc 57.040\n",
      "L 8 Epoch 154 Loss 0.013 Val Acc 56.900\n",
      "L 8 Epoch 155 Loss 0.017 Val Acc 56.460\n",
      "L 8 Epoch 156 Loss 0.010 Val Acc 56.480\n",
      "L 8 Epoch 157 Loss 0.013 Val Acc 56.640\n",
      "L 8 Epoch 158 Loss 0.017 Val Acc 56.860\n",
      "L 8 Epoch 159 Loss 0.013 Val Acc 57.060\n",
      "L 8 Epoch 160 Loss 0.012 Val Acc 57.080\n",
      "L 8 Epoch 161 Loss 0.009 Val Acc 56.960\n",
      "L 8 Epoch 162 Loss 0.021 Val Acc 56.960\n",
      "L 8 Epoch 163 Loss 0.020 Val Acc 56.660\n",
      "L 8 Epoch 164 Loss 0.022 Val Acc 56.680\n",
      "L 8 Epoch 165 Loss 0.010 Val Acc 56.880\n",
      "L 8 Epoch 166 Loss 0.010 Val Acc 56.980\n",
      "L 8 Epoch 167 Loss 0.011 Val Acc 57.200\n",
      "L 8 Epoch 168 Loss 0.011 Val Acc 57.160\n",
      "L 8 Epoch 169 Loss 0.016 Val Acc 57.200\n",
      "L 8 Epoch 170 Loss 0.017 Val Acc 57.340\n",
      "L 8 Epoch 171 Loss 0.008 Val Acc 57.300\n",
      "L 8 Epoch 172 Loss 0.017 Val Acc 57.220\n",
      "L 8 Epoch 173 Loss 0.021 Val Acc 57.040\n",
      "L 8 Epoch 174 Loss 0.017 Val Acc 57.120\n",
      "L 8 Epoch 175 Loss 0.016 Val Acc 57.380\n",
      "L 8 Epoch 176 Loss 0.013 Val Acc 57.200\n",
      "==> Saving model ...\n",
      "L 8 Epoch 177 Loss 0.008 Val Acc 57.500\n",
      "==> Saving model ...\n",
      "L 8 Epoch 178 Loss 0.005 Val Acc 57.760\n",
      "L 8 Epoch 179 Loss 0.021 Val Acc 57.100\n",
      "L 8 Epoch 180 Loss 0.021 Val Acc 57.420\n",
      "L 8 Epoch 181 Loss 0.013 Val Acc 57.620\n",
      "L 8 Epoch 182 Loss 0.014 Val Acc 57.420\n",
      "L 8 Epoch 183 Loss 0.018 Val Acc 57.560\n",
      "L 8 Epoch 184 Loss 0.010 Val Acc 57.580\n",
      "L 8 Epoch 185 Loss 0.008 Val Acc 57.580\n",
      "L 8 Epoch 186 Loss 0.010 Val Acc 57.700\n",
      "L 8 Epoch 187 Loss 0.006 Val Acc 57.700\n",
      "L 8 Epoch 188 Loss 0.014 Val Acc 57.460\n",
      "L 8 Epoch 189 Loss 0.007 Val Acc 57.520\n",
      "L 8 Epoch 190 Loss 0.002 Val Acc 57.620\n",
      "L 8 Epoch 191 Loss 0.011 Val Acc 57.560\n",
      "L 8 Epoch 192 Loss 0.014 Val Acc 57.620\n",
      "L 8 Epoch 193 Loss 0.029 Val Acc 57.220\n",
      "L 8 Epoch 194 Loss 0.008 Val Acc 57.480\n",
      "==> Saving model ...\n",
      "L 8 Epoch 195 Loss 0.009 Val Acc 57.900\n",
      "L 8 Epoch 196 Loss 0.011 Val Acc 57.680\n",
      "L 8 Epoch 197 Loss 0.011 Val Acc 57.600\n",
      "L 8 Epoch 198 Loss 0.010 Val Acc 57.620\n",
      "L 8 Epoch 199 Loss 0.016 Val Acc 57.560\n",
      "L 10 Epoch 0 Loss 3.788 Val Acc 10.000\n",
      "L 10 Epoch 1 Loss 2.687 Val Acc 9.980\n",
      "L 10 Epoch 2 Loss 2.612 Val Acc 9.900\n",
      "L 10 Epoch 3 Loss 2.395 Val Acc 11.340\n",
      "L 10 Epoch 4 Loss 2.183 Val Acc 11.860\n",
      "L 10 Epoch 5 Loss 2.018 Val Acc 11.180\n",
      "L 10 Epoch 6 Loss 1.942 Val Acc 16.540\n",
      "L 10 Epoch 7 Loss 1.913 Val Acc 18.860\n",
      "L 10 Epoch 8 Loss 1.796 Val Acc 33.000\n",
      "L 10 Epoch 9 Loss 1.680 Val Acc 36.520\n",
      "L 10 Epoch 10 Loss 1.625 Val Acc 36.500\n",
      "L 10 Epoch 11 Loss 1.612 Val Acc 38.340\n",
      "L 10 Epoch 12 Loss 1.556 Val Acc 33.360\n",
      "L 10 Epoch 13 Loss 1.521 Val Acc 37.980\n",
      "==> Saving model ...\n",
      "L 10 Epoch 14 Loss 1.473 Val Acc 40.240\n",
      "==> Saving model ...\n",
      "L 10 Epoch 15 Loss 1.412 Val Acc 41.500\n",
      "L 10 Epoch 16 Loss 1.366 Val Acc 40.780\n",
      "==> Saving model ...\n",
      "L 10 Epoch 17 Loss 1.337 Val Acc 42.380\n",
      "L 10 Epoch 18 Loss 1.325 Val Acc 40.940\n",
      "L 10 Epoch 19 Loss 1.290 Val Acc 42.100\n",
      "L 10 Epoch 20 Loss 1.236 Val Acc 37.380\n",
      "==> Saving model ...\n",
      "L 10 Epoch 21 Loss 1.220 Val Acc 43.220\n",
      "L 10 Epoch 22 Loss 1.266 Val Acc 40.220\n",
      "==> Saving model ...\n",
      "L 10 Epoch 23 Loss 1.139 Val Acc 45.320\n",
      "==> Saving model ...\n",
      "L 10 Epoch 24 Loss 1.031 Val Acc 45.840\n",
      "==> Saving model ...\n",
      "L 10 Epoch 25 Loss 0.928 Val Acc 47.180\n",
      "L 10 Epoch 26 Loss 1.019 Val Acc 37.460\n",
      "L 10 Epoch 27 Loss 1.041 Val Acc 45.800\n",
      "L 10 Epoch 28 Loss 0.869 Val Acc 45.400\n",
      "==> Saving model ...\n",
      "L 10 Epoch 29 Loss 0.962 Val Acc 48.040\n",
      "L 10 Epoch 30 Loss 0.769 Val Acc 47.600\n",
      "L 10 Epoch 31 Loss 0.795 Val Acc 48.020\n",
      "==> Saving model ...\n",
      "L 10 Epoch 32 Loss 0.786 Val Acc 48.260\n",
      "L 10 Epoch 33 Loss 0.859 Val Acc 43.380\n",
      "L 10 Epoch 34 Loss 0.793 Val Acc 46.800\n",
      "==> Saving model ...\n",
      "L 10 Epoch 35 Loss 0.671 Val Acc 49.240\n",
      "==> Saving model ...\n",
      "L 10 Epoch 36 Loss 0.643 Val Acc 49.460\n",
      "L 10 Epoch 37 Loss 0.668 Val Acc 49.060\n",
      "L 10 Epoch 38 Loss 0.632 Val Acc 49.020\n",
      "L 10 Epoch 39 Loss 0.516 Val Acc 48.620\n",
      "==> Saving model ...\n",
      "L 10 Epoch 40 Loss 0.507 Val Acc 51.060\n",
      "L 10 Epoch 41 Loss 0.545 Val Acc 43.820\n",
      "L 10 Epoch 42 Loss 0.404 Val Acc 48.340\n",
      "L 10 Epoch 43 Loss 0.421 Val Acc 43.040\n",
      "L 10 Epoch 44 Loss 0.480 Val Acc 47.440\n",
      "==> Saving model ...\n",
      "L 10 Epoch 45 Loss 0.414 Val Acc 52.460\n",
      "L 10 Epoch 46 Loss 0.438 Val Acc 49.940\n",
      "L 10 Epoch 47 Loss 0.404 Val Acc 52.160\n",
      "==> Saving model ...\n",
      "L 10 Epoch 48 Loss 0.390 Val Acc 52.700\n",
      "L 10 Epoch 49 Loss 0.446 Val Acc 49.940\n",
      "L 10 Epoch 50 Loss 0.383 Val Acc 51.620\n",
      "L 10 Epoch 51 Loss 0.328 Val Acc 49.640\n",
      "L 10 Epoch 52 Loss 0.320 Val Acc 52.280\n",
      "L 10 Epoch 53 Loss 0.297 Val Acc 50.280\n",
      "==> Saving model ...\n",
      "L 10 Epoch 54 Loss 0.326 Val Acc 53.460\n",
      "L 10 Epoch 55 Loss 0.349 Val Acc 51.760\n",
      "L 10 Epoch 56 Loss 0.313 Val Acc 50.660\n",
      "L 10 Epoch 57 Loss 0.270 Val Acc 50.500\n",
      "L 10 Epoch 58 Loss 0.277 Val Acc 52.840\n",
      "L 10 Epoch 59 Loss 0.191 Val Acc 52.840\n",
      "L 10 Epoch 60 Loss 0.230 Val Acc 51.440\n",
      "L 10 Epoch 61 Loss 0.276 Val Acc 52.820\n",
      "L 10 Epoch 62 Loss 0.243 Val Acc 49.480\n",
      "L 10 Epoch 63 Loss 0.266 Val Acc 52.360\n",
      "==> Saving model ...\n",
      "L 10 Epoch 64 Loss 0.182 Val Acc 53.940\n",
      "==> Saving model ...\n",
      "L 10 Epoch 65 Loss 0.144 Val Acc 54.720\n",
      "L 10 Epoch 66 Loss 0.237 Val Acc 54.080\n",
      "L 10 Epoch 67 Loss 0.137 Val Acc 53.260\n",
      "L 10 Epoch 68 Loss 0.142 Val Acc 53.820\n",
      "L 10 Epoch 69 Loss 0.132 Val Acc 53.600\n",
      "==> Saving model ...\n",
      "L 10 Epoch 70 Loss 0.126 Val Acc 55.800\n",
      "L 10 Epoch 71 Loss 0.112 Val Acc 53.480\n",
      "L 10 Epoch 72 Loss 0.134 Val Acc 54.480\n",
      "L 10 Epoch 73 Loss 0.108 Val Acc 51.720\n",
      "L 10 Epoch 74 Loss 0.160 Val Acc 53.380\n",
      "L 10 Epoch 75 Loss 0.136 Val Acc 53.660\n",
      "L 10 Epoch 76 Loss 0.126 Val Acc 54.100\n",
      "==> Saving model ...\n",
      "L 10 Epoch 77 Loss 0.120 Val Acc 55.960\n",
      "L 10 Epoch 78 Loss 0.113 Val Acc 54.680\n",
      "L 10 Epoch 79 Loss 0.137 Val Acc 53.920\n",
      "L 10 Epoch 80 Loss 0.075 Val Acc 54.720\n",
      "L 10 Epoch 81 Loss 0.107 Val Acc 54.620\n",
      "L 10 Epoch 82 Loss 0.141 Val Acc 54.080\n",
      "L 10 Epoch 83 Loss 0.123 Val Acc 53.260\n",
      "L 10 Epoch 84 Loss 0.078 Val Acc 52.600\n",
      "L 10 Epoch 85 Loss 0.080 Val Acc 54.900\n",
      "L 10 Epoch 86 Loss 0.054 Val Acc 54.180\n",
      "L 10 Epoch 87 Loss 0.114 Val Acc 54.160\n",
      "L 10 Epoch 88 Loss 0.089 Val Acc 54.920\n",
      "L 10 Epoch 89 Loss 0.112 Val Acc 53.920\n",
      "L 10 Epoch 90 Loss 0.067 Val Acc 55.380\n",
      "==> Saving model ...\n",
      "L 10 Epoch 91 Loss 0.076 Val Acc 56.060\n",
      "L 10 Epoch 92 Loss 0.062 Val Acc 55.700\n",
      "==> Saving model ...\n",
      "L 10 Epoch 93 Loss 0.057 Val Acc 56.520\n",
      "L 10 Epoch 94 Loss 0.063 Val Acc 55.820\n",
      "==> Saving model ...\n",
      "L 10 Epoch 95 Loss 0.061 Val Acc 57.060\n",
      "L 10 Epoch 96 Loss 0.070 Val Acc 55.380\n",
      "L 10 Epoch 97 Loss 0.083 Val Acc 55.740\n",
      "L 10 Epoch 98 Loss 0.064 Val Acc 55.080\n",
      "L 10 Epoch 99 Loss 0.064 Val Acc 55.240\n",
      "L 10 Epoch 100 Loss 0.051 Val Acc 54.840\n",
      "L 10 Epoch 101 Loss 0.055 Val Acc 55.820\n",
      "L 10 Epoch 102 Loss 0.051 Val Acc 56.880\n",
      "L 10 Epoch 103 Loss 0.042 Val Acc 57.000\n",
      "L 10 Epoch 104 Loss 0.069 Val Acc 55.000\n",
      "L 10 Epoch 105 Loss 0.066 Val Acc 55.580\n",
      "L 10 Epoch 106 Loss 0.036 Val Acc 55.880\n",
      "L 10 Epoch 107 Loss 0.066 Val Acc 56.180\n",
      "L 10 Epoch 108 Loss 0.032 Val Acc 56.840\n",
      "L 10 Epoch 109 Loss 0.070 Val Acc 57.000\n",
      "L 10 Epoch 110 Loss 0.035 Val Acc 55.800\n",
      "L 10 Epoch 111 Loss 0.070 Val Acc 56.280\n",
      "L 10 Epoch 112 Loss 0.038 Val Acc 54.660\n",
      "L 10 Epoch 113 Loss 0.029 Val Acc 55.580\n",
      "L 10 Epoch 114 Loss 0.050 Val Acc 56.740\n",
      "L 10 Epoch 115 Loss 0.024 Val Acc 56.900\n",
      "==> Saving model ...\n",
      "L 10 Epoch 116 Loss 0.021 Val Acc 57.360\n",
      "L 10 Epoch 117 Loss 0.025 Val Acc 56.940\n",
      "L 10 Epoch 118 Loss 0.022 Val Acc 56.960\n",
      "==> Saving model ...\n",
      "L 10 Epoch 119 Loss 0.024 Val Acc 58.000\n",
      "==> Saving model ...\n",
      "L 10 Epoch 120 Loss 0.021 Val Acc 58.400\n",
      "==> Saving model ...\n",
      "L 10 Epoch 121 Loss 0.009 Val Acc 58.820\n",
      "L 10 Epoch 122 Loss 0.033 Val Acc 58.800\n",
      "L 10 Epoch 123 Loss 0.032 Val Acc 58.220\n",
      "L 10 Epoch 124 Loss 0.017 Val Acc 57.880\n",
      "L 10 Epoch 125 Loss 0.028 Val Acc 58.260\n",
      "L 10 Epoch 126 Loss 0.035 Val Acc 58.120\n",
      "L 10 Epoch 127 Loss 0.021 Val Acc 57.720\n",
      "L 10 Epoch 128 Loss 0.016 Val Acc 58.040\n",
      "L 10 Epoch 129 Loss 0.018 Val Acc 58.420\n",
      "L 10 Epoch 130 Loss 0.008 Val Acc 58.760\n",
      "==> Saving model ...\n",
      "L 10 Epoch 131 Loss 0.011 Val Acc 59.100\n",
      "L 10 Epoch 132 Loss 0.023 Val Acc 58.620\n",
      "L 10 Epoch 133 Loss 0.018 Val Acc 58.360\n",
      "L 10 Epoch 134 Loss 0.016 Val Acc 58.740\n",
      "L 10 Epoch 135 Loss 0.008 Val Acc 58.520\n",
      "L 10 Epoch 136 Loss 0.018 Val Acc 58.400\n",
      "L 10 Epoch 137 Loss 0.015 Val Acc 58.380\n",
      "L 10 Epoch 138 Loss 0.024 Val Acc 57.700\n",
      "L 10 Epoch 139 Loss 0.025 Val Acc 57.200\n",
      "L 10 Epoch 140 Loss 0.018 Val Acc 56.780\n",
      "L 10 Epoch 141 Loss 0.017 Val Acc 57.620\n",
      "L 10 Epoch 142 Loss 0.022 Val Acc 57.820\n",
      "L 10 Epoch 143 Loss 0.023 Val Acc 58.020\n",
      "L 10 Epoch 144 Loss 0.014 Val Acc 58.380\n",
      "L 10 Epoch 145 Loss 0.013 Val Acc 57.940\n",
      "L 10 Epoch 146 Loss 0.022 Val Acc 57.780\n",
      "L 10 Epoch 147 Loss 0.014 Val Acc 57.780\n",
      "L 10 Epoch 148 Loss 0.023 Val Acc 58.360\n",
      "L 10 Epoch 149 Loss 0.006 Val Acc 58.360\n",
      "L 10 Epoch 150 Loss 0.016 Val Acc 58.140\n",
      "L 10 Epoch 151 Loss 0.012 Val Acc 57.740\n",
      "L 10 Epoch 152 Loss 0.003 Val Acc 58.260\n",
      "L 10 Epoch 153 Loss 0.015 Val Acc 58.160\n",
      "L 10 Epoch 154 Loss 0.016 Val Acc 57.960\n",
      "L 10 Epoch 155 Loss 0.006 Val Acc 58.580\n",
      "L 10 Epoch 156 Loss 0.009 Val Acc 58.580\n",
      "L 10 Epoch 157 Loss 0.016 Val Acc 58.380\n",
      "L 10 Epoch 158 Loss 0.016 Val Acc 58.800\n",
      "L 10 Epoch 159 Loss 0.007 Val Acc 58.640\n",
      "L 10 Epoch 160 Loss 0.006 Val Acc 58.920\n",
      "L 10 Epoch 161 Loss 0.008 Val Acc 58.840\n",
      "L 10 Epoch 162 Loss 0.006 Val Acc 58.840\n",
      "L 10 Epoch 163 Loss 0.020 Val Acc 58.600\n",
      "L 10 Epoch 164 Loss 0.017 Val Acc 57.720\n",
      "L 10 Epoch 165 Loss 0.011 Val Acc 57.900\n",
      "L 10 Epoch 166 Loss 0.015 Val Acc 57.880\n",
      "L 10 Epoch 167 Loss 0.006 Val Acc 58.600\n",
      "L 10 Epoch 168 Loss 0.006 Val Acc 58.520\n",
      "L 10 Epoch 169 Loss 0.004 Val Acc 58.960\n",
      "L 10 Epoch 170 Loss 0.010 Val Acc 58.900\n",
      "L 10 Epoch 171 Loss 0.013 Val Acc 58.480\n",
      "L 10 Epoch 172 Loss 0.003 Val Acc 58.920\n",
      "L 10 Epoch 173 Loss 0.023 Val Acc 58.380\n",
      "L 10 Epoch 174 Loss 0.006 Val Acc 58.640\n",
      "L 10 Epoch 175 Loss 0.006 Val Acc 58.480\n",
      "L 10 Epoch 176 Loss 0.006 Val Acc 58.340\n",
      "L 10 Epoch 177 Loss 0.018 Val Acc 58.600\n",
      "L 10 Epoch 178 Loss 0.006 Val Acc 58.500\n",
      "L 10 Epoch 179 Loss 0.008 Val Acc 58.400\n",
      "L 10 Epoch 180 Loss 0.009 Val Acc 58.840\n",
      "L 10 Epoch 181 Loss 0.007 Val Acc 59.000\n",
      "L 10 Epoch 182 Loss 0.009 Val Acc 58.760\n",
      "L 10 Epoch 183 Loss 0.006 Val Acc 58.800\n",
      "L 10 Epoch 184 Loss 0.009 Val Acc 58.640\n",
      "L 10 Epoch 185 Loss 0.012 Val Acc 58.280\n",
      "L 10 Epoch 186 Loss 0.007 Val Acc 58.720\n",
      "L 10 Epoch 187 Loss 0.012 Val Acc 59.000\n",
      "L 10 Epoch 188 Loss 0.014 Val Acc 58.860\n",
      "L 10 Epoch 189 Loss 0.007 Val Acc 58.660\n",
      "L 10 Epoch 190 Loss 0.003 Val Acc 58.620\n",
      "L 10 Epoch 191 Loss 0.012 Val Acc 58.640\n",
      "L 10 Epoch 192 Loss 0.008 Val Acc 58.060\n",
      "L 10 Epoch 193 Loss 0.007 Val Acc 58.720\n",
      "L 10 Epoch 194 Loss 0.014 Val Acc 58.380\n",
      "L 10 Epoch 195 Loss 0.017 Val Acc 58.240\n",
      "L 10 Epoch 196 Loss 0.010 Val Acc 58.080\n",
      "L 10 Epoch 197 Loss 0.005 Val Acc 58.960\n",
      "==> Saving model ...\n",
      "L 10 Epoch 198 Loss 0.003 Val Acc 59.300\n",
      "L 10 Epoch 199 Loss 0.014 Val Acc 58.460\n",
      "L 4 Epoch 0 Loss 3.285 Val Acc 10.000\n",
      "L 4 Epoch 1 Loss 3.040 Val Acc 9.980\n",
      "L 4 Epoch 2 Loss 2.307 Val Acc 11.220\n",
      "L 4 Epoch 3 Loss 1.953 Val Acc 21.600\n",
      "L 4 Epoch 4 Loss 1.908 Val Acc 27.920\n",
      "L 4 Epoch 5 Loss 1.757 Val Acc 28.040\n",
      "L 4 Epoch 6 Loss 1.730 Val Acc 25.320\n",
      "L 4 Epoch 7 Loss 1.653 Val Acc 35.780\n",
      "L 4 Epoch 8 Loss 1.627 Val Acc 36.840\n",
      "L 4 Epoch 9 Loss 1.528 Val Acc 38.160\n",
      "==> Saving model ...\n",
      "L 4 Epoch 10 Loss 1.500 Val Acc 40.740\n",
      "L 4 Epoch 11 Loss 1.457 Val Acc 37.740\n",
      "==> Saving model ...\n",
      "L 4 Epoch 12 Loss 1.450 Val Acc 41.760\n",
      "L 4 Epoch 13 Loss 1.415 Val Acc 40.980\n",
      "L 4 Epoch 14 Loss 1.373 Val Acc 40.640\n",
      "L 4 Epoch 15 Loss 1.386 Val Acc 41.360\n",
      "==> Saving model ...\n",
      "L 4 Epoch 16 Loss 1.336 Val Acc 43.980\n",
      "L 4 Epoch 17 Loss 1.334 Val Acc 43.520\n",
      "L 4 Epoch 18 Loss 1.194 Val Acc 43.140\n",
      "L 4 Epoch 19 Loss 1.207 Val Acc 40.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 20 Loss 1.197 Val Acc 45.960\n",
      "L 4 Epoch 21 Loss 1.172 Val Acc 44.800\n",
      "L 4 Epoch 22 Loss 1.042 Val Acc 45.320\n",
      "L 4 Epoch 23 Loss 1.163 Val Acc 44.920\n",
      "L 4 Epoch 24 Loss 1.141 Val Acc 36.220\n",
      "L 4 Epoch 25 Loss 1.045 Val Acc 45.140\n",
      "L 4 Epoch 26 Loss 0.970 Val Acc 44.120\n",
      "L 4 Epoch 27 Loss 0.998 Val Acc 44.140\n",
      "L 4 Epoch 28 Loss 1.031 Val Acc 43.080\n",
      "==> Saving model ...\n",
      "L 4 Epoch 29 Loss 0.944 Val Acc 46.960\n",
      "L 4 Epoch 30 Loss 0.794 Val Acc 45.920\n",
      "==> Saving model ...\n",
      "L 4 Epoch 31 Loss 0.891 Val Acc 47.780\n",
      "L 4 Epoch 32 Loss 0.863 Val Acc 45.440\n",
      "L 4 Epoch 33 Loss 0.867 Val Acc 43.220\n",
      "L 4 Epoch 34 Loss 0.749 Val Acc 47.320\n",
      "L 4 Epoch 35 Loss 0.646 Val Acc 47.360\n",
      "==> Saving model ...\n",
      "L 4 Epoch 36 Loss 0.724 Val Acc 48.960\n",
      "L 4 Epoch 37 Loss 0.622 Val Acc 46.560\n",
      "==> Saving model ...\n",
      "L 4 Epoch 38 Loss 0.429 Val Acc 50.300\n",
      "L 4 Epoch 39 Loss 0.598 Val Acc 47.940\n",
      "L 4 Epoch 40 Loss 0.555 Val Acc 49.020\n",
      "==> Saving model ...\n",
      "L 4 Epoch 41 Loss 0.464 Val Acc 51.100\n",
      "L 4 Epoch 42 Loss 0.395 Val Acc 47.940\n",
      "L 4 Epoch 43 Loss 0.288 Val Acc 50.580\n",
      "L 4 Epoch 44 Loss 0.410 Val Acc 47.280\n",
      "L 4 Epoch 45 Loss 0.482 Val Acc 45.260\n",
      "L 4 Epoch 46 Loss 0.443 Val Acc 44.860\n",
      "L 4 Epoch 47 Loss 0.470 Val Acc 47.560\n",
      "L 4 Epoch 48 Loss 0.430 Val Acc 50.660\n",
      "L 4 Epoch 49 Loss 0.293 Val Acc 49.780\n",
      "L 4 Epoch 50 Loss 0.348 Val Acc 50.100\n",
      "==> Saving model ...\n",
      "L 4 Epoch 51 Loss 0.211 Val Acc 52.360\n",
      "L 4 Epoch 52 Loss 0.299 Val Acc 49.720\n",
      "L 4 Epoch 53 Loss 0.397 Val Acc 49.160\n",
      "L 4 Epoch 54 Loss 0.279 Val Acc 49.500\n",
      "L 4 Epoch 55 Loss 0.309 Val Acc 49.700\n",
      "L 4 Epoch 56 Loss 0.255 Val Acc 51.080\n",
      "L 4 Epoch 57 Loss 0.281 Val Acc 48.940\n",
      "L 4 Epoch 58 Loss 0.337 Val Acc 50.940\n",
      "L 4 Epoch 59 Loss 0.227 Val Acc 49.560\n",
      "L 4 Epoch 60 Loss 0.204 Val Acc 51.840\n",
      "L 4 Epoch 61 Loss 0.153 Val Acc 51.120\n",
      "L 4 Epoch 62 Loss 0.216 Val Acc 51.420\n",
      "L 4 Epoch 63 Loss 0.184 Val Acc 50.260\n",
      "L 4 Epoch 64 Loss 0.234 Val Acc 51.620\n",
      "L 4 Epoch 65 Loss 0.300 Val Acc 49.600\n",
      "L 4 Epoch 66 Loss 0.280 Val Acc 48.320\n",
      "L 4 Epoch 67 Loss 0.256 Val Acc 49.180\n",
      "L 4 Epoch 68 Loss 0.261 Val Acc 48.400\n",
      "L 4 Epoch 69 Loss 0.167 Val Acc 49.120\n",
      "==> Saving model ...\n",
      "L 4 Epoch 70 Loss 0.126 Val Acc 52.820\n",
      "==> Saving model ...\n",
      "L 4 Epoch 71 Loss 0.108 Val Acc 53.220\n",
      "L 4 Epoch 72 Loss 0.151 Val Acc 52.260\n",
      "L 4 Epoch 73 Loss 0.076 Val Acc 52.500\n",
      "L 4 Epoch 74 Loss 0.163 Val Acc 51.200\n",
      "L 4 Epoch 75 Loss 0.119 Val Acc 49.860\n",
      "L 4 Epoch 76 Loss 0.102 Val Acc 51.700\n",
      "L 4 Epoch 77 Loss 0.132 Val Acc 52.280\n",
      "L 4 Epoch 78 Loss 0.096 Val Acc 52.160\n",
      "L 4 Epoch 79 Loss 0.138 Val Acc 51.300\n",
      "L 4 Epoch 80 Loss 0.108 Val Acc 51.780\n",
      "L 4 Epoch 81 Loss 0.069 Val Acc 52.100\n",
      "L 4 Epoch 82 Loss 0.085 Val Acc 53.020\n",
      "L 4 Epoch 83 Loss 0.080 Val Acc 51.360\n",
      "L 4 Epoch 84 Loss 0.075 Val Acc 50.280\n",
      "L 4 Epoch 85 Loss 0.095 Val Acc 52.200\n",
      "L 4 Epoch 86 Loss 0.109 Val Acc 52.000\n",
      "L 4 Epoch 87 Loss 0.135 Val Acc 52.080\n",
      "L 4 Epoch 88 Loss 0.086 Val Acc 52.880\n",
      "L 4 Epoch 89 Loss 0.092 Val Acc 52.260\n",
      "L 4 Epoch 90 Loss 0.101 Val Acc 51.880\n",
      "L 4 Epoch 91 Loss 0.095 Val Acc 52.400\n",
      "==> Saving model ...\n",
      "L 4 Epoch 92 Loss 0.049 Val Acc 53.740\n",
      "L 4 Epoch 93 Loss 0.057 Val Acc 52.920\n",
      "==> Saving model ...\n",
      "L 4 Epoch 94 Loss 0.073 Val Acc 54.200\n",
      "==> Saving model ...\n",
      "L 4 Epoch 95 Loss 0.070 Val Acc 55.080\n",
      "L 4 Epoch 96 Loss 0.065 Val Acc 54.520\n",
      "L 4 Epoch 97 Loss 0.027 Val Acc 53.100\n",
      "L 4 Epoch 98 Loss 0.092 Val Acc 53.460\n",
      "L 4 Epoch 99 Loss 0.065 Val Acc 52.260\n",
      "L 4 Epoch 100 Loss 0.069 Val Acc 53.340\n",
      "L 4 Epoch 101 Loss 0.045 Val Acc 54.120\n",
      "L 4 Epoch 102 Loss 0.044 Val Acc 53.840\n",
      "L 4 Epoch 103 Loss 0.064 Val Acc 53.120\n",
      "L 4 Epoch 104 Loss 0.058 Val Acc 52.600\n",
      "L 4 Epoch 105 Loss 0.042 Val Acc 53.520\n",
      "L 4 Epoch 106 Loss 0.082 Val Acc 54.240\n",
      "L 4 Epoch 107 Loss 0.025 Val Acc 53.980\n",
      "L 4 Epoch 108 Loss 0.027 Val Acc 54.080\n",
      "L 4 Epoch 109 Loss 0.039 Val Acc 53.800\n",
      "L 4 Epoch 110 Loss 0.047 Val Acc 53.920\n",
      "L 4 Epoch 111 Loss 0.026 Val Acc 53.160\n",
      "L 4 Epoch 112 Loss 0.042 Val Acc 54.180\n",
      "L 4 Epoch 113 Loss 0.065 Val Acc 54.280\n",
      "L 4 Epoch 114 Loss 0.075 Val Acc 53.460\n",
      "L 4 Epoch 115 Loss 0.045 Val Acc 54.200\n",
      "L 4 Epoch 116 Loss 0.050 Val Acc 54.960\n",
      "L 4 Epoch 117 Loss 0.039 Val Acc 54.920\n",
      "L 4 Epoch 118 Loss 0.026 Val Acc 54.220\n",
      "L 4 Epoch 119 Loss 0.012 Val Acc 54.660\n",
      "==> Saving model ...\n",
      "L 4 Epoch 120 Loss 0.034 Val Acc 55.220\n",
      "==> Saving model ...\n",
      "L 4 Epoch 121 Loss 0.018 Val Acc 55.540\n",
      "L 4 Epoch 122 Loss 0.027 Val Acc 55.340\n",
      "==> Saving model ...\n",
      "L 4 Epoch 123 Loss 0.023 Val Acc 55.820\n",
      "L 4 Epoch 124 Loss 0.039 Val Acc 54.840\n",
      "L 4 Epoch 125 Loss 0.043 Val Acc 54.640\n",
      "L 4 Epoch 126 Loss 0.040 Val Acc 54.980\n",
      "L 4 Epoch 127 Loss 0.042 Val Acc 54.860\n",
      "L 4 Epoch 128 Loss 0.017 Val Acc 55.160\n",
      "L 4 Epoch 129 Loss 0.007 Val Acc 55.120\n",
      "L 4 Epoch 130 Loss 0.012 Val Acc 54.720\n",
      "L 4 Epoch 131 Loss 0.020 Val Acc 55.720\n",
      "L 4 Epoch 132 Loss 0.031 Val Acc 55.700\n",
      "L 4 Epoch 133 Loss 0.022 Val Acc 55.320\n",
      "L 4 Epoch 134 Loss 0.034 Val Acc 54.980\n",
      "L 4 Epoch 135 Loss 0.012 Val Acc 55.320\n",
      "L 4 Epoch 136 Loss 0.014 Val Acc 55.620\n",
      "L 4 Epoch 137 Loss 0.019 Val Acc 55.440\n",
      "L 4 Epoch 138 Loss 0.031 Val Acc 55.140\n",
      "L 4 Epoch 139 Loss 0.014 Val Acc 55.740\n",
      "L 4 Epoch 140 Loss 0.034 Val Acc 55.200\n",
      "L 4 Epoch 141 Loss 0.016 Val Acc 54.800\n",
      "L 4 Epoch 142 Loss 0.018 Val Acc 55.740\n",
      "==> Saving model ...\n",
      "L 4 Epoch 143 Loss 0.012 Val Acc 56.140\n",
      "==> Saving model ...\n",
      "L 4 Epoch 144 Loss 0.006 Val Acc 56.640\n",
      "L 4 Epoch 145 Loss 0.015 Val Acc 56.020\n",
      "L 4 Epoch 146 Loss 0.020 Val Acc 55.020\n",
      "L 4 Epoch 147 Loss 0.026 Val Acc 54.940\n",
      "L 4 Epoch 148 Loss 0.018 Val Acc 55.640\n",
      "L 4 Epoch 149 Loss 0.022 Val Acc 55.000\n",
      "L 4 Epoch 150 Loss 0.015 Val Acc 55.160\n",
      "L 4 Epoch 151 Loss 0.007 Val Acc 55.300\n",
      "L 4 Epoch 152 Loss 0.003 Val Acc 55.680\n",
      "L 4 Epoch 153 Loss 0.017 Val Acc 55.780\n",
      "L 4 Epoch 154 Loss 0.010 Val Acc 55.860\n",
      "L 4 Epoch 155 Loss 0.013 Val Acc 55.980\n",
      "L 4 Epoch 156 Loss 0.008 Val Acc 56.200\n",
      "L 4 Epoch 157 Loss 0.008 Val Acc 56.160\n",
      "L 4 Epoch 158 Loss 0.022 Val Acc 56.340\n",
      "L 4 Epoch 159 Loss 0.010 Val Acc 56.260\n",
      "L 4 Epoch 160 Loss 0.012 Val Acc 56.020\n",
      "L 4 Epoch 161 Loss 0.024 Val Acc 55.880\n",
      "L 4 Epoch 162 Loss 0.016 Val Acc 55.440\n",
      "L 4 Epoch 163 Loss 0.014 Val Acc 55.800\n",
      "L 4 Epoch 164 Loss 0.009 Val Acc 55.880\n",
      "L 4 Epoch 165 Loss 0.011 Val Acc 56.180\n",
      "L 4 Epoch 166 Loss 0.007 Val Acc 56.540\n",
      "L 4 Epoch 167 Loss 0.012 Val Acc 56.260\n",
      "L 4 Epoch 168 Loss 0.013 Val Acc 56.580\n",
      "L 4 Epoch 169 Loss 0.013 Val Acc 55.860\n",
      "==> Saving model ...\n",
      "L 4 Epoch 170 Loss 0.003 Val Acc 56.960\n",
      "L 4 Epoch 171 Loss 0.008 Val Acc 56.860\n",
      "L 4 Epoch 172 Loss 0.011 Val Acc 56.680\n",
      "==> Saving model ...\n",
      "L 4 Epoch 173 Loss 0.007 Val Acc 56.980\n",
      "L 4 Epoch 174 Loss 0.010 Val Acc 56.780\n",
      "L 4 Epoch 175 Loss 0.007 Val Acc 56.980\n",
      "==> Saving model ...\n",
      "L 4 Epoch 176 Loss 0.007 Val Acc 57.320\n",
      "L 4 Epoch 177 Loss 0.012 Val Acc 56.980\n",
      "L 4 Epoch 178 Loss 0.014 Val Acc 56.380\n",
      "L 4 Epoch 179 Loss 0.006 Val Acc 56.780\n",
      "L 4 Epoch 180 Loss 0.006 Val Acc 57.080\n",
      "L 4 Epoch 181 Loss 0.009 Val Acc 57.080\n",
      "L 4 Epoch 182 Loss 0.008 Val Acc 56.920\n",
      "L 4 Epoch 183 Loss 0.008 Val Acc 56.700\n",
      "L 4 Epoch 184 Loss 0.011 Val Acc 56.780\n",
      "L 4 Epoch 185 Loss 0.003 Val Acc 57.000\n",
      "L 4 Epoch 186 Loss 0.017 Val Acc 56.820\n",
      "L 4 Epoch 187 Loss 0.012 Val Acc 56.400\n",
      "L 4 Epoch 188 Loss 0.017 Val Acc 56.080\n",
      "L 4 Epoch 189 Loss 0.013 Val Acc 56.080\n",
      "L 4 Epoch 190 Loss 0.003 Val Acc 57.020\n",
      "L 4 Epoch 191 Loss 0.011 Val Acc 56.500\n",
      "L 4 Epoch 192 Loss 0.010 Val Acc 56.660\n",
      "L 4 Epoch 193 Loss 0.008 Val Acc 56.840\n",
      "L 4 Epoch 194 Loss 0.018 Val Acc 56.520\n",
      "L 4 Epoch 195 Loss 0.006 Val Acc 56.880\n",
      "L 4 Epoch 196 Loss 0.006 Val Acc 56.900\n",
      "L 4 Epoch 197 Loss 0.008 Val Acc 56.980\n",
      "L 4 Epoch 198 Loss 0.008 Val Acc 56.840\n",
      "L 4 Epoch 199 Loss 0.006 Val Acc 56.980\n",
      "L 6 Epoch 0 Loss 5.104 Val Acc 10.000\n",
      "L 6 Epoch 1 Loss 4.458 Val Acc 10.000\n",
      "L 6 Epoch 2 Loss 3.546 Val Acc 9.940\n",
      "L 6 Epoch 3 Loss 2.880 Val Acc 7.700\n",
      "L 6 Epoch 4 Loss 2.639 Val Acc 13.900\n",
      "L 6 Epoch 5 Loss 2.504 Val Acc 13.040\n",
      "L 6 Epoch 6 Loss 2.401 Val Acc 15.580\n",
      "L 6 Epoch 7 Loss 2.442 Val Acc 19.240\n",
      "L 6 Epoch 8 Loss 2.220 Val Acc 19.620\n",
      "L 6 Epoch 9 Loss 2.103 Val Acc 24.220\n",
      "L 6 Epoch 10 Loss 2.052 Val Acc 26.380\n",
      "L 6 Epoch 11 Loss 2.053 Val Acc 27.320\n",
      "L 6 Epoch 12 Loss 2.077 Val Acc 25.160\n",
      "L 6 Epoch 13 Loss 1.963 Val Acc 25.240\n",
      "L 6 Epoch 14 Loss 1.895 Val Acc 27.860\n",
      "L 6 Epoch 15 Loss 1.838 Val Acc 26.560\n",
      "L 6 Epoch 16 Loss 1.766 Val Acc 33.480\n",
      "L 6 Epoch 17 Loss 1.731 Val Acc 30.640\n",
      "L 6 Epoch 18 Loss 1.654 Val Acc 35.000\n",
      "L 6 Epoch 19 Loss 1.666 Val Acc 33.220\n",
      "L 6 Epoch 20 Loss 1.625 Val Acc 33.420\n",
      "L 6 Epoch 21 Loss 1.592 Val Acc 35.160\n",
      "L 6 Epoch 22 Loss 1.547 Val Acc 36.600\n",
      "L 6 Epoch 23 Loss 1.491 Val Acc 37.880\n",
      "L 6 Epoch 24 Loss 1.489 Val Acc 36.420\n",
      "L 6 Epoch 25 Loss 1.426 Val Acc 35.840\n",
      "L 6 Epoch 26 Loss 1.454 Val Acc 36.060\n",
      "L 6 Epoch 27 Loss 1.485 Val Acc 38.780\n",
      "L 6 Epoch 28 Loss 1.438 Val Acc 36.980\n",
      "L 6 Epoch 29 Loss 1.421 Val Acc 38.880\n",
      "==> Saving model ...\n",
      "L 6 Epoch 30 Loss 1.374 Val Acc 40.160\n",
      "L 6 Epoch 31 Loss 1.348 Val Acc 39.920\n",
      "L 6 Epoch 32 Loss 1.361 Val Acc 38.280\n",
      "==> Saving model ...\n",
      "L 6 Epoch 33 Loss 1.292 Val Acc 41.020\n",
      "==> Saving model ...\n",
      "L 6 Epoch 34 Loss 1.285 Val Acc 42.020\n",
      "L 6 Epoch 35 Loss 1.187 Val Acc 40.520\n",
      "L 6 Epoch 36 Loss 1.281 Val Acc 41.600\n",
      "L 6 Epoch 37 Loss 1.146 Val Acc 38.880\n",
      "==> Saving model ...\n",
      "L 6 Epoch 38 Loss 1.060 Val Acc 45.420\n",
      "L 6 Epoch 39 Loss 1.097 Val Acc 40.400\n",
      "L 6 Epoch 40 Loss 1.180 Val Acc 40.540\n",
      "L 6 Epoch 41 Loss 1.074 Val Acc 42.320\n",
      "L 6 Epoch 42 Loss 1.004 Val Acc 43.420\n",
      "L 6 Epoch 43 Loss 1.078 Val Acc 37.320\n",
      "L 6 Epoch 44 Loss 1.104 Val Acc 43.040\n",
      "==> Saving model ...\n",
      "L 6 Epoch 45 Loss 1.003 Val Acc 46.760\n",
      "L 6 Epoch 46 Loss 1.000 Val Acc 45.680\n",
      "L 6 Epoch 47 Loss 0.813 Val Acc 44.120\n",
      "L 6 Epoch 48 Loss 0.847 Val Acc 45.240\n",
      "L 6 Epoch 49 Loss 0.818 Val Acc 44.740\n",
      "L 6 Epoch 50 Loss 0.825 Val Acc 44.040\n",
      "L 6 Epoch 51 Loss 0.865 Val Acc 46.640\n",
      "L 6 Epoch 52 Loss 0.790 Val Acc 43.620\n",
      "L 6 Epoch 53 Loss 0.677 Val Acc 45.720\n",
      "L 6 Epoch 54 Loss 0.608 Val Acc 46.580\n",
      "L 6 Epoch 55 Loss 0.614 Val Acc 46.360\n",
      "==> Saving model ...\n",
      "L 6 Epoch 56 Loss 0.671 Val Acc 47.040\n",
      "L 6 Epoch 57 Loss 0.648 Val Acc 42.520\n",
      "L 6 Epoch 58 Loss 0.664 Val Acc 46.500\n",
      "==> Saving model ...\n",
      "L 6 Epoch 59 Loss 0.386 Val Acc 49.520\n",
      "L 6 Epoch 60 Loss 0.303 Val Acc 48.780\n",
      "L 6 Epoch 61 Loss 0.483 Val Acc 46.400\n",
      "==> Saving model ...\n",
      "L 6 Epoch 62 Loss 0.425 Val Acc 49.620\n",
      "L 6 Epoch 63 Loss 0.374 Val Acc 48.020\n",
      "L 6 Epoch 64 Loss 0.356 Val Acc 49.400\n",
      "L 6 Epoch 65 Loss 0.350 Val Acc 49.000\n",
      "==> Saving model ...\n",
      "L 6 Epoch 66 Loss 0.304 Val Acc 51.520\n",
      "L 6 Epoch 67 Loss 0.272 Val Acc 49.860\n",
      "L 6 Epoch 68 Loss 0.293 Val Acc 49.900\n",
      "==> Saving model ...\n",
      "L 6 Epoch 69 Loss 0.192 Val Acc 51.760\n",
      "==> Saving model ...\n",
      "L 6 Epoch 70 Loss 0.294 Val Acc 51.980\n",
      "L 6 Epoch 71 Loss 0.183 Val Acc 49.940\n",
      "L 6 Epoch 72 Loss 0.273 Val Acc 50.140\n",
      "L 6 Epoch 73 Loss 0.260 Val Acc 50.160\n",
      "L 6 Epoch 74 Loss 0.237 Val Acc 49.580\n",
      "L 6 Epoch 75 Loss 0.248 Val Acc 48.040\n",
      "L 6 Epoch 76 Loss 0.242 Val Acc 46.440\n",
      "L 6 Epoch 77 Loss 0.290 Val Acc 50.240\n",
      "L 6 Epoch 78 Loss 0.269 Val Acc 51.540\n",
      "L 6 Epoch 79 Loss 0.242 Val Acc 50.820\n",
      "L 6 Epoch 80 Loss 0.227 Val Acc 50.020\n",
      "==> Saving model ...\n",
      "L 6 Epoch 81 Loss 0.133 Val Acc 52.300\n",
      "==> Saving model ...\n",
      "L 6 Epoch 82 Loss 0.155 Val Acc 52.480\n",
      "==> Saving model ...\n",
      "L 6 Epoch 83 Loss 0.141 Val Acc 53.060\n",
      "L 6 Epoch 84 Loss 0.167 Val Acc 51.900\n",
      "L 6 Epoch 85 Loss 0.217 Val Acc 50.320\n",
      "L 6 Epoch 86 Loss 0.185 Val Acc 50.660\n",
      "L 6 Epoch 87 Loss 0.178 Val Acc 51.280\n",
      "L 6 Epoch 88 Loss 0.137 Val Acc 52.380\n",
      "L 6 Epoch 89 Loss 0.177 Val Acc 50.500\n",
      "L 6 Epoch 90 Loss 0.147 Val Acc 52.520\n",
      "L 6 Epoch 91 Loss 0.156 Val Acc 52.620\n",
      "L 6 Epoch 92 Loss 0.157 Val Acc 52.800\n",
      "==> Saving model ...\n",
      "L 6 Epoch 93 Loss 0.162 Val Acc 53.300\n",
      "==> Saving model ...\n",
      "L 6 Epoch 94 Loss 0.070 Val Acc 53.340\n",
      "L 6 Epoch 95 Loss 0.088 Val Acc 53.320\n",
      "==> Saving model ...\n",
      "L 6 Epoch 96 Loss 0.085 Val Acc 53.780\n",
      "L 6 Epoch 97 Loss 0.045 Val Acc 52.260\n",
      "L 6 Epoch 98 Loss 0.118 Val Acc 53.060\n",
      "L 6 Epoch 99 Loss 0.097 Val Acc 52.940\n",
      "L 6 Epoch 100 Loss 0.109 Val Acc 52.940\n",
      "L 6 Epoch 101 Loss 0.104 Val Acc 52.920\n",
      "L 6 Epoch 102 Loss 0.068 Val Acc 52.820\n",
      "L 6 Epoch 103 Loss 0.134 Val Acc 53.320\n",
      "L 6 Epoch 104 Loss 0.139 Val Acc 52.940\n",
      "L 6 Epoch 105 Loss 0.126 Val Acc 51.560\n",
      "==> Saving model ...\n",
      "L 6 Epoch 106 Loss 0.135 Val Acc 54.140\n",
      "L 6 Epoch 107 Loss 0.091 Val Acc 52.360\n",
      "==> Saving model ...\n",
      "L 6 Epoch 108 Loss 0.077 Val Acc 54.220\n",
      "L 6 Epoch 109 Loss 0.042 Val Acc 52.740\n",
      "L 6 Epoch 110 Loss 0.059 Val Acc 52.940\n",
      "L 6 Epoch 111 Loss 0.053 Val Acc 53.180\n",
      "L 6 Epoch 112 Loss 0.068 Val Acc 53.180\n",
      "L 6 Epoch 113 Loss 0.095 Val Acc 53.460\n",
      "L 6 Epoch 114 Loss 0.043 Val Acc 52.800\n",
      "L 6 Epoch 115 Loss 0.047 Val Acc 53.020\n",
      "L 6 Epoch 116 Loss 0.083 Val Acc 53.860\n",
      "==> Saving model ...\n",
      "L 6 Epoch 117 Loss 0.077 Val Acc 54.260\n",
      "L 6 Epoch 118 Loss 0.060 Val Acc 54.060\n",
      "==> Saving model ...\n",
      "L 6 Epoch 119 Loss 0.059 Val Acc 55.000\n",
      "==> Saving model ...\n",
      "L 6 Epoch 120 Loss 0.066 Val Acc 55.100\n",
      "==> Saving model ...\n",
      "L 6 Epoch 121 Loss 0.016 Val Acc 55.180\n",
      "==> Saving model ...\n",
      "L 6 Epoch 122 Loss 0.054 Val Acc 55.460\n",
      "L 6 Epoch 123 Loss 0.045 Val Acc 55.320\n",
      "L 6 Epoch 124 Loss 0.048 Val Acc 54.820\n",
      "L 6 Epoch 125 Loss 0.016 Val Acc 54.740\n",
      "L 6 Epoch 126 Loss 0.055 Val Acc 54.620\n",
      "L 6 Epoch 127 Loss 0.030 Val Acc 54.200\n",
      "L 6 Epoch 128 Loss 0.021 Val Acc 55.080\n",
      "L 6 Epoch 129 Loss 0.054 Val Acc 55.360\n",
      "L 6 Epoch 130 Loss 0.057 Val Acc 55.040\n",
      "L 6 Epoch 131 Loss 0.053 Val Acc 53.700\n",
      "L 6 Epoch 132 Loss 0.036 Val Acc 53.820\n",
      "L 6 Epoch 133 Loss 0.035 Val Acc 54.060\n",
      "L 6 Epoch 134 Loss 0.047 Val Acc 54.780\n",
      "==> Saving model ...\n",
      "L 6 Epoch 135 Loss 0.034 Val Acc 56.000\n",
      "L 6 Epoch 136 Loss 0.048 Val Acc 55.620\n",
      "L 6 Epoch 137 Loss 0.051 Val Acc 55.280\n",
      "L 6 Epoch 138 Loss 0.060 Val Acc 55.600\n",
      "L 6 Epoch 139 Loss 0.045 Val Acc 55.260\n",
      "L 6 Epoch 140 Loss 0.030 Val Acc 55.280\n",
      "L 6 Epoch 141 Loss 0.033 Val Acc 55.120\n",
      "L 6 Epoch 142 Loss 0.018 Val Acc 55.600\n",
      "L 6 Epoch 143 Loss 0.031 Val Acc 55.920\n",
      "==> Saving model ...\n",
      "L 6 Epoch 144 Loss 0.019 Val Acc 56.240\n",
      "==> Saving model ...\n",
      "L 6 Epoch 145 Loss 0.027 Val Acc 56.420\n",
      "L 6 Epoch 146 Loss 0.035 Val Acc 55.620\n",
      "L 6 Epoch 147 Loss 0.007 Val Acc 55.640\n",
      "L 6 Epoch 148 Loss 0.037 Val Acc 55.380\n",
      "L 6 Epoch 149 Loss 0.030 Val Acc 55.300\n",
      "L 6 Epoch 150 Loss 0.013 Val Acc 55.580\n",
      "L 6 Epoch 151 Loss 0.014 Val Acc 56.100\n",
      "L 6 Epoch 152 Loss 0.018 Val Acc 56.180\n",
      "L 6 Epoch 153 Loss 0.014 Val Acc 56.240\n",
      "L 6 Epoch 154 Loss 0.021 Val Acc 56.360\n",
      "L 6 Epoch 155 Loss 0.016 Val Acc 55.880\n",
      "L 6 Epoch 156 Loss 0.010 Val Acc 55.740\n",
      "L 6 Epoch 157 Loss 0.009 Val Acc 55.760\n",
      "L 6 Epoch 158 Loss 0.003 Val Acc 55.920\n",
      "L 6 Epoch 159 Loss 0.036 Val Acc 56.040\n",
      "L 6 Epoch 160 Loss 0.012 Val Acc 55.760\n",
      "L 6 Epoch 161 Loss 0.036 Val Acc 55.340\n",
      "L 6 Epoch 162 Loss 0.024 Val Acc 54.920\n",
      "L 6 Epoch 163 Loss 0.018 Val Acc 55.200\n",
      "L 6 Epoch 164 Loss 0.020 Val Acc 55.120\n",
      "L 6 Epoch 165 Loss 0.017 Val Acc 55.520\n",
      "L 6 Epoch 166 Loss 0.014 Val Acc 55.760\n",
      "L 6 Epoch 167 Loss 0.013 Val Acc 55.560\n",
      "L 6 Epoch 168 Loss 0.034 Val Acc 54.900\n",
      "L 6 Epoch 169 Loss 0.019 Val Acc 55.700\n",
      "L 6 Epoch 170 Loss 0.020 Val Acc 55.660\n",
      "L 6 Epoch 171 Loss 0.002 Val Acc 56.100\n",
      "L 6 Epoch 172 Loss 0.022 Val Acc 56.100\n",
      "L 6 Epoch 173 Loss 0.031 Val Acc 55.660\n",
      "L 6 Epoch 174 Loss 0.024 Val Acc 55.900\n",
      "L 6 Epoch 175 Loss 0.010 Val Acc 56.060\n",
      "L 6 Epoch 176 Loss 0.008 Val Acc 56.280\n",
      "L 6 Epoch 177 Loss 0.019 Val Acc 55.880\n",
      "L 6 Epoch 178 Loss 0.010 Val Acc 55.820\n",
      "L 6 Epoch 179 Loss 0.019 Val Acc 55.720\n",
      "L 6 Epoch 180 Loss 0.035 Val Acc 55.600\n",
      "L 6 Epoch 181 Loss 0.009 Val Acc 55.840\n",
      "L 6 Epoch 182 Loss 0.016 Val Acc 55.860\n",
      "L 6 Epoch 183 Loss 0.011 Val Acc 55.900\n",
      "L 6 Epoch 184 Loss 0.016 Val Acc 55.880\n",
      "L 6 Epoch 185 Loss 0.014 Val Acc 55.680\n",
      "L 6 Epoch 186 Loss 0.013 Val Acc 55.700\n",
      "L 6 Epoch 187 Loss 0.022 Val Acc 55.640\n",
      "L 6 Epoch 188 Loss 0.011 Val Acc 56.060\n",
      "L 6 Epoch 189 Loss 0.016 Val Acc 56.080\n",
      "L 6 Epoch 190 Loss 0.023 Val Acc 55.900\n",
      "L 6 Epoch 191 Loss 0.008 Val Acc 56.120\n",
      "L 6 Epoch 192 Loss 0.015 Val Acc 55.980\n",
      "L 6 Epoch 193 Loss 0.011 Val Acc 55.960\n",
      "L 6 Epoch 194 Loss 0.008 Val Acc 56.400\n",
      "L 6 Epoch 195 Loss 0.022 Val Acc 55.960\n",
      "L 6 Epoch 196 Loss 0.015 Val Acc 56.160\n",
      "L 6 Epoch 197 Loss 0.013 Val Acc 56.060\n",
      "L 6 Epoch 198 Loss 0.017 Val Acc 56.160\n",
      "L 6 Epoch 199 Loss 0.017 Val Acc 55.880\n",
      "L 8 Epoch 0 Loss 5.158 Val Acc 10.000\n",
      "L 8 Epoch 1 Loss 3.596 Val Acc 9.980\n",
      "L 8 Epoch 2 Loss 2.693 Val Acc 8.060\n",
      "L 8 Epoch 3 Loss 2.596 Val Acc 14.220\n",
      "L 8 Epoch 4 Loss 2.504 Val Acc 16.640\n",
      "L 8 Epoch 5 Loss 2.250 Val Acc 16.440\n",
      "L 8 Epoch 6 Loss 2.064 Val Acc 17.480\n",
      "L 8 Epoch 7 Loss 1.898 Val Acc 22.100\n",
      "L 8 Epoch 8 Loss 1.842 Val Acc 27.180\n",
      "L 8 Epoch 9 Loss 1.746 Val Acc 28.080\n",
      "L 8 Epoch 10 Loss 1.770 Val Acc 28.020\n",
      "L 8 Epoch 11 Loss 1.734 Val Acc 32.840\n",
      "L 8 Epoch 12 Loss 1.646 Val Acc 32.860\n",
      "L 8 Epoch 13 Loss 1.686 Val Acc 29.900\n",
      "L 8 Epoch 14 Loss 1.609 Val Acc 35.280\n",
      "L 8 Epoch 15 Loss 1.550 Val Acc 33.680\n",
      "L 8 Epoch 16 Loss 1.507 Val Acc 33.560\n",
      "L 8 Epoch 17 Loss 1.463 Val Acc 38.300\n",
      "L 8 Epoch 18 Loss 1.508 Val Acc 33.280\n",
      "L 8 Epoch 19 Loss 1.488 Val Acc 37.600\n",
      "==> Saving model ...\n",
      "L 8 Epoch 20 Loss 1.426 Val Acc 41.400\n",
      "L 8 Epoch 21 Loss 1.379 Val Acc 38.180\n",
      "L 8 Epoch 22 Loss 1.390 Val Acc 41.120\n",
      "L 8 Epoch 23 Loss 1.349 Val Acc 38.740\n",
      "L 8 Epoch 24 Loss 1.347 Val Acc 37.920\n",
      "L 8 Epoch 25 Loss 1.337 Val Acc 38.480\n",
      "==> Saving model ...\n",
      "L 8 Epoch 26 Loss 1.353 Val Acc 42.120\n",
      "L 8 Epoch 27 Loss 1.271 Val Acc 41.940\n",
      "L 8 Epoch 28 Loss 1.234 Val Acc 36.440\n",
      "==> Saving model ...\n",
      "L 8 Epoch 29 Loss 1.182 Val Acc 43.200\n",
      "L 8 Epoch 30 Loss 1.205 Val Acc 39.540\n",
      "==> Saving model ...\n",
      "L 8 Epoch 31 Loss 1.143 Val Acc 45.220\n",
      "L 8 Epoch 32 Loss 1.101 Val Acc 42.520\n",
      "L 8 Epoch 33 Loss 1.153 Val Acc 45.140\n",
      "==> Saving model ...\n",
      "L 8 Epoch 34 Loss 1.025 Val Acc 47.380\n",
      "L 8 Epoch 35 Loss 1.046 Val Acc 44.460\n",
      "L 8 Epoch 36 Loss 0.961 Val Acc 44.520\n",
      "L 8 Epoch 37 Loss 0.961 Val Acc 45.660\n",
      "L 8 Epoch 38 Loss 0.972 Val Acc 42.000\n",
      "==> Saving model ...\n",
      "L 8 Epoch 39 Loss 0.870 Val Acc 50.600\n",
      "L 8 Epoch 40 Loss 0.830 Val Acc 46.600\n",
      "L 8 Epoch 41 Loss 0.707 Val Acc 50.000\n",
      "L 8 Epoch 42 Loss 0.788 Val Acc 47.220\n",
      "L 8 Epoch 43 Loss 0.745 Val Acc 49.160\n",
      "L 8 Epoch 44 Loss 0.706 Val Acc 48.100\n",
      "L 8 Epoch 45 Loss 0.633 Val Acc 48.380\n",
      "L 8 Epoch 46 Loss 0.663 Val Acc 47.940\n",
      "L 8 Epoch 47 Loss 0.521 Val Acc 47.320\n",
      "L 8 Epoch 48 Loss 0.700 Val Acc 46.320\n",
      "L 8 Epoch 49 Loss 0.496 Val Acc 49.080\n",
      "L 8 Epoch 50 Loss 0.418 Val Acc 50.040\n",
      "==> Saving model ...\n",
      "L 8 Epoch 51 Loss 0.447 Val Acc 50.920\n",
      "==> Saving model ...\n",
      "L 8 Epoch 52 Loss 0.492 Val Acc 52.500\n",
      "L 8 Epoch 53 Loss 0.420 Val Acc 50.720\n",
      "L 8 Epoch 54 Loss 0.446 Val Acc 48.660\n",
      "L 8 Epoch 55 Loss 0.381 Val Acc 49.400\n",
      "L 8 Epoch 56 Loss 0.539 Val Acc 47.020\n",
      "L 8 Epoch 57 Loss 0.421 Val Acc 49.900\n",
      "L 8 Epoch 58 Loss 0.453 Val Acc 51.120\n",
      "L 8 Epoch 59 Loss 0.517 Val Acc 50.060\n",
      "L 8 Epoch 60 Loss 0.373 Val Acc 50.940\n",
      "L 8 Epoch 61 Loss 0.403 Val Acc 51.860\n",
      "==> Saving model ...\n",
      "L 8 Epoch 62 Loss 0.326 Val Acc 52.680\n",
      "L 8 Epoch 63 Loss 0.333 Val Acc 50.960\n",
      "L 8 Epoch 64 Loss 0.182 Val Acc 51.640\n",
      "L 8 Epoch 65 Loss 0.220 Val Acc 52.040\n",
      "==> Saving model ...\n",
      "L 8 Epoch 66 Loss 0.268 Val Acc 52.740\n",
      "==> Saving model ...\n",
      "L 8 Epoch 67 Loss 0.234 Val Acc 53.800\n",
      "L 8 Epoch 68 Loss 0.194 Val Acc 53.460\n",
      "==> Saving model ...\n",
      "L 8 Epoch 69 Loss 0.229 Val Acc 54.100\n",
      "L 8 Epoch 70 Loss 0.284 Val Acc 51.420\n",
      "L 8 Epoch 71 Loss 0.204 Val Acc 52.060\n",
      "L 8 Epoch 72 Loss 0.186 Val Acc 51.360\n",
      "L 8 Epoch 73 Loss 0.159 Val Acc 52.800\n",
      "==> Saving model ...\n",
      "L 8 Epoch 74 Loss 0.160 Val Acc 55.820\n",
      "L 8 Epoch 75 Loss 0.240 Val Acc 54.400\n",
      "L 8 Epoch 76 Loss 0.204 Val Acc 52.340\n",
      "L 8 Epoch 77 Loss 0.251 Val Acc 51.960\n",
      "L 8 Epoch 78 Loss 0.166 Val Acc 53.920\n",
      "L 8 Epoch 79 Loss 0.144 Val Acc 54.380\n",
      "L 8 Epoch 80 Loss 0.149 Val Acc 55.100\n",
      "L 8 Epoch 81 Loss 0.186 Val Acc 54.920\n",
      "L 8 Epoch 82 Loss 0.217 Val Acc 53.460\n",
      "L 8 Epoch 83 Loss 0.115 Val Acc 52.540\n",
      "L 8 Epoch 84 Loss 0.121 Val Acc 53.360\n",
      "L 8 Epoch 85 Loss 0.107 Val Acc 53.960\n",
      "L 8 Epoch 86 Loss 0.098 Val Acc 55.520\n",
      "L 8 Epoch 87 Loss 0.134 Val Acc 54.900\n",
      "L 8 Epoch 88 Loss 0.137 Val Acc 55.380\n",
      "L 8 Epoch 89 Loss 0.143 Val Acc 54.060\n",
      "L 8 Epoch 90 Loss 0.101 Val Acc 54.600\n",
      "L 8 Epoch 91 Loss 0.110 Val Acc 54.780\n",
      "L 8 Epoch 92 Loss 0.139 Val Acc 55.420\n",
      "==> Saving model ...\n",
      "L 8 Epoch 93 Loss 0.127 Val Acc 56.300\n",
      "L 8 Epoch 94 Loss 0.110 Val Acc 54.380\n",
      "L 8 Epoch 95 Loss 0.060 Val Acc 54.800\n",
      "==> Saving model ...\n",
      "L 8 Epoch 96 Loss 0.116 Val Acc 56.640\n",
      "L 8 Epoch 97 Loss 0.116 Val Acc 55.240\n",
      "L 8 Epoch 98 Loss 0.098 Val Acc 54.840\n",
      "L 8 Epoch 99 Loss 0.110 Val Acc 55.100\n",
      "L 8 Epoch 100 Loss 0.063 Val Acc 55.420\n",
      "L 8 Epoch 101 Loss 0.108 Val Acc 55.680\n",
      "L 8 Epoch 102 Loss 0.081 Val Acc 55.700\n",
      "L 8 Epoch 103 Loss 0.068 Val Acc 55.360\n",
      "L 8 Epoch 104 Loss 0.088 Val Acc 56.220\n",
      "L 8 Epoch 105 Loss 0.081 Val Acc 56.480\n",
      "L 8 Epoch 106 Loss 0.058 Val Acc 56.600\n",
      "L 8 Epoch 107 Loss 0.053 Val Acc 55.220\n",
      "L 8 Epoch 108 Loss 0.065 Val Acc 56.180\n",
      "==> Saving model ...\n",
      "L 8 Epoch 109 Loss 0.048 Val Acc 56.780\n",
      "L 8 Epoch 110 Loss 0.072 Val Acc 56.440\n",
      "L 8 Epoch 111 Loss 0.026 Val Acc 56.200\n",
      "==> Saving model ...\n",
      "L 8 Epoch 112 Loss 0.029 Val Acc 56.820\n",
      "==> Saving model ...\n",
      "L 8 Epoch 113 Loss 0.044 Val Acc 57.120\n",
      "==> Saving model ...\n",
      "L 8 Epoch 114 Loss 0.047 Val Acc 57.160\n",
      "L 8 Epoch 115 Loss 0.058 Val Acc 56.460\n",
      "L 8 Epoch 116 Loss 0.050 Val Acc 56.580\n",
      "L 8 Epoch 117 Loss 0.031 Val Acc 56.460\n",
      "==> Saving model ...\n",
      "L 8 Epoch 118 Loss 0.042 Val Acc 57.920\n",
      "L 8 Epoch 119 Loss 0.052 Val Acc 57.240\n",
      "L 8 Epoch 120 Loss 0.065 Val Acc 56.880\n",
      "L 8 Epoch 121 Loss 0.028 Val Acc 56.740\n",
      "L 8 Epoch 122 Loss 0.059 Val Acc 56.040\n",
      "L 8 Epoch 123 Loss 0.063 Val Acc 56.580\n",
      "L 8 Epoch 124 Loss 0.047 Val Acc 55.820\n",
      "L 8 Epoch 125 Loss 0.065 Val Acc 55.980\n",
      "L 8 Epoch 126 Loss 0.042 Val Acc 56.260\n",
      "L 8 Epoch 127 Loss 0.031 Val Acc 57.660\n",
      "L 8 Epoch 128 Loss 0.044 Val Acc 57.560\n",
      "L 8 Epoch 129 Loss 0.016 Val Acc 57.240\n",
      "L 8 Epoch 130 Loss 0.033 Val Acc 56.960\n",
      "L 8 Epoch 131 Loss 0.030 Val Acc 56.660\n",
      "L 8 Epoch 132 Loss 0.036 Val Acc 57.600\n",
      "L 8 Epoch 133 Loss 0.038 Val Acc 57.700\n",
      "L 8 Epoch 134 Loss 0.031 Val Acc 57.600\n",
      "L 8 Epoch 135 Loss 0.042 Val Acc 57.160\n",
      "L 8 Epoch 136 Loss 0.022 Val Acc 56.540\n",
      "L 8 Epoch 137 Loss 0.029 Val Acc 56.340\n",
      "L 8 Epoch 138 Loss 0.045 Val Acc 56.600\n",
      "L 8 Epoch 139 Loss 0.021 Val Acc 57.000\n",
      "==> Saving model ...\n",
      "L 8 Epoch 140 Loss 0.030 Val Acc 58.000\n",
      "L 8 Epoch 141 Loss 0.037 Val Acc 57.260\n",
      "L 8 Epoch 142 Loss 0.026 Val Acc 57.560\n",
      "==> Saving model ...\n",
      "L 8 Epoch 143 Loss 0.028 Val Acc 58.400\n",
      "L 8 Epoch 144 Loss 0.040 Val Acc 58.260\n",
      "L 8 Epoch 145 Loss 0.041 Val Acc 57.640\n",
      "L 8 Epoch 146 Loss 0.034 Val Acc 57.640\n",
      "L 8 Epoch 147 Loss 0.024 Val Acc 58.120\n",
      "L 8 Epoch 148 Loss 0.038 Val Acc 58.000\n",
      "L 8 Epoch 149 Loss 0.030 Val Acc 57.200\n",
      "L 8 Epoch 150 Loss 0.029 Val Acc 57.240\n",
      "L 8 Epoch 151 Loss 0.031 Val Acc 57.020\n",
      "L 8 Epoch 152 Loss 0.026 Val Acc 56.920\n",
      "L 8 Epoch 153 Loss 0.025 Val Acc 57.240\n",
      "L 8 Epoch 154 Loss 0.012 Val Acc 58.180\n",
      "==> Saving model ...\n",
      "L 8 Epoch 155 Loss 0.016 Val Acc 58.560\n",
      "==> Saving model ...\n",
      "L 8 Epoch 156 Loss 0.014 Val Acc 58.700\n",
      "==> Saving model ...\n",
      "L 8 Epoch 157 Loss 0.019 Val Acc 58.800\n",
      "L 8 Epoch 158 Loss 0.004 Val Acc 58.720\n",
      "L 8 Epoch 159 Loss 0.030 Val Acc 58.620\n",
      "==> Saving model ...\n",
      "L 8 Epoch 160 Loss 0.017 Val Acc 58.820\n",
      "==> Saving model ...\n",
      "L 8 Epoch 161 Loss 0.010 Val Acc 59.140\n",
      "L 8 Epoch 162 Loss 0.010 Val Acc 58.960\n",
      "L 8 Epoch 163 Loss 0.018 Val Acc 58.600\n",
      "L 8 Epoch 164 Loss 0.007 Val Acc 59.120\n",
      "L 8 Epoch 165 Loss 0.019 Val Acc 59.000\n",
      "L 8 Epoch 166 Loss 0.009 Val Acc 59.080\n",
      "==> Saving model ...\n",
      "L 8 Epoch 167 Loss 0.008 Val Acc 59.260\n",
      "==> Saving model ...\n",
      "L 8 Epoch 168 Loss 0.016 Val Acc 59.520\n",
      "L 8 Epoch 169 Loss 0.013 Val Acc 59.040\n",
      "==> Saving model ...\n",
      "L 8 Epoch 170 Loss 0.010 Val Acc 59.540\n",
      "L 8 Epoch 171 Loss 0.007 Val Acc 59.480\n",
      "L 8 Epoch 172 Loss 0.011 Val Acc 58.980\n",
      "L 8 Epoch 173 Loss 0.009 Val Acc 59.400\n",
      "L 8 Epoch 174 Loss 0.024 Val Acc 59.080\n",
      "L 8 Epoch 175 Loss 0.022 Val Acc 59.040\n",
      "L 8 Epoch 176 Loss 0.009 Val Acc 58.760\n",
      "L 8 Epoch 177 Loss 0.022 Val Acc 58.840\n",
      "L 8 Epoch 178 Loss 0.012 Val Acc 58.480\n",
      "L 8 Epoch 179 Loss 0.014 Val Acc 58.960\n",
      "L 8 Epoch 180 Loss 0.020 Val Acc 58.800\n",
      "L 8 Epoch 181 Loss 0.012 Val Acc 59.080\n",
      "L 8 Epoch 182 Loss 0.012 Val Acc 59.100\n",
      "L 8 Epoch 183 Loss 0.011 Val Acc 58.900\n",
      "L 8 Epoch 184 Loss 0.007 Val Acc 58.940\n",
      "L 8 Epoch 185 Loss 0.020 Val Acc 58.900\n",
      "L 8 Epoch 186 Loss 0.013 Val Acc 58.940\n",
      "L 8 Epoch 187 Loss 0.022 Val Acc 58.800\n",
      "L 8 Epoch 188 Loss 0.010 Val Acc 58.940\n",
      "L 8 Epoch 189 Loss 0.018 Val Acc 58.820\n",
      "L 8 Epoch 190 Loss 0.011 Val Acc 58.880\n",
      "L 8 Epoch 191 Loss 0.015 Val Acc 58.920\n",
      "L 8 Epoch 192 Loss 0.016 Val Acc 59.200\n",
      "L 8 Epoch 193 Loss 0.016 Val Acc 58.960\n",
      "L 8 Epoch 194 Loss 0.014 Val Acc 58.700\n",
      "L 8 Epoch 195 Loss 0.019 Val Acc 58.660\n",
      "L 8 Epoch 196 Loss 0.008 Val Acc 58.960\n",
      "L 8 Epoch 197 Loss 0.010 Val Acc 58.620\n",
      "L 8 Epoch 198 Loss 0.028 Val Acc 58.000\n",
      "L 8 Epoch 199 Loss 0.006 Val Acc 58.840\n",
      "L 10 Epoch 0 Loss 4.118 Val Acc 10.000\n",
      "L 10 Epoch 1 Loss 3.065 Val Acc 10.000\n",
      "L 10 Epoch 2 Loss 2.604 Val Acc 10.720\n",
      "L 10 Epoch 3 Loss 2.172 Val Acc 14.480\n",
      "L 10 Epoch 4 Loss 1.988 Val Acc 19.720\n",
      "L 10 Epoch 5 Loss 1.884 Val Acc 17.720\n",
      "L 10 Epoch 6 Loss 1.837 Val Acc 22.520\n",
      "L 10 Epoch 7 Loss 1.779 Val Acc 25.140\n",
      "L 10 Epoch 8 Loss 1.721 Val Acc 28.120\n",
      "L 10 Epoch 9 Loss 1.663 Val Acc 31.640\n",
      "L 10 Epoch 10 Loss 1.695 Val Acc 28.280\n",
      "L 10 Epoch 11 Loss 1.649 Val Acc 34.400\n",
      "L 10 Epoch 12 Loss 1.581 Val Acc 35.420\n",
      "L 10 Epoch 13 Loss 1.519 Val Acc 36.060\n",
      "L 10 Epoch 14 Loss 1.509 Val Acc 38.360\n",
      "L 10 Epoch 15 Loss 1.441 Val Acc 36.040\n",
      "==> Saving model ...\n",
      "L 10 Epoch 16 Loss 1.446 Val Acc 40.240\n",
      "L 10 Epoch 17 Loss 1.430 Val Acc 38.360\n",
      "L 10 Epoch 18 Loss 1.373 Val Acc 37.480\n",
      "L 10 Epoch 19 Loss 1.395 Val Acc 37.080\n",
      "==> Saving model ...\n",
      "L 10 Epoch 20 Loss 1.324 Val Acc 40.820\n",
      "==> Saving model ...\n",
      "L 10 Epoch 21 Loss 1.311 Val Acc 42.300\n",
      "L 10 Epoch 22 Loss 1.257 Val Acc 39.360\n",
      "L 10 Epoch 23 Loss 1.211 Val Acc 39.140\n",
      "==> Saving model ...\n",
      "L 10 Epoch 24 Loss 1.240 Val Acc 42.680\n",
      "==> Saving model ...\n",
      "L 10 Epoch 25 Loss 1.154 Val Acc 43.720\n",
      "L 10 Epoch 26 Loss 1.110 Val Acc 42.580\n",
      "L 10 Epoch 27 Loss 1.106 Val Acc 42.620\n",
      "==> Saving model ...\n",
      "L 10 Epoch 28 Loss 1.100 Val Acc 46.180\n",
      "==> Saving model ...\n",
      "L 10 Epoch 29 Loss 1.027 Val Acc 47.060\n",
      "L 10 Epoch 30 Loss 1.105 Val Acc 41.360\n",
      "L 10 Epoch 31 Loss 0.988 Val Acc 45.120\n",
      "L 10 Epoch 32 Loss 0.974 Val Acc 45.340\n",
      "L 10 Epoch 33 Loss 0.879 Val Acc 42.580\n",
      "L 10 Epoch 34 Loss 0.873 Val Acc 44.960\n",
      "L 10 Epoch 35 Loss 0.844 Val Acc 47.060\n",
      "L 10 Epoch 36 Loss 0.851 Val Acc 45.600\n",
      "L 10 Epoch 37 Loss 0.858 Val Acc 41.660\n",
      "L 10 Epoch 38 Loss 0.924 Val Acc 45.180\n",
      "L 10 Epoch 39 Loss 0.842 Val Acc 46.160\n",
      "==> Saving model ...\n",
      "L 10 Epoch 40 Loss 0.657 Val Acc 49.260\n",
      "L 10 Epoch 41 Loss 0.622 Val Acc 48.460\n",
      "L 10 Epoch 42 Loss 0.681 Val Acc 48.620\n",
      "L 10 Epoch 43 Loss 0.561 Val Acc 47.040\n",
      "L 10 Epoch 44 Loss 0.592 Val Acc 45.600\n",
      "L 10 Epoch 45 Loss 0.648 Val Acc 46.660\n",
      "==> Saving model ...\n",
      "L 10 Epoch 46 Loss 0.666 Val Acc 50.400\n",
      "L 10 Epoch 47 Loss 0.593 Val Acc 47.780\n",
      "L 10 Epoch 48 Loss 0.466 Val Acc 48.280\n",
      "L 10 Epoch 49 Loss 0.436 Val Acc 48.220\n",
      "L 10 Epoch 50 Loss 0.501 Val Acc 49.660\n",
      "L 10 Epoch 51 Loss 0.448 Val Acc 47.220\n",
      "L 10 Epoch 52 Loss 0.404 Val Acc 49.180\n",
      "L 10 Epoch 53 Loss 0.362 Val Acc 47.700\n",
      "L 10 Epoch 54 Loss 0.297 Val Acc 50.360\n",
      "L 10 Epoch 55 Loss 0.298 Val Acc 49.580\n",
      "L 10 Epoch 56 Loss 0.362 Val Acc 48.400\n",
      "L 10 Epoch 57 Loss 0.240 Val Acc 49.220\n",
      "L 10 Epoch 58 Loss 0.293 Val Acc 50.200\n",
      "==> Saving model ...\n",
      "L 10 Epoch 59 Loss 0.314 Val Acc 52.200\n",
      "L 10 Epoch 60 Loss 0.382 Val Acc 49.800\n",
      "L 10 Epoch 61 Loss 0.302 Val Acc 46.980\n",
      "L 10 Epoch 62 Loss 0.365 Val Acc 47.720\n",
      "L 10 Epoch 63 Loss 0.262 Val Acc 48.000\n",
      "L 10 Epoch 64 Loss 0.314 Val Acc 49.960\n",
      "L 10 Epoch 65 Loss 0.187 Val Acc 51.480\n",
      "L 10 Epoch 66 Loss 0.209 Val Acc 49.120\n",
      "L 10 Epoch 67 Loss 0.215 Val Acc 51.420\n",
      "L 10 Epoch 68 Loss 0.191 Val Acc 51.360\n",
      "L 10 Epoch 69 Loss 0.189 Val Acc 50.400\n",
      "L 10 Epoch 70 Loss 0.299 Val Acc 51.560\n",
      "L 10 Epoch 71 Loss 0.131 Val Acc 51.540\n",
      "==> Saving model ...\n",
      "L 10 Epoch 72 Loss 0.207 Val Acc 52.760\n",
      "==> Saving model ...\n",
      "L 10 Epoch 73 Loss 0.086 Val Acc 52.980\n",
      "L 10 Epoch 74 Loss 0.151 Val Acc 51.680\n",
      "L 10 Epoch 75 Loss 0.149 Val Acc 50.240\n",
      "L 10 Epoch 76 Loss 0.111 Val Acc 51.860\n",
      "L 10 Epoch 77 Loss 0.134 Val Acc 52.640\n",
      "==> Saving model ...\n",
      "L 10 Epoch 78 Loss 0.153 Val Acc 53.760\n",
      "L 10 Epoch 79 Loss 0.135 Val Acc 52.660\n",
      "L 10 Epoch 80 Loss 0.125 Val Acc 53.040\n",
      "L 10 Epoch 81 Loss 0.195 Val Acc 53.100\n",
      "L 10 Epoch 82 Loss 0.133 Val Acc 50.700\n",
      "L 10 Epoch 83 Loss 0.213 Val Acc 52.480\n",
      "L 10 Epoch 84 Loss 0.156 Val Acc 52.460\n",
      "L 10 Epoch 85 Loss 0.144 Val Acc 52.820\n",
      "L 10 Epoch 86 Loss 0.187 Val Acc 52.760\n",
      "L 10 Epoch 87 Loss 0.169 Val Acc 51.400\n",
      "L 10 Epoch 88 Loss 0.138 Val Acc 51.560\n",
      "L 10 Epoch 89 Loss 0.131 Val Acc 52.820\n",
      "L 10 Epoch 90 Loss 0.101 Val Acc 53.740\n",
      "L 10 Epoch 91 Loss 0.095 Val Acc 53.020\n",
      "L 10 Epoch 92 Loss 0.093 Val Acc 52.800\n",
      "==> Saving model ...\n",
      "L 10 Epoch 93 Loss 0.117 Val Acc 53.960\n",
      "L 10 Epoch 94 Loss 0.072 Val Acc 53.320\n",
      "==> Saving model ...\n",
      "L 10 Epoch 95 Loss 0.077 Val Acc 55.080\n",
      "L 10 Epoch 96 Loss 0.065 Val Acc 55.000\n",
      "L 10 Epoch 97 Loss 0.066 Val Acc 54.760\n",
      "L 10 Epoch 98 Loss 0.069 Val Acc 53.320\n",
      "L 10 Epoch 99 Loss 0.053 Val Acc 54.420\n",
      "==> Saving model ...\n",
      "L 10 Epoch 100 Loss 0.035 Val Acc 55.920\n",
      "L 10 Epoch 101 Loss 0.071 Val Acc 55.020\n",
      "L 10 Epoch 102 Loss 0.067 Val Acc 54.560\n",
      "L 10 Epoch 103 Loss 0.073 Val Acc 54.240\n",
      "L 10 Epoch 104 Loss 0.059 Val Acc 54.540\n",
      "L 10 Epoch 105 Loss 0.054 Val Acc 54.280\n",
      "L 10 Epoch 106 Loss 0.055 Val Acc 54.980\n",
      "L 10 Epoch 107 Loss 0.082 Val Acc 54.600\n",
      "L 10 Epoch 108 Loss 0.103 Val Acc 54.160\n",
      "L 10 Epoch 109 Loss 0.051 Val Acc 55.180\n",
      "L 10 Epoch 110 Loss 0.044 Val Acc 53.760\n",
      "L 10 Epoch 111 Loss 0.080 Val Acc 54.320\n",
      "L 10 Epoch 112 Loss 0.107 Val Acc 54.900\n",
      "L 10 Epoch 113 Loss 0.079 Val Acc 53.820\n",
      "L 10 Epoch 114 Loss 0.052 Val Acc 54.540\n",
      "L 10 Epoch 115 Loss 0.048 Val Acc 54.820\n",
      "L 10 Epoch 116 Loss 0.041 Val Acc 54.500\n",
      "L 10 Epoch 117 Loss 0.062 Val Acc 54.660\n",
      "L 10 Epoch 118 Loss 0.075 Val Acc 55.160\n",
      "L 10 Epoch 119 Loss 0.032 Val Acc 54.980\n",
      "L 10 Epoch 120 Loss 0.046 Val Acc 55.840\n",
      "L 10 Epoch 121 Loss 0.051 Val Acc 55.820\n",
      "L 10 Epoch 122 Loss 0.027 Val Acc 55.200\n",
      "L 10 Epoch 123 Loss 0.023 Val Acc 55.500\n",
      "L 10 Epoch 124 Loss 0.018 Val Acc 55.820\n",
      "==> Saving model ...\n",
      "L 10 Epoch 125 Loss 0.024 Val Acc 56.360\n",
      "L 10 Epoch 126 Loss 0.030 Val Acc 56.160\n",
      "L 10 Epoch 127 Loss 0.030 Val Acc 55.560\n",
      "L 10 Epoch 128 Loss 0.035 Val Acc 55.900\n",
      "L 10 Epoch 129 Loss 0.021 Val Acc 55.600\n",
      "L 10 Epoch 130 Loss 0.015 Val Acc 55.880\n",
      "L 10 Epoch 131 Loss 0.013 Val Acc 55.880\n",
      "==> Saving model ...\n",
      "L 10 Epoch 132 Loss 0.024 Val Acc 56.420\n",
      "L 10 Epoch 133 Loss 0.028 Val Acc 56.200\n",
      "L 10 Epoch 134 Loss 0.022 Val Acc 56.020\n",
      "L 10 Epoch 135 Loss 0.013 Val Acc 56.200\n",
      "==> Saving model ...\n",
      "L 10 Epoch 136 Loss 0.028 Val Acc 56.580\n",
      "L 10 Epoch 137 Loss 0.005 Val Acc 56.340\n",
      "L 10 Epoch 138 Loss 0.014 Val Acc 56.320\n",
      "L 10 Epoch 139 Loss 0.031 Val Acc 56.220\n",
      "L 10 Epoch 140 Loss 0.013 Val Acc 56.420\n",
      "L 10 Epoch 141 Loss 0.011 Val Acc 56.220\n",
      "L 10 Epoch 142 Loss 0.024 Val Acc 56.040\n",
      "L 10 Epoch 143 Loss 0.022 Val Acc 55.660\n",
      "L 10 Epoch 144 Loss 0.022 Val Acc 55.620\n",
      "L 10 Epoch 145 Loss 0.027 Val Acc 56.400\n",
      "L 10 Epoch 146 Loss 0.018 Val Acc 56.060\n",
      "==> Saving model ...\n",
      "L 10 Epoch 147 Loss 0.032 Val Acc 56.740\n",
      "L 10 Epoch 148 Loss 0.037 Val Acc 56.680\n",
      "L 10 Epoch 149 Loss 0.019 Val Acc 56.580\n",
      "L 10 Epoch 150 Loss 0.012 Val Acc 56.740\n",
      "L 10 Epoch 151 Loss 0.021 Val Acc 56.720\n",
      "L 10 Epoch 152 Loss 0.020 Val Acc 56.720\n",
      "==> Saving model ...\n",
      "L 10 Epoch 153 Loss 0.012 Val Acc 56.820\n",
      "L 10 Epoch 154 Loss 0.021 Val Acc 56.660\n",
      "L 10 Epoch 155 Loss 0.021 Val Acc 56.620\n",
      "L 10 Epoch 156 Loss 0.014 Val Acc 56.540\n",
      "L 10 Epoch 157 Loss 0.009 Val Acc 56.560\n",
      "L 10 Epoch 158 Loss 0.020 Val Acc 56.140\n",
      "L 10 Epoch 159 Loss 0.010 Val Acc 56.380\n",
      "L 10 Epoch 160 Loss 0.014 Val Acc 56.560\n",
      "L 10 Epoch 161 Loss 0.008 Val Acc 56.720\n",
      "L 10 Epoch 162 Loss 0.014 Val Acc 56.540\n",
      "L 10 Epoch 163 Loss 0.023 Val Acc 56.040\n",
      "L 10 Epoch 164 Loss 0.022 Val Acc 56.380\n",
      "==> Saving model ...\n",
      "L 10 Epoch 165 Loss 0.006 Val Acc 56.840\n",
      "L 10 Epoch 166 Loss 0.013 Val Acc 56.600\n",
      "L 10 Epoch 167 Loss 0.024 Val Acc 56.500\n",
      "L 10 Epoch 168 Loss 0.012 Val Acc 56.480\n",
      "L 10 Epoch 169 Loss 0.016 Val Acc 56.500\n",
      "L 10 Epoch 170 Loss 0.016 Val Acc 56.560\n",
      "L 10 Epoch 171 Loss 0.013 Val Acc 56.800\n",
      "L 10 Epoch 172 Loss 0.017 Val Acc 56.500\n",
      "L 10 Epoch 173 Loss 0.012 Val Acc 56.420\n",
      "L 10 Epoch 174 Loss 0.018 Val Acc 56.680\n",
      "L 10 Epoch 175 Loss 0.012 Val Acc 56.660\n",
      "L 10 Epoch 176 Loss 0.005 Val Acc 56.760\n",
      "L 10 Epoch 177 Loss 0.021 Val Acc 56.680\n",
      "L 10 Epoch 178 Loss 0.016 Val Acc 56.660\n",
      "L 10 Epoch 179 Loss 0.020 Val Acc 56.620\n",
      "L 10 Epoch 180 Loss 0.010 Val Acc 56.620\n",
      "L 10 Epoch 181 Loss 0.004 Val Acc 56.620\n",
      "L 10 Epoch 182 Loss 0.004 Val Acc 56.640\n",
      "L 10 Epoch 183 Loss 0.013 Val Acc 56.620\n",
      "L 10 Epoch 184 Loss 0.012 Val Acc 56.800\n",
      "L 10 Epoch 185 Loss 0.013 Val Acc 56.640\n",
      "L 10 Epoch 186 Loss 0.009 Val Acc 56.520\n",
      "L 10 Epoch 187 Loss 0.009 Val Acc 56.720\n",
      "L 10 Epoch 188 Loss 0.025 Val Acc 56.580\n",
      "L 10 Epoch 189 Loss 0.005 Val Acc 56.820\n",
      "L 10 Epoch 190 Loss 0.017 Val Acc 56.360\n",
      "L 10 Epoch 191 Loss 0.012 Val Acc 56.540\n",
      "L 10 Epoch 192 Loss 0.009 Val Acc 56.560\n",
      "L 10 Epoch 193 Loss 0.008 Val Acc 56.700\n",
      "L 10 Epoch 194 Loss 0.007 Val Acc 56.780\n",
      "L 10 Epoch 195 Loss 0.005 Val Acc 56.800\n",
      "L 10 Epoch 196 Loss 0.002 Val Acc 56.820\n",
      "L 10 Epoch 197 Loss 0.027 Val Acc 56.480\n",
      "L 10 Epoch 198 Loss 0.017 Val Acc 56.540\n",
      "L 10 Epoch 199 Loss 0.022 Val Acc 56.660\n",
      "L 4 Epoch 0 Loss 3.789 Val Acc 10.000\n",
      "L 4 Epoch 1 Loss 2.997 Val Acc 10.000\n",
      "L 4 Epoch 2 Loss 2.414 Val Acc 10.560\n",
      "L 4 Epoch 3 Loss 2.040 Val Acc 15.000\n",
      "L 4 Epoch 4 Loss 2.018 Val Acc 18.580\n",
      "L 4 Epoch 5 Loss 1.898 Val Acc 21.940\n",
      "L 4 Epoch 6 Loss 1.738 Val Acc 29.000\n",
      "L 4 Epoch 7 Loss 1.762 Val Acc 32.580\n",
      "L 4 Epoch 8 Loss 1.741 Val Acc 35.160\n",
      "L 4 Epoch 9 Loss 1.657 Val Acc 30.180\n",
      "L 4 Epoch 10 Loss 1.588 Val Acc 33.640\n",
      "L 4 Epoch 11 Loss 1.618 Val Acc 34.760\n",
      "L 4 Epoch 12 Loss 1.534 Val Acc 37.180\n",
      "L 4 Epoch 13 Loss 1.488 Val Acc 37.040\n",
      "L 4 Epoch 14 Loss 1.423 Val Acc 38.200\n",
      "L 4 Epoch 15 Loss 1.388 Val Acc 39.480\n",
      "L 4 Epoch 16 Loss 1.356 Val Acc 38.200\n",
      "==> Saving model ...\n",
      "L 4 Epoch 17 Loss 1.347 Val Acc 41.160\n",
      "==> Saving model ...\n",
      "L 4 Epoch 18 Loss 1.316 Val Acc 43.220\n",
      "L 4 Epoch 19 Loss 1.259 Val Acc 38.900\n",
      "L 4 Epoch 20 Loss 1.281 Val Acc 39.460\n",
      "==> Saving model ...\n",
      "L 4 Epoch 21 Loss 1.172 Val Acc 43.300\n",
      "L 4 Epoch 22 Loss 1.170 Val Acc 42.420\n",
      "L 4 Epoch 23 Loss 1.039 Val Acc 43.100\n",
      "==> Saving model ...\n",
      "L 4 Epoch 24 Loss 1.047 Val Acc 43.400\n",
      "L 4 Epoch 25 Loss 1.170 Val Acc 39.640\n",
      "L 4 Epoch 26 Loss 1.040 Val Acc 37.380\n",
      "==> Saving model ...\n",
      "L 4 Epoch 27 Loss 1.096 Val Acc 44.660\n",
      "==> Saving model ...\n",
      "L 4 Epoch 28 Loss 1.012 Val Acc 45.920\n",
      "L 4 Epoch 29 Loss 0.917 Val Acc 45.120\n",
      "L 4 Epoch 30 Loss 0.848 Val Acc 44.260\n",
      "L 4 Epoch 31 Loss 0.906 Val Acc 42.140\n",
      "L 4 Epoch 32 Loss 0.816 Val Acc 45.240\n",
      "L 4 Epoch 33 Loss 0.807 Val Acc 43.660\n",
      "L 4 Epoch 34 Loss 0.722 Val Acc 44.260\n",
      "L 4 Epoch 35 Loss 0.784 Val Acc 44.060\n",
      "L 4 Epoch 36 Loss 0.646 Val Acc 43.920\n",
      "L 4 Epoch 37 Loss 0.576 Val Acc 42.560\n",
      "L 4 Epoch 38 Loss 0.676 Val Acc 44.380\n",
      "==> Saving model ...\n",
      "L 4 Epoch 39 Loss 0.564 Val Acc 47.660\n",
      "L 4 Epoch 40 Loss 0.547 Val Acc 42.360\n",
      "L 4 Epoch 41 Loss 0.497 Val Acc 46.420\n",
      "L 4 Epoch 42 Loss 0.561 Val Acc 47.540\n",
      "L 4 Epoch 43 Loss 0.583 Val Acc 44.600\n",
      "L 4 Epoch 44 Loss 0.422 Val Acc 44.700\n",
      "==> Saving model ...\n",
      "L 4 Epoch 45 Loss 0.321 Val Acc 48.560\n",
      "L 4 Epoch 46 Loss 0.386 Val Acc 45.900\n",
      "L 4 Epoch 47 Loss 0.384 Val Acc 47.080\n",
      "L 4 Epoch 48 Loss 0.269 Val Acc 42.840\n",
      "L 4 Epoch 49 Loss 0.342 Val Acc 45.820\n",
      "L 4 Epoch 50 Loss 0.372 Val Acc 44.120\n",
      "L 4 Epoch 51 Loss 0.427 Val Acc 44.460\n",
      "L 4 Epoch 52 Loss 0.421 Val Acc 46.540\n",
      "L 4 Epoch 53 Loss 0.376 Val Acc 45.460\n",
      "L 4 Epoch 54 Loss 0.281 Val Acc 44.780\n",
      "==> Saving model ...\n",
      "L 4 Epoch 55 Loss 0.252 Val Acc 48.640\n",
      "L 4 Epoch 56 Loss 0.264 Val Acc 47.740\n",
      "==> Saving model ...\n",
      "L 4 Epoch 57 Loss 0.249 Val Acc 49.700\n",
      "L 4 Epoch 58 Loss 0.397 Val Acc 48.520\n",
      "L 4 Epoch 59 Loss 0.223 Val Acc 47.260\n",
      "L 4 Epoch 60 Loss 0.296 Val Acc 46.920\n",
      "L 4 Epoch 61 Loss 0.292 Val Acc 48.500\n",
      "L 4 Epoch 62 Loss 0.205 Val Acc 48.120\n",
      "==> Saving model ...\n",
      "L 4 Epoch 63 Loss 0.169 Val Acc 50.660\n",
      "L 4 Epoch 64 Loss 0.216 Val Acc 48.380\n",
      "L 4 Epoch 65 Loss 0.167 Val Acc 49.380\n",
      "L 4 Epoch 66 Loss 0.164 Val Acc 49.900\n",
      "L 4 Epoch 67 Loss 0.222 Val Acc 48.860\n",
      "L 4 Epoch 68 Loss 0.182 Val Acc 49.080\n",
      "L 4 Epoch 69 Loss 0.215 Val Acc 49.840\n",
      "L 4 Epoch 70 Loss 0.128 Val Acc 46.700\n",
      "==> Saving model ...\n",
      "L 4 Epoch 71 Loss 0.117 Val Acc 50.760\n",
      "L 4 Epoch 72 Loss 0.149 Val Acc 50.000\n",
      "L 4 Epoch 73 Loss 0.132 Val Acc 47.520\n",
      "L 4 Epoch 74 Loss 0.144 Val Acc 49.920\n",
      "L 4 Epoch 75 Loss 0.083 Val Acc 50.040\n",
      "==> Saving model ...\n",
      "L 4 Epoch 76 Loss 0.113 Val Acc 51.200\n",
      "L 4 Epoch 77 Loss 0.165 Val Acc 49.000\n",
      "L 4 Epoch 78 Loss 0.109 Val Acc 49.440\n",
      "==> Saving model ...\n",
      "L 4 Epoch 79 Loss 0.124 Val Acc 51.320\n",
      "L 4 Epoch 80 Loss 0.091 Val Acc 50.760\n",
      "L 4 Epoch 81 Loss 0.118 Val Acc 50.500\n",
      "L 4 Epoch 82 Loss 0.143 Val Acc 50.660\n",
      "L 4 Epoch 83 Loss 0.109 Val Acc 51.280\n",
      "L 4 Epoch 84 Loss 0.115 Val Acc 50.340\n",
      "L 4 Epoch 85 Loss 0.105 Val Acc 49.300\n",
      "==> Saving model ...\n",
      "L 4 Epoch 86 Loss 0.096 Val Acc 52.000\n",
      "L 4 Epoch 87 Loss 0.096 Val Acc 50.720\n",
      "L 4 Epoch 88 Loss 0.114 Val Acc 50.180\n",
      "L 4 Epoch 89 Loss 0.072 Val Acc 50.780\n",
      "L 4 Epoch 90 Loss 0.103 Val Acc 50.840\n",
      "L 4 Epoch 91 Loss 0.121 Val Acc 50.920\n",
      "L 4 Epoch 92 Loss 0.090 Val Acc 51.640\n",
      "L 4 Epoch 93 Loss 0.082 Val Acc 51.740\n",
      "L 4 Epoch 94 Loss 0.051 Val Acc 51.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 95 Loss 0.070 Val Acc 52.200\n",
      "L 4 Epoch 96 Loss 0.080 Val Acc 51.380\n",
      "L 4 Epoch 97 Loss 0.075 Val Acc 51.820\n",
      "==> Saving model ...\n",
      "L 4 Epoch 98 Loss 0.063 Val Acc 52.480\n",
      "L 4 Epoch 99 Loss 0.081 Val Acc 51.300\n",
      "L 4 Epoch 100 Loss 0.050 Val Acc 51.040\n",
      "L 4 Epoch 101 Loss 0.073 Val Acc 51.920\n",
      "L 4 Epoch 102 Loss 0.064 Val Acc 52.000\n",
      "L 4 Epoch 103 Loss 0.059 Val Acc 51.240\n",
      "L 4 Epoch 104 Loss 0.057 Val Acc 51.360\n",
      "L 4 Epoch 105 Loss 0.028 Val Acc 51.100\n",
      "L 4 Epoch 106 Loss 0.019 Val Acc 51.880\n",
      "==> Saving model ...\n",
      "L 4 Epoch 107 Loss 0.029 Val Acc 52.980\n",
      "L 4 Epoch 108 Loss 0.038 Val Acc 52.640\n",
      "L 4 Epoch 109 Loss 0.026 Val Acc 52.720\n",
      "L 4 Epoch 110 Loss 0.045 Val Acc 52.520\n",
      "L 4 Epoch 111 Loss 0.036 Val Acc 52.000\n",
      "L 4 Epoch 112 Loss 0.040 Val Acc 52.620\n",
      "L 4 Epoch 113 Loss 0.066 Val Acc 52.500\n",
      "L 4 Epoch 114 Loss 0.037 Val Acc 52.220\n",
      "L 4 Epoch 115 Loss 0.040 Val Acc 52.100\n",
      "L 4 Epoch 116 Loss 0.070 Val Acc 51.500\n",
      "L 4 Epoch 117 Loss 0.038 Val Acc 52.060\n",
      "L 4 Epoch 118 Loss 0.020 Val Acc 52.940\n",
      "==> Saving model ...\n",
      "L 4 Epoch 119 Loss 0.028 Val Acc 53.160\n",
      "L 4 Epoch 120 Loss 0.040 Val Acc 52.360\n",
      "L 4 Epoch 121 Loss 0.042 Val Acc 50.800\n",
      "L 4 Epoch 122 Loss 0.022 Val Acc 51.240\n",
      "L 4 Epoch 123 Loss 0.049 Val Acc 52.220\n",
      "L 4 Epoch 124 Loss 0.031 Val Acc 52.180\n",
      "L 4 Epoch 125 Loss 0.028 Val Acc 52.760\n",
      "L 4 Epoch 126 Loss 0.036 Val Acc 53.160\n",
      "L 4 Epoch 127 Loss 0.033 Val Acc 52.620\n",
      "L 4 Epoch 128 Loss 0.020 Val Acc 51.920\n",
      "L 4 Epoch 129 Loss 0.022 Val Acc 52.500\n",
      "L 4 Epoch 130 Loss 0.021 Val Acc 52.060\n",
      "L 4 Epoch 131 Loss 0.029 Val Acc 52.920\n",
      "==> Saving model ...\n",
      "L 4 Epoch 132 Loss 0.024 Val Acc 53.580\n",
      "L 4 Epoch 133 Loss 0.033 Val Acc 53.280\n",
      "L 4 Epoch 134 Loss 0.018 Val Acc 53.380\n",
      "==> Saving model ...\n",
      "L 4 Epoch 135 Loss 0.019 Val Acc 53.960\n",
      "L 4 Epoch 136 Loss 0.010 Val Acc 53.940\n",
      "L 4 Epoch 137 Loss 0.031 Val Acc 53.460\n",
      "L 4 Epoch 138 Loss 0.020 Val Acc 53.080\n",
      "L 4 Epoch 139 Loss 0.016 Val Acc 53.440\n",
      "L 4 Epoch 140 Loss 0.040 Val Acc 53.040\n",
      "L 4 Epoch 141 Loss 0.012 Val Acc 53.080\n",
      "L 4 Epoch 142 Loss 0.010 Val Acc 53.480\n",
      "L 4 Epoch 143 Loss 0.016 Val Acc 53.500\n",
      "L 4 Epoch 144 Loss 0.019 Val Acc 53.220\n",
      "L 4 Epoch 145 Loss 0.011 Val Acc 53.460\n",
      "L 4 Epoch 146 Loss 0.020 Val Acc 53.640\n",
      "L 4 Epoch 147 Loss 0.029 Val Acc 53.540\n",
      "L 4 Epoch 148 Loss 0.019 Val Acc 53.160\n",
      "L 4 Epoch 149 Loss 0.012 Val Acc 53.380\n",
      "L 4 Epoch 150 Loss 0.007 Val Acc 53.500\n",
      "L 4 Epoch 151 Loss 0.014 Val Acc 53.520\n",
      "L 4 Epoch 152 Loss 0.016 Val Acc 53.860\n",
      "L 4 Epoch 153 Loss 0.010 Val Acc 53.960\n",
      "L 4 Epoch 154 Loss 0.011 Val Acc 53.500\n",
      "L 4 Epoch 155 Loss 0.006 Val Acc 53.800\n",
      "L 4 Epoch 156 Loss 0.025 Val Acc 53.700\n",
      "L 4 Epoch 157 Loss 0.016 Val Acc 53.120\n",
      "L 4 Epoch 158 Loss 0.019 Val Acc 53.020\n",
      "L 4 Epoch 159 Loss 0.009 Val Acc 53.680\n",
      "==> Saving model ...\n",
      "L 4 Epoch 160 Loss 0.006 Val Acc 54.040\n",
      "L 4 Epoch 161 Loss 0.009 Val Acc 54.040\n",
      "L 4 Epoch 162 Loss 0.012 Val Acc 53.820\n",
      "L 4 Epoch 163 Loss 0.011 Val Acc 53.400\n",
      "L 4 Epoch 164 Loss 0.008 Val Acc 53.880\n",
      "L 4 Epoch 165 Loss 0.019 Val Acc 53.880\n",
      "L 4 Epoch 166 Loss 0.007 Val Acc 53.900\n",
      "==> Saving model ...\n",
      "L 4 Epoch 167 Loss 0.009 Val Acc 54.100\n",
      "L 4 Epoch 168 Loss 0.013 Val Acc 53.760\n",
      "==> Saving model ...\n",
      "L 4 Epoch 169 Loss 0.012 Val Acc 54.120\n",
      "L 4 Epoch 170 Loss 0.008 Val Acc 53.920\n",
      "L 4 Epoch 171 Loss 0.006 Val Acc 54.020\n",
      "==> Saving model ...\n",
      "L 4 Epoch 172 Loss 0.004 Val Acc 54.420\n",
      "L 4 Epoch 173 Loss 0.018 Val Acc 53.960\n",
      "L 4 Epoch 174 Loss 0.012 Val Acc 53.800\n",
      "L 4 Epoch 175 Loss 0.010 Val Acc 53.980\n",
      "L 4 Epoch 176 Loss 0.017 Val Acc 53.520\n",
      "L 4 Epoch 177 Loss 0.013 Val Acc 52.880\n",
      "L 4 Epoch 178 Loss 0.004 Val Acc 54.140\n",
      "L 4 Epoch 179 Loss 0.010 Val Acc 54.380\n",
      "L 4 Epoch 180 Loss 0.007 Val Acc 54.360\n",
      "L 4 Epoch 181 Loss 0.008 Val Acc 54.280\n",
      "L 4 Epoch 182 Loss 0.009 Val Acc 54.200\n",
      "L 4 Epoch 183 Loss 0.005 Val Acc 53.940\n",
      "L 4 Epoch 184 Loss 0.003 Val Acc 54.320\n",
      "L 4 Epoch 185 Loss 0.010 Val Acc 54.000\n",
      "L 4 Epoch 186 Loss 0.006 Val Acc 54.180\n",
      "L 4 Epoch 187 Loss 0.006 Val Acc 54.260\n",
      "==> Saving model ...\n",
      "L 4 Epoch 188 Loss 0.005 Val Acc 54.460\n",
      "L 4 Epoch 189 Loss 0.009 Val Acc 54.300\n",
      "L 4 Epoch 190 Loss 0.007 Val Acc 54.140\n",
      "L 4 Epoch 191 Loss 0.011 Val Acc 53.480\n",
      "L 4 Epoch 192 Loss 0.012 Val Acc 53.520\n",
      "L 4 Epoch 193 Loss 0.007 Val Acc 53.980\n",
      "L 4 Epoch 194 Loss 0.007 Val Acc 53.960\n",
      "L 4 Epoch 195 Loss 0.009 Val Acc 53.740\n",
      "L 4 Epoch 196 Loss 0.004 Val Acc 54.240\n",
      "L 4 Epoch 197 Loss 0.004 Val Acc 54.120\n",
      "L 4 Epoch 198 Loss 0.006 Val Acc 53.920\n",
      "L 4 Epoch 199 Loss 0.006 Val Acc 54.100\n",
      "L 6 Epoch 0 Loss 4.031 Val Acc 10.000\n",
      "L 6 Epoch 1 Loss 3.322 Val Acc 9.360\n",
      "L 6 Epoch 2 Loss 2.667 Val Acc 13.840\n",
      "L 6 Epoch 3 Loss 2.289 Val Acc 17.540\n",
      "L 6 Epoch 4 Loss 2.178 Val Acc 21.080\n",
      "L 6 Epoch 5 Loss 2.133 Val Acc 19.120\n",
      "L 6 Epoch 6 Loss 2.035 Val Acc 22.640\n",
      "L 6 Epoch 7 Loss 1.953 Val Acc 28.760\n",
      "L 6 Epoch 8 Loss 1.777 Val Acc 31.800\n",
      "L 6 Epoch 9 Loss 1.768 Val Acc 33.460\n",
      "L 6 Epoch 10 Loss 1.714 Val Acc 33.960\n",
      "L 6 Epoch 11 Loss 1.630 Val Acc 34.100\n",
      "L 6 Epoch 12 Loss 1.599 Val Acc 38.300\n",
      "L 6 Epoch 13 Loss 1.530 Val Acc 38.520\n",
      "L 6 Epoch 14 Loss 1.519 Val Acc 37.800\n",
      "L 6 Epoch 15 Loss 1.524 Val Acc 39.760\n",
      "L 6 Epoch 16 Loss 1.484 Val Acc 38.580\n",
      "L 6 Epoch 17 Loss 1.448 Val Acc 39.480\n",
      "L 6 Epoch 18 Loss 1.452 Val Acc 38.060\n",
      "==> Saving model ...\n",
      "L 6 Epoch 19 Loss 1.352 Val Acc 42.960\n",
      "L 6 Epoch 20 Loss 1.406 Val Acc 40.220\n",
      "L 6 Epoch 21 Loss 1.323 Val Acc 42.000\n",
      "L 6 Epoch 22 Loss 1.279 Val Acc 42.560\n",
      "==> Saving model ...\n",
      "L 6 Epoch 23 Loss 1.275 Val Acc 44.500\n",
      "L 6 Epoch 24 Loss 1.165 Val Acc 43.380\n",
      "L 6 Epoch 25 Loss 1.201 Val Acc 42.480\n",
      "L 6 Epoch 26 Loss 1.124 Val Acc 38.780\n",
      "==> Saving model ...\n",
      "L 6 Epoch 27 Loss 1.113 Val Acc 44.540\n",
      "==> Saving model ...\n",
      "L 6 Epoch 28 Loss 1.036 Val Acc 46.520\n",
      "L 6 Epoch 29 Loss 0.991 Val Acc 46.340\n",
      "==> Saving model ...\n",
      "L 6 Epoch 30 Loss 1.084 Val Acc 49.020\n",
      "L 6 Epoch 31 Loss 0.945 Val Acc 39.440\n",
      "L 6 Epoch 32 Loss 0.865 Val Acc 43.780\n",
      "L 6 Epoch 33 Loss 0.986 Val Acc 46.980\n",
      "L 6 Epoch 34 Loss 0.778 Val Acc 46.240\n",
      "L 6 Epoch 35 Loss 0.824 Val Acc 42.720\n",
      "L 6 Epoch 36 Loss 0.850 Val Acc 48.820\n",
      "L 6 Epoch 37 Loss 0.743 Val Acc 44.380\n",
      "L 6 Epoch 38 Loss 0.831 Val Acc 47.580\n",
      "L 6 Epoch 39 Loss 0.789 Val Acc 45.900\n",
      "L 6 Epoch 40 Loss 0.538 Val Acc 42.840\n",
      "L 6 Epoch 41 Loss 0.694 Val Acc 44.400\n",
      "L 6 Epoch 42 Loss 0.672 Val Acc 45.860\n",
      "L 6 Epoch 43 Loss 0.678 Val Acc 46.580\n",
      "L 6 Epoch 44 Loss 0.618 Val Acc 46.540\n",
      "L 6 Epoch 45 Loss 0.548 Val Acc 46.220\n",
      "==> Saving model ...\n",
      "L 6 Epoch 46 Loss 0.448 Val Acc 50.980\n",
      "L 6 Epoch 47 Loss 0.597 Val Acc 49.000\n",
      "==> Saving model ...\n",
      "L 6 Epoch 48 Loss 0.382 Val Acc 51.640\n",
      "L 6 Epoch 49 Loss 0.446 Val Acc 49.780\n",
      "==> Saving model ...\n",
      "L 6 Epoch 50 Loss 0.323 Val Acc 52.380\n",
      "L 6 Epoch 51 Loss 0.290 Val Acc 50.100\n",
      "==> Saving model ...\n",
      "L 6 Epoch 52 Loss 0.254 Val Acc 53.940\n",
      "L 6 Epoch 53 Loss 0.372 Val Acc 51.260\n",
      "L 6 Epoch 54 Loss 0.323 Val Acc 51.260\n",
      "L 6 Epoch 55 Loss 0.350 Val Acc 51.800\n",
      "L 6 Epoch 56 Loss 0.240 Val Acc 50.420\n",
      "L 6 Epoch 57 Loss 0.332 Val Acc 51.720\n",
      "L 6 Epoch 58 Loss 0.379 Val Acc 49.260\n",
      "L 6 Epoch 59 Loss 0.304 Val Acc 51.860\n",
      "L 6 Epoch 60 Loss 0.306 Val Acc 50.300\n",
      "L 6 Epoch 61 Loss 0.289 Val Acc 53.260\n",
      "L 6 Epoch 62 Loss 0.249 Val Acc 52.400\n",
      "L 6 Epoch 63 Loss 0.195 Val Acc 51.120\n",
      "L 6 Epoch 64 Loss 0.189 Val Acc 53.860\n",
      "L 6 Epoch 65 Loss 0.149 Val Acc 52.240\n",
      "L 6 Epoch 66 Loss 0.192 Val Acc 51.400\n",
      "L 6 Epoch 67 Loss 0.123 Val Acc 53.560\n",
      "L 6 Epoch 68 Loss 0.137 Val Acc 53.600\n",
      "L 6 Epoch 69 Loss 0.188 Val Acc 51.320\n",
      "L 6 Epoch 70 Loss 0.156 Val Acc 53.200\n",
      "L 6 Epoch 71 Loss 0.218 Val Acc 52.080\n",
      "L 6 Epoch 72 Loss 0.145 Val Acc 51.140\n",
      "L 6 Epoch 73 Loss 0.159 Val Acc 52.300\n",
      "==> Saving model ...\n",
      "L 6 Epoch 74 Loss 0.238 Val Acc 54.180\n",
      "L 6 Epoch 75 Loss 0.201 Val Acc 52.520\n",
      "L 6 Epoch 76 Loss 0.214 Val Acc 52.960\n",
      "L 6 Epoch 77 Loss 0.229 Val Acc 51.900\n",
      "L 6 Epoch 78 Loss 0.202 Val Acc 53.020\n",
      "L 6 Epoch 79 Loss 0.171 Val Acc 47.080\n",
      "L 6 Epoch 80 Loss 0.153 Val Acc 53.080\n",
      "L 6 Epoch 81 Loss 0.148 Val Acc 53.040\n",
      "L 6 Epoch 82 Loss 0.146 Val Acc 53.100\n",
      "L 6 Epoch 83 Loss 0.136 Val Acc 53.800\n",
      "L 6 Epoch 84 Loss 0.120 Val Acc 53.800\n",
      "==> Saving model ...\n",
      "L 6 Epoch 85 Loss 0.067 Val Acc 54.720\n",
      "L 6 Epoch 86 Loss 0.099 Val Acc 52.460\n",
      "L 6 Epoch 87 Loss 0.055 Val Acc 54.660\n",
      "==> Saving model ...\n",
      "L 6 Epoch 88 Loss 0.077 Val Acc 55.400\n",
      "L 6 Epoch 89 Loss 0.046 Val Acc 54.380\n",
      "L 6 Epoch 90 Loss 0.084 Val Acc 55.380\n",
      "L 6 Epoch 91 Loss 0.037 Val Acc 54.500\n",
      "L 6 Epoch 92 Loss 0.076 Val Acc 53.260\n",
      "L 6 Epoch 93 Loss 0.078 Val Acc 52.920\n",
      "L 6 Epoch 94 Loss 0.128 Val Acc 53.980\n",
      "L 6 Epoch 95 Loss 0.081 Val Acc 52.420\n",
      "L 6 Epoch 96 Loss 0.104 Val Acc 54.340\n",
      "L 6 Epoch 97 Loss 0.086 Val Acc 53.600\n",
      "L 6 Epoch 98 Loss 0.067 Val Acc 54.040\n",
      "L 6 Epoch 99 Loss 0.063 Val Acc 54.420\n",
      "L 6 Epoch 100 Loss 0.049 Val Acc 55.060\n",
      "L 6 Epoch 101 Loss 0.065 Val Acc 54.360\n",
      "L 6 Epoch 102 Loss 0.044 Val Acc 53.620\n",
      "L 6 Epoch 103 Loss 0.019 Val Acc 55.120\n",
      "L 6 Epoch 104 Loss 0.076 Val Acc 54.420\n",
      "L 6 Epoch 105 Loss 0.050 Val Acc 54.700\n",
      "L 6 Epoch 106 Loss 0.058 Val Acc 54.640\n",
      "L 6 Epoch 107 Loss 0.031 Val Acc 54.680\n",
      "==> Saving model ...\n",
      "L 6 Epoch 108 Loss 0.064 Val Acc 55.440\n",
      "L 6 Epoch 109 Loss 0.062 Val Acc 55.080\n",
      "==> Saving model ...\n",
      "L 6 Epoch 110 Loss 0.042 Val Acc 55.700\n",
      "L 6 Epoch 111 Loss 0.033 Val Acc 55.660\n",
      "L 6 Epoch 112 Loss 0.056 Val Acc 55.040\n",
      "L 6 Epoch 113 Loss 0.039 Val Acc 54.000\n",
      "L 6 Epoch 114 Loss 0.039 Val Acc 55.420\n",
      "==> Saving model ...\n",
      "L 6 Epoch 115 Loss 0.026 Val Acc 56.800\n",
      "==> Saving model ...\n",
      "L 6 Epoch 116 Loss 0.027 Val Acc 56.980\n",
      "==> Saving model ...\n",
      "L 6 Epoch 117 Loss 0.011 Val Acc 57.060\n",
      "L 6 Epoch 118 Loss 0.034 Val Acc 57.040\n",
      "L 6 Epoch 119 Loss 0.042 Val Acc 56.620\n",
      "L 6 Epoch 120 Loss 0.054 Val Acc 56.500\n",
      "L 6 Epoch 121 Loss 0.018 Val Acc 56.480\n",
      "L 6 Epoch 122 Loss 0.024 Val Acc 56.600\n",
      "L 6 Epoch 123 Loss 0.023 Val Acc 56.560\n",
      "L 6 Epoch 124 Loss 0.030 Val Acc 56.780\n",
      "L 6 Epoch 125 Loss 0.027 Val Acc 56.420\n",
      "L 6 Epoch 126 Loss 0.029 Val Acc 56.220\n",
      "==> Saving model ...\n",
      "L 6 Epoch 127 Loss 0.031 Val Acc 57.240\n",
      "L 6 Epoch 128 Loss 0.034 Val Acc 56.840\n",
      "L 6 Epoch 129 Loss 0.021 Val Acc 56.780\n",
      "L 6 Epoch 130 Loss 0.018 Val Acc 56.860\n",
      "L 6 Epoch 131 Loss 0.024 Val Acc 56.420\n",
      "L 6 Epoch 132 Loss 0.029 Val Acc 56.240\n",
      "L 6 Epoch 133 Loss 0.025 Val Acc 56.160\n",
      "L 6 Epoch 134 Loss 0.024 Val Acc 57.200\n",
      "==> Saving model ...\n",
      "L 6 Epoch 135 Loss 0.020 Val Acc 57.360\n",
      "==> Saving model ...\n",
      "L 6 Epoch 136 Loss 0.036 Val Acc 57.420\n",
      "L 6 Epoch 137 Loss 0.028 Val Acc 56.740\n",
      "L 6 Epoch 138 Loss 0.033 Val Acc 56.360\n",
      "L 6 Epoch 139 Loss 0.014 Val Acc 56.260\n",
      "L 6 Epoch 140 Loss 0.014 Val Acc 56.600\n",
      "L 6 Epoch 141 Loss 0.012 Val Acc 57.140\n",
      "L 6 Epoch 142 Loss 0.010 Val Acc 57.360\n",
      "==> Saving model ...\n",
      "L 6 Epoch 143 Loss 0.012 Val Acc 57.980\n",
      "L 6 Epoch 144 Loss 0.010 Val Acc 57.800\n",
      "L 6 Epoch 145 Loss 0.032 Val Acc 57.320\n",
      "L 6 Epoch 146 Loss 0.014 Val Acc 57.500\n",
      "L 6 Epoch 147 Loss 0.014 Val Acc 57.320\n",
      "L 6 Epoch 148 Loss 0.025 Val Acc 57.040\n",
      "L 6 Epoch 149 Loss 0.010 Val Acc 56.860\n",
      "L 6 Epoch 150 Loss 0.023 Val Acc 56.780\n",
      "L 6 Epoch 151 Loss 0.011 Val Acc 57.220\n",
      "L 6 Epoch 152 Loss 0.008 Val Acc 57.860\n",
      "L 6 Epoch 153 Loss 0.026 Val Acc 57.920\n",
      "L 6 Epoch 154 Loss 0.014 Val Acc 57.880\n",
      "L 6 Epoch 155 Loss 0.012 Val Acc 57.820\n",
      "==> Saving model ...\n",
      "L 6 Epoch 156 Loss 0.017 Val Acc 58.040\n",
      "==> Saving model ...\n",
      "L 6 Epoch 157 Loss 0.022 Val Acc 58.140\n",
      "L 6 Epoch 158 Loss 0.015 Val Acc 58.000\n",
      "L 6 Epoch 159 Loss 0.007 Val Acc 57.820\n",
      "L 6 Epoch 160 Loss 0.009 Val Acc 57.840\n",
      "L 6 Epoch 161 Loss 0.006 Val Acc 57.740\n",
      "L 6 Epoch 162 Loss 0.017 Val Acc 57.600\n",
      "L 6 Epoch 163 Loss 0.006 Val Acc 57.580\n",
      "L 6 Epoch 164 Loss 0.016 Val Acc 57.580\n",
      "L 6 Epoch 165 Loss 0.008 Val Acc 57.580\n",
      "L 6 Epoch 166 Loss 0.014 Val Acc 57.820\n",
      "L 6 Epoch 167 Loss 0.010 Val Acc 57.740\n",
      "L 6 Epoch 168 Loss 0.014 Val Acc 57.460\n",
      "L 6 Epoch 169 Loss 0.013 Val Acc 56.980\n",
      "L 6 Epoch 170 Loss 0.015 Val Acc 57.180\n",
      "L 6 Epoch 171 Loss 0.017 Val Acc 57.440\n",
      "L 6 Epoch 172 Loss 0.008 Val Acc 57.600\n",
      "L 6 Epoch 173 Loss 0.017 Val Acc 57.180\n",
      "L 6 Epoch 174 Loss 0.012 Val Acc 57.480\n",
      "L 6 Epoch 175 Loss 0.011 Val Acc 57.560\n",
      "L 6 Epoch 176 Loss 0.004 Val Acc 57.660\n",
      "L 6 Epoch 177 Loss 0.012 Val Acc 57.500\n",
      "L 6 Epoch 178 Loss 0.006 Val Acc 57.580\n",
      "L 6 Epoch 179 Loss 0.010 Val Acc 57.540\n",
      "L 6 Epoch 180 Loss 0.012 Val Acc 56.900\n",
      "L 6 Epoch 181 Loss 0.014 Val Acc 57.480\n",
      "L 6 Epoch 182 Loss 0.012 Val Acc 57.640\n",
      "L 6 Epoch 183 Loss 0.010 Val Acc 57.580\n",
      "L 6 Epoch 184 Loss 0.004 Val Acc 57.860\n",
      "L 6 Epoch 185 Loss 0.009 Val Acc 57.340\n",
      "L 6 Epoch 186 Loss 0.022 Val Acc 57.160\n",
      "L 6 Epoch 187 Loss 0.006 Val Acc 57.620\n",
      "L 6 Epoch 188 Loss 0.018 Val Acc 57.260\n",
      "L 6 Epoch 189 Loss 0.010 Val Acc 57.200\n",
      "L 6 Epoch 190 Loss 0.011 Val Acc 57.240\n",
      "L 6 Epoch 191 Loss 0.005 Val Acc 57.880\n",
      "L 6 Epoch 192 Loss 0.005 Val Acc 57.840\n",
      "L 6 Epoch 193 Loss 0.008 Val Acc 57.780\n",
      "L 6 Epoch 194 Loss 0.008 Val Acc 57.740\n",
      "L 6 Epoch 195 Loss 0.011 Val Acc 57.700\n",
      "L 6 Epoch 196 Loss 0.004 Val Acc 57.840\n",
      "L 6 Epoch 197 Loss 0.013 Val Acc 57.280\n",
      "L 6 Epoch 198 Loss 0.009 Val Acc 57.420\n",
      "L 6 Epoch 199 Loss 0.012 Val Acc 57.640\n",
      "L 8 Epoch 0 Loss 6.694 Val Acc 10.000\n",
      "L 8 Epoch 1 Loss 6.995 Val Acc 10.000\n",
      "L 8 Epoch 2 Loss 3.062 Val Acc 9.320\n",
      "L 8 Epoch 3 Loss 2.558 Val Acc 13.200\n",
      "L 8 Epoch 4 Loss 2.388 Val Acc 15.780\n",
      "L 8 Epoch 5 Loss 2.256 Val Acc 15.820\n",
      "L 8 Epoch 6 Loss 2.182 Val Acc 17.840\n",
      "L 8 Epoch 7 Loss 2.112 Val Acc 20.260\n",
      "L 8 Epoch 8 Loss 2.095 Val Acc 20.860\n",
      "L 8 Epoch 9 Loss 2.046 Val Acc 22.640\n",
      "L 8 Epoch 10 Loss 2.005 Val Acc 18.600\n",
      "L 8 Epoch 11 Loss 1.984 Val Acc 17.200\n",
      "L 8 Epoch 12 Loss 1.947 Val Acc 26.700\n",
      "L 8 Epoch 13 Loss 1.921 Val Acc 23.280\n",
      "L 8 Epoch 14 Loss 1.838 Val Acc 28.020\n",
      "L 8 Epoch 15 Loss 1.803 Val Acc 26.960\n",
      "L 8 Epoch 16 Loss 1.768 Val Acc 24.600\n",
      "L 8 Epoch 17 Loss 1.717 Val Acc 27.780\n",
      "L 8 Epoch 18 Loss 1.688 Val Acc 32.160\n",
      "L 8 Epoch 19 Loss 1.685 Val Acc 33.120\n",
      "L 8 Epoch 20 Loss 1.611 Val Acc 34.860\n",
      "L 8 Epoch 21 Loss 1.668 Val Acc 33.740\n",
      "L 8 Epoch 22 Loss 1.644 Val Acc 35.540\n",
      "L 8 Epoch 23 Loss 1.564 Val Acc 37.680\n",
      "L 8 Epoch 24 Loss 1.507 Val Acc 35.480\n",
      "L 8 Epoch 25 Loss 1.523 Val Acc 38.960\n",
      "L 8 Epoch 26 Loss 1.476 Val Acc 34.340\n",
      "L 8 Epoch 27 Loss 1.472 Val Acc 35.280\n",
      "L 8 Epoch 28 Loss 1.502 Val Acc 36.740\n",
      "L 8 Epoch 29 Loss 1.458 Val Acc 35.880\n",
      "==> Saving model ...\n",
      "L 8 Epoch 30 Loss 1.403 Val Acc 40.960\n",
      "L 8 Epoch 31 Loss 1.384 Val Acc 40.320\n",
      "L 8 Epoch 32 Loss 1.367 Val Acc 39.660\n",
      "==> Saving model ...\n",
      "L 8 Epoch 33 Loss 1.320 Val Acc 41.420\n",
      "L 8 Epoch 34 Loss 1.278 Val Acc 40.520\n",
      "L 8 Epoch 35 Loss 1.311 Val Acc 40.900\n",
      "==> Saving model ...\n",
      "L 8 Epoch 36 Loss 1.278 Val Acc 43.420\n",
      "L 8 Epoch 37 Loss 1.216 Val Acc 40.000\n",
      "L 8 Epoch 38 Loss 1.247 Val Acc 38.680\n",
      "L 8 Epoch 39 Loss 1.231 Val Acc 42.880\n",
      "==> Saving model ...\n",
      "L 8 Epoch 40 Loss 1.214 Val Acc 44.000\n",
      "L 8 Epoch 41 Loss 1.100 Val Acc 43.300\n",
      "L 8 Epoch 42 Loss 1.090 Val Acc 41.320\n",
      "L 8 Epoch 43 Loss 1.176 Val Acc 41.320\n",
      "L 8 Epoch 44 Loss 1.056 Val Acc 40.080\n",
      "L 8 Epoch 45 Loss 1.089 Val Acc 43.660\n",
      "L 8 Epoch 46 Loss 1.015 Val Acc 42.560\n",
      "==> Saving model ...\n",
      "L 8 Epoch 47 Loss 1.097 Val Acc 44.060\n",
      "L 8 Epoch 48 Loss 0.992 Val Acc 43.100\n",
      "L 8 Epoch 49 Loss 0.966 Val Acc 44.020\n",
      "L 8 Epoch 50 Loss 0.883 Val Acc 43.500\n",
      "==> Saving model ...\n",
      "L 8 Epoch 51 Loss 0.812 Val Acc 45.220\n",
      "L 8 Epoch 52 Loss 0.863 Val Acc 39.460\n",
      "L 8 Epoch 53 Loss 0.970 Val Acc 45.020\n",
      "==> Saving model ...\n",
      "L 8 Epoch 54 Loss 0.827 Val Acc 47.000\n",
      "==> Saving model ...\n",
      "L 8 Epoch 55 Loss 0.799 Val Acc 48.220\n",
      "L 8 Epoch 56 Loss 0.766 Val Acc 45.760\n",
      "L 8 Epoch 57 Loss 0.652 Val Acc 46.140\n",
      "==> Saving model ...\n",
      "L 8 Epoch 58 Loss 0.627 Val Acc 49.700\n",
      "L 8 Epoch 59 Loss 0.656 Val Acc 46.720\n",
      "L 8 Epoch 60 Loss 0.698 Val Acc 48.420\n",
      "L 8 Epoch 61 Loss 0.661 Val Acc 45.500\n",
      "L 8 Epoch 62 Loss 0.574 Val Acc 47.960\n",
      "L 8 Epoch 63 Loss 0.645 Val Acc 48.080\n",
      "==> Saving model ...\n",
      "L 8 Epoch 64 Loss 0.586 Val Acc 50.680\n",
      "L 8 Epoch 65 Loss 0.543 Val Acc 48.620\n",
      "==> Saving model ...\n",
      "L 8 Epoch 66 Loss 0.474 Val Acc 51.080\n",
      "==> Saving model ...\n",
      "L 8 Epoch 67 Loss 0.395 Val Acc 52.360\n",
      "L 8 Epoch 68 Loss 0.511 Val Acc 49.980\n",
      "L 8 Epoch 69 Loss 0.446 Val Acc 45.220\n",
      "L 8 Epoch 70 Loss 0.479 Val Acc 50.280\n",
      "L 8 Epoch 71 Loss 0.476 Val Acc 46.280\n",
      "L 8 Epoch 72 Loss 0.352 Val Acc 49.760\n",
      "L 8 Epoch 73 Loss 0.381 Val Acc 51.880\n",
      "L 8 Epoch 74 Loss 0.344 Val Acc 49.880\n",
      "L 8 Epoch 75 Loss 0.382 Val Acc 50.560\n",
      "L 8 Epoch 76 Loss 0.416 Val Acc 49.620\n",
      "L 8 Epoch 77 Loss 0.313 Val Acc 49.500\n",
      "L 8 Epoch 78 Loss 0.294 Val Acc 50.380\n",
      "L 8 Epoch 79 Loss 0.349 Val Acc 52.300\n",
      "L 8 Epoch 80 Loss 0.354 Val Acc 50.800\n",
      "L 8 Epoch 81 Loss 0.243 Val Acc 51.760\n",
      "L 8 Epoch 82 Loss 0.268 Val Acc 49.140\n",
      "L 8 Epoch 83 Loss 0.299 Val Acc 49.240\n",
      "L 8 Epoch 84 Loss 0.202 Val Acc 51.880\n",
      "==> Saving model ...\n",
      "L 8 Epoch 85 Loss 0.269 Val Acc 52.940\n",
      "L 8 Epoch 86 Loss 0.258 Val Acc 52.440\n",
      "L 8 Epoch 87 Loss 0.219 Val Acc 48.740\n",
      "L 8 Epoch 88 Loss 0.302 Val Acc 49.840\n",
      "==> Saving model ...\n",
      "L 8 Epoch 89 Loss 0.137 Val Acc 54.000\n",
      "L 8 Epoch 90 Loss 0.206 Val Acc 53.600\n",
      "L 8 Epoch 91 Loss 0.266 Val Acc 50.020\n",
      "L 8 Epoch 92 Loss 0.193 Val Acc 52.120\n",
      "L 8 Epoch 93 Loss 0.180 Val Acc 53.560\n",
      "L 8 Epoch 94 Loss 0.141 Val Acc 52.240\n",
      "L 8 Epoch 95 Loss 0.126 Val Acc 50.180\n",
      "L 8 Epoch 96 Loss 0.224 Val Acc 53.200\n",
      "L 8 Epoch 97 Loss 0.189 Val Acc 51.320\n",
      "L 8 Epoch 98 Loss 0.131 Val Acc 51.560\n",
      "L 8 Epoch 99 Loss 0.221 Val Acc 51.000\n",
      "L 8 Epoch 100 Loss 0.089 Val Acc 53.180\n",
      "L 8 Epoch 101 Loss 0.095 Val Acc 52.500\n",
      "L 8 Epoch 102 Loss 0.069 Val Acc 51.800\n",
      "L 8 Epoch 103 Loss 0.123 Val Acc 53.360\n",
      "L 8 Epoch 104 Loss 0.144 Val Acc 51.880\n",
      "L 8 Epoch 105 Loss 0.044 Val Acc 51.540\n",
      "L 8 Epoch 106 Loss 0.124 Val Acc 53.980\n",
      "L 8 Epoch 107 Loss 0.069 Val Acc 53.660\n",
      "L 8 Epoch 108 Loss 0.069 Val Acc 53.780\n",
      "L 8 Epoch 109 Loss 0.089 Val Acc 52.480\n",
      "L 8 Epoch 110 Loss 0.150 Val Acc 53.980\n",
      "L 8 Epoch 111 Loss 0.077 Val Acc 53.560\n",
      "==> Saving model ...\n",
      "L 8 Epoch 112 Loss 0.067 Val Acc 54.520\n",
      "L 8 Epoch 113 Loss 0.077 Val Acc 52.380\n",
      "L 8 Epoch 114 Loss 0.088 Val Acc 53.320\n",
      "L 8 Epoch 115 Loss 0.087 Val Acc 52.580\n",
      "L 8 Epoch 116 Loss 0.068 Val Acc 54.380\n",
      "L 8 Epoch 117 Loss 0.049 Val Acc 53.920\n",
      "L 8 Epoch 118 Loss 0.076 Val Acc 52.920\n",
      "L 8 Epoch 119 Loss 0.056 Val Acc 54.280\n",
      "L 8 Epoch 120 Loss 0.110 Val Acc 52.620\n",
      "==> Saving model ...\n",
      "L 8 Epoch 121 Loss 0.078 Val Acc 55.020\n",
      "L 8 Epoch 122 Loss 0.084 Val Acc 54.320\n",
      "L 8 Epoch 123 Loss 0.082 Val Acc 53.820\n",
      "L 8 Epoch 124 Loss 0.059 Val Acc 54.420\n",
      "L 8 Epoch 125 Loss 0.086 Val Acc 53.620\n",
      "L 8 Epoch 126 Loss 0.037 Val Acc 54.920\n",
      "L 8 Epoch 127 Loss 0.065 Val Acc 54.520\n",
      "L 8 Epoch 128 Loss 0.027 Val Acc 54.640\n",
      "L 8 Epoch 129 Loss 0.050 Val Acc 54.460\n",
      "L 8 Epoch 130 Loss 0.043 Val Acc 53.800\n",
      "L 8 Epoch 131 Loss 0.057 Val Acc 53.800\n",
      "L 8 Epoch 132 Loss 0.089 Val Acc 53.800\n",
      "L 8 Epoch 133 Loss 0.037 Val Acc 54.380\n",
      "L 8 Epoch 134 Loss 0.035 Val Acc 54.460\n",
      "L 8 Epoch 135 Loss 0.067 Val Acc 54.540\n",
      "L 8 Epoch 136 Loss 0.021 Val Acc 54.620\n",
      "L 8 Epoch 137 Loss 0.076 Val Acc 55.020\n",
      "L 8 Epoch 138 Loss 0.026 Val Acc 54.280\n",
      "L 8 Epoch 139 Loss 0.062 Val Acc 54.700\n",
      "L 8 Epoch 140 Loss 0.014 Val Acc 54.880\n",
      "L 8 Epoch 141 Loss 0.052 Val Acc 54.520\n",
      "L 8 Epoch 142 Loss 0.037 Val Acc 54.860\n",
      "L 8 Epoch 143 Loss 0.037 Val Acc 54.780\n",
      "L 8 Epoch 144 Loss 0.049 Val Acc 54.940\n",
      "L 8 Epoch 145 Loss 0.043 Val Acc 54.740\n",
      "L 8 Epoch 146 Loss 0.048 Val Acc 54.940\n",
      "==> Saving model ...\n",
      "L 8 Epoch 147 Loss 0.045 Val Acc 55.200\n",
      "L 8 Epoch 148 Loss 0.044 Val Acc 54.760\n",
      "L 8 Epoch 149 Loss 0.039 Val Acc 54.560\n",
      "L 8 Epoch 150 Loss 0.051 Val Acc 54.680\n",
      "L 8 Epoch 151 Loss 0.036 Val Acc 54.660\n",
      "L 8 Epoch 152 Loss 0.044 Val Acc 54.620\n",
      "L 8 Epoch 153 Loss 0.024 Val Acc 55.040\n",
      "==> Saving model ...\n",
      "L 8 Epoch 154 Loss 0.025 Val Acc 55.320\n",
      "L 8 Epoch 155 Loss 0.039 Val Acc 54.840\n",
      "==> Saving model ...\n",
      "L 8 Epoch 156 Loss 0.010 Val Acc 55.780\n",
      "L 8 Epoch 157 Loss 0.030 Val Acc 55.360\n",
      "L 8 Epoch 158 Loss 0.025 Val Acc 55.240\n",
      "L 8 Epoch 159 Loss 0.037 Val Acc 55.440\n",
      "L 8 Epoch 160 Loss 0.021 Val Acc 55.420\n",
      "L 8 Epoch 161 Loss 0.028 Val Acc 55.340\n",
      "L 8 Epoch 162 Loss 0.012 Val Acc 55.140\n",
      "L 8 Epoch 163 Loss 0.051 Val Acc 55.560\n",
      "L 8 Epoch 164 Loss 0.030 Val Acc 55.060\n",
      "L 8 Epoch 165 Loss 0.013 Val Acc 55.380\n",
      "L 8 Epoch 166 Loss 0.017 Val Acc 55.460\n",
      "L 8 Epoch 167 Loss 0.007 Val Acc 55.600\n",
      "L 8 Epoch 168 Loss 0.037 Val Acc 55.480\n",
      "L 8 Epoch 169 Loss 0.030 Val Acc 55.440\n",
      "L 8 Epoch 170 Loss 0.024 Val Acc 55.380\n",
      "L 8 Epoch 171 Loss 0.020 Val Acc 55.660\n",
      "==> Saving model ...\n",
      "L 8 Epoch 172 Loss 0.005 Val Acc 55.980\n",
      "L 8 Epoch 173 Loss 0.039 Val Acc 55.520\n",
      "L 8 Epoch 174 Loss 0.025 Val Acc 55.640\n",
      "L 8 Epoch 175 Loss 0.017 Val Acc 55.840\n",
      "L 8 Epoch 176 Loss 0.011 Val Acc 55.720\n",
      "L 8 Epoch 177 Loss 0.014 Val Acc 55.740\n",
      "L 8 Epoch 178 Loss 0.035 Val Acc 55.640\n",
      "L 8 Epoch 179 Loss 0.015 Val Acc 55.680\n",
      "L 8 Epoch 180 Loss 0.028 Val Acc 55.700\n",
      "L 8 Epoch 181 Loss 0.015 Val Acc 55.920\n",
      "L 8 Epoch 182 Loss 0.019 Val Acc 55.900\n",
      "L 8 Epoch 183 Loss 0.012 Val Acc 55.840\n",
      "L 8 Epoch 184 Loss 0.018 Val Acc 55.860\n",
      "L 8 Epoch 185 Loss 0.019 Val Acc 55.820\n",
      "L 8 Epoch 186 Loss 0.015 Val Acc 55.980\n",
      "L 8 Epoch 187 Loss 0.012 Val Acc 55.940\n",
      "==> Saving model ...\n",
      "L 8 Epoch 188 Loss 0.009 Val Acc 56.080\n",
      "L 8 Epoch 189 Loss 0.013 Val Acc 55.940\n",
      "L 8 Epoch 190 Loss 0.025 Val Acc 55.920\n",
      "L 8 Epoch 191 Loss 0.028 Val Acc 55.700\n",
      "L 8 Epoch 192 Loss 0.023 Val Acc 55.900\n",
      "L 8 Epoch 193 Loss 0.018 Val Acc 55.840\n",
      "L 8 Epoch 194 Loss 0.026 Val Acc 55.600\n",
      "L 8 Epoch 195 Loss 0.025 Val Acc 55.740\n",
      "L 8 Epoch 196 Loss 0.019 Val Acc 55.700\n",
      "L 8 Epoch 197 Loss 0.016 Val Acc 55.940\n",
      "L 8 Epoch 198 Loss 0.009 Val Acc 55.940\n",
      "L 8 Epoch 199 Loss 0.013 Val Acc 56.020\n",
      "L 10 Epoch 0 Loss 4.607 Val Acc 10.000\n",
      "L 10 Epoch 1 Loss 3.271 Val Acc 9.920\n",
      "L 10 Epoch 2 Loss 2.590 Val Acc 10.420\n",
      "L 10 Epoch 3 Loss 2.542 Val Acc 13.860\n",
      "L 10 Epoch 4 Loss 2.341 Val Acc 17.300\n",
      "L 10 Epoch 5 Loss 2.206 Val Acc 18.640\n",
      "L 10 Epoch 6 Loss 2.064 Val Acc 19.760\n",
      "L 10 Epoch 7 Loss 2.120 Val Acc 16.580\n",
      "L 10 Epoch 8 Loss 2.002 Val Acc 26.320\n",
      "L 10 Epoch 9 Loss 1.918 Val Acc 30.520\n",
      "L 10 Epoch 10 Loss 1.774 Val Acc 32.060\n",
      "L 10 Epoch 11 Loss 1.719 Val Acc 31.280\n",
      "L 10 Epoch 12 Loss 1.728 Val Acc 26.420\n",
      "L 10 Epoch 13 Loss 1.634 Val Acc 35.280\n",
      "L 10 Epoch 14 Loss 1.587 Val Acc 35.780\n",
      "L 10 Epoch 15 Loss 1.583 Val Acc 35.780\n",
      "L 10 Epoch 16 Loss 1.560 Val Acc 38.480\n",
      "L 10 Epoch 17 Loss 1.576 Val Acc 37.060\n",
      "L 10 Epoch 18 Loss 1.515 Val Acc 39.100\n",
      "L 10 Epoch 19 Loss 1.500 Val Acc 39.320\n",
      "L 10 Epoch 20 Loss 1.450 Val Acc 37.900\n",
      "==> Saving model ...\n",
      "L 10 Epoch 21 Loss 1.424 Val Acc 41.760\n",
      "==> Saving model ...\n",
      "L 10 Epoch 22 Loss 1.386 Val Acc 42.480\n",
      "L 10 Epoch 23 Loss 1.328 Val Acc 40.880\n",
      "==> Saving model ...\n",
      "L 10 Epoch 24 Loss 1.322 Val Acc 43.140\n",
      "==> Saving model ...\n",
      "L 10 Epoch 25 Loss 1.247 Val Acc 43.420\n",
      "L 10 Epoch 26 Loss 1.194 Val Acc 38.600\n",
      "==> Saving model ...\n",
      "L 10 Epoch 27 Loss 1.218 Val Acc 44.480\n",
      "==> Saving model ...\n",
      "L 10 Epoch 28 Loss 1.167 Val Acc 45.120\n",
      "==> Saving model ...\n",
      "L 10 Epoch 29 Loss 1.147 Val Acc 45.300\n",
      "==> Saving model ...\n",
      "L 10 Epoch 30 Loss 1.029 Val Acc 47.560\n",
      "L 10 Epoch 31 Loss 1.018 Val Acc 45.760\n",
      "L 10 Epoch 32 Loss 1.031 Val Acc 47.100\n",
      "L 10 Epoch 33 Loss 0.870 Val Acc 46.260\n",
      "L 10 Epoch 34 Loss 1.009 Val Acc 44.320\n",
      "L 10 Epoch 35 Loss 1.003 Val Acc 46.600\n",
      "==> Saving model ...\n",
      "L 10 Epoch 36 Loss 0.872 Val Acc 48.340\n",
      "L 10 Epoch 37 Loss 0.795 Val Acc 48.340\n",
      "L 10 Epoch 38 Loss 0.807 Val Acc 44.020\n",
      "L 10 Epoch 39 Loss 0.751 Val Acc 48.240\n",
      "L 10 Epoch 40 Loss 0.774 Val Acc 45.820\n",
      "==> Saving model ...\n",
      "L 10 Epoch 41 Loss 0.738 Val Acc 48.460\n",
      "==> Saving model ...\n",
      "L 10 Epoch 42 Loss 0.782 Val Acc 49.460\n",
      "L 10 Epoch 43 Loss 0.706 Val Acc 47.840\n",
      "L 10 Epoch 44 Loss 0.490 Val Acc 48.520\n",
      "==> Saving model ...\n",
      "L 10 Epoch 45 Loss 0.540 Val Acc 51.980\n",
      "L 10 Epoch 46 Loss 0.645 Val Acc 43.660\n",
      "L 10 Epoch 47 Loss 0.649 Val Acc 47.720\n",
      "L 10 Epoch 48 Loss 0.597 Val Acc 44.300\n",
      "L 10 Epoch 49 Loss 0.591 Val Acc 48.140\n",
      "L 10 Epoch 50 Loss 0.562 Val Acc 49.860\n",
      "==> Saving model ...\n",
      "L 10 Epoch 51 Loss 0.411 Val Acc 52.160\n",
      "L 10 Epoch 52 Loss 0.405 Val Acc 48.380\n",
      "==> Saving model ...\n",
      "L 10 Epoch 53 Loss 0.404 Val Acc 52.520\n",
      "L 10 Epoch 54 Loss 0.413 Val Acc 49.780\n",
      "L 10 Epoch 55 Loss 0.312 Val Acc 51.220\n",
      "L 10 Epoch 56 Loss 0.250 Val Acc 50.560\n",
      "L 10 Epoch 57 Loss 0.271 Val Acc 49.800\n",
      "L 10 Epoch 58 Loss 0.249 Val Acc 48.680\n",
      "L 10 Epoch 59 Loss 0.308 Val Acc 52.080\n",
      "L 10 Epoch 60 Loss 0.311 Val Acc 50.260\n",
      "L 10 Epoch 61 Loss 0.287 Val Acc 46.940\n",
      "L 10 Epoch 62 Loss 0.256 Val Acc 51.520\n",
      "L 10 Epoch 63 Loss 0.257 Val Acc 49.640\n",
      "L 10 Epoch 64 Loss 0.294 Val Acc 51.020\n",
      "L 10 Epoch 65 Loss 0.303 Val Acc 49.860\n",
      "==> Saving model ...\n",
      "L 10 Epoch 66 Loss 0.209 Val Acc 52.940\n",
      "L 10 Epoch 67 Loss 0.271 Val Acc 51.980\n",
      "L 10 Epoch 68 Loss 0.213 Val Acc 51.140\n",
      "==> Saving model ...\n",
      "L 10 Epoch 69 Loss 0.155 Val Acc 53.740\n",
      "L 10 Epoch 70 Loss 0.206 Val Acc 52.160\n",
      "==> Saving model ...\n",
      "L 10 Epoch 71 Loss 0.137 Val Acc 54.300\n",
      "L 10 Epoch 72 Loss 0.189 Val Acc 52.320\n",
      "L 10 Epoch 73 Loss 0.089 Val Acc 54.060\n",
      "L 10 Epoch 74 Loss 0.093 Val Acc 54.300\n",
      "L 10 Epoch 75 Loss 0.126 Val Acc 53.660\n",
      "L 10 Epoch 76 Loss 0.137 Val Acc 51.620\n",
      "L 10 Epoch 77 Loss 0.112 Val Acc 52.460\n",
      "L 10 Epoch 78 Loss 0.128 Val Acc 52.600\n",
      "L 10 Epoch 79 Loss 0.168 Val Acc 51.980\n",
      "L 10 Epoch 80 Loss 0.141 Val Acc 53.860\n",
      "L 10 Epoch 81 Loss 0.096 Val Acc 51.980\n",
      "L 10 Epoch 82 Loss 0.134 Val Acc 53.580\n",
      "L 10 Epoch 83 Loss 0.179 Val Acc 48.900\n",
      "L 10 Epoch 84 Loss 0.095 Val Acc 52.720\n",
      "L 10 Epoch 85 Loss 0.123 Val Acc 53.160\n",
      "L 10 Epoch 86 Loss 0.117 Val Acc 53.600\n",
      "L 10 Epoch 87 Loss 0.200 Val Acc 53.620\n",
      "L 10 Epoch 88 Loss 0.140 Val Acc 52.460\n",
      "L 10 Epoch 89 Loss 0.135 Val Acc 51.820\n",
      "L 10 Epoch 90 Loss 0.118 Val Acc 54.100\n",
      "==> Saving model ...\n",
      "L 10 Epoch 91 Loss 0.125 Val Acc 55.600\n",
      "L 10 Epoch 92 Loss 0.096 Val Acc 54.380\n",
      "L 10 Epoch 93 Loss 0.145 Val Acc 52.240\n",
      "L 10 Epoch 94 Loss 0.158 Val Acc 53.000\n",
      "L 10 Epoch 95 Loss 0.092 Val Acc 54.180\n",
      "L 10 Epoch 96 Loss 0.085 Val Acc 54.040\n",
      "L 10 Epoch 97 Loss 0.079 Val Acc 54.920\n",
      "L 10 Epoch 98 Loss 0.035 Val Acc 55.380\n",
      "L 10 Epoch 99 Loss 0.091 Val Acc 55.400\n",
      "L 10 Epoch 100 Loss 0.042 Val Acc 54.640\n",
      "L 10 Epoch 101 Loss 0.096 Val Acc 53.120\n",
      "L 10 Epoch 102 Loss 0.078 Val Acc 54.980\n",
      "==> Saving model ...\n",
      "L 10 Epoch 103 Loss 0.077 Val Acc 55.760\n",
      "L 10 Epoch 104 Loss 0.056 Val Acc 53.820\n",
      "L 10 Epoch 105 Loss 0.077 Val Acc 55.100\n",
      "L 10 Epoch 106 Loss 0.061 Val Acc 55.240\n",
      "L 10 Epoch 107 Loss 0.053 Val Acc 53.200\n",
      "L 10 Epoch 108 Loss 0.041 Val Acc 54.600\n",
      "L 10 Epoch 109 Loss 0.073 Val Acc 55.100\n",
      "L 10 Epoch 110 Loss 0.060 Val Acc 55.600\n",
      "L 10 Epoch 111 Loss 0.025 Val Acc 55.660\n",
      "L 10 Epoch 112 Loss 0.044 Val Acc 54.900\n",
      "L 10 Epoch 113 Loss 0.049 Val Acc 54.040\n",
      "==> Saving model ...\n",
      "L 10 Epoch 114 Loss 0.037 Val Acc 56.260\n",
      "==> Saving model ...\n",
      "L 10 Epoch 115 Loss 0.061 Val Acc 56.400\n",
      "L 10 Epoch 116 Loss 0.045 Val Acc 55.320\n",
      "L 10 Epoch 117 Loss 0.058 Val Acc 56.080\n",
      "L 10 Epoch 118 Loss 0.064 Val Acc 56.140\n",
      "L 10 Epoch 119 Loss 0.052 Val Acc 56.040\n",
      "==> Saving model ...\n",
      "L 10 Epoch 120 Loss 0.040 Val Acc 56.840\n",
      "L 10 Epoch 121 Loss 0.041 Val Acc 56.500\n",
      "L 10 Epoch 122 Loss 0.051 Val Acc 55.780\n",
      "L 10 Epoch 123 Loss 0.033 Val Acc 55.800\n",
      "L 10 Epoch 124 Loss 0.012 Val Acc 56.300\n",
      "L 10 Epoch 125 Loss 0.026 Val Acc 56.520\n",
      "L 10 Epoch 126 Loss 0.037 Val Acc 56.460\n",
      "==> Saving model ...\n",
      "L 10 Epoch 127 Loss 0.018 Val Acc 56.960\n",
      "==> Saving model ...\n",
      "L 10 Epoch 128 Loss 0.043 Val Acc 57.000\n",
      "L 10 Epoch 129 Loss 0.041 Val Acc 56.200\n",
      "L 10 Epoch 130 Loss 0.020 Val Acc 56.420\n",
      "L 10 Epoch 131 Loss 0.025 Val Acc 56.620\n",
      "==> Saving model ...\n",
      "L 10 Epoch 132 Loss 0.017 Val Acc 57.320\n",
      "L 10 Epoch 133 Loss 0.017 Val Acc 57.160\n",
      "L 10 Epoch 134 Loss 0.037 Val Acc 56.300\n",
      "L 10 Epoch 135 Loss 0.031 Val Acc 55.760\n",
      "L 10 Epoch 136 Loss 0.018 Val Acc 56.440\n",
      "L 10 Epoch 137 Loss 0.029 Val Acc 56.660\n",
      "L 10 Epoch 138 Loss 0.025 Val Acc 57.000\n",
      "==> Saving model ...\n",
      "L 10 Epoch 139 Loss 0.028 Val Acc 57.500\n",
      "L 10 Epoch 140 Loss 0.033 Val Acc 57.100\n",
      "L 10 Epoch 141 Loss 0.022 Val Acc 57.100\n",
      "L 10 Epoch 142 Loss 0.024 Val Acc 57.220\n",
      "L 10 Epoch 143 Loss 0.027 Val Acc 57.300\n",
      "L 10 Epoch 144 Loss 0.018 Val Acc 57.240\n",
      "==> Saving model ...\n",
      "L 10 Epoch 145 Loss 0.027 Val Acc 57.640\n",
      "L 10 Epoch 146 Loss 0.014 Val Acc 56.660\n",
      "L 10 Epoch 147 Loss 0.008 Val Acc 56.860\n",
      "L 10 Epoch 148 Loss 0.012 Val Acc 56.820\n",
      "L 10 Epoch 149 Loss 0.018 Val Acc 57.060\n",
      "L 10 Epoch 150 Loss 0.015 Val Acc 57.360\n",
      "L 10 Epoch 151 Loss 0.012 Val Acc 57.300\n",
      "L 10 Epoch 152 Loss 0.011 Val Acc 57.420\n",
      "==> Saving model ...\n",
      "L 10 Epoch 153 Loss 0.013 Val Acc 57.700\n",
      "==> Saving model ...\n",
      "L 10 Epoch 154 Loss 0.010 Val Acc 57.740\n",
      "L 10 Epoch 155 Loss 0.022 Val Acc 57.360\n",
      "L 10 Epoch 156 Loss 0.013 Val Acc 57.600\n",
      "==> Saving model ...\n",
      "L 10 Epoch 157 Loss 0.009 Val Acc 57.880\n",
      "L 10 Epoch 158 Loss 0.010 Val Acc 57.840\n",
      "L 10 Epoch 159 Loss 0.009 Val Acc 57.720\n",
      "L 10 Epoch 160 Loss 0.015 Val Acc 57.600\n",
      "L 10 Epoch 161 Loss 0.019 Val Acc 57.280\n",
      "L 10 Epoch 162 Loss 0.014 Val Acc 57.400\n",
      "L 10 Epoch 163 Loss 0.013 Val Acc 57.720\n",
      "L 10 Epoch 164 Loss 0.020 Val Acc 57.520\n",
      "L 10 Epoch 165 Loss 0.030 Val Acc 57.540\n",
      "L 10 Epoch 166 Loss 0.011 Val Acc 57.400\n",
      "L 10 Epoch 167 Loss 0.015 Val Acc 57.440\n",
      "L 10 Epoch 168 Loss 0.020 Val Acc 57.520\n",
      "L 10 Epoch 169 Loss 0.014 Val Acc 57.180\n",
      "==> Saving model ...\n",
      "L 10 Epoch 170 Loss 0.003 Val Acc 58.140\n",
      "L 10 Epoch 171 Loss 0.015 Val Acc 57.880\n",
      "L 10 Epoch 172 Loss 0.009 Val Acc 57.700\n",
      "L 10 Epoch 173 Loss 0.009 Val Acc 57.940\n",
      "L 10 Epoch 174 Loss 0.010 Val Acc 58.100\n",
      "L 10 Epoch 175 Loss 0.020 Val Acc 57.480\n",
      "L 10 Epoch 176 Loss 0.012 Val Acc 57.280\n",
      "L 10 Epoch 177 Loss 0.014 Val Acc 57.400\n",
      "L 10 Epoch 178 Loss 0.003 Val Acc 57.960\n",
      "L 10 Epoch 179 Loss 0.011 Val Acc 58.040\n",
      "==> Saving model ...\n",
      "L 10 Epoch 180 Loss 0.004 Val Acc 58.420\n",
      "L 10 Epoch 181 Loss 0.021 Val Acc 58.120\n",
      "L 10 Epoch 182 Loss 0.015 Val Acc 57.920\n",
      "L 10 Epoch 183 Loss 0.012 Val Acc 58.140\n",
      "==> Saving model ...\n",
      "L 10 Epoch 184 Loss 0.004 Val Acc 58.560\n",
      "==> Saving model ...\n",
      "L 10 Epoch 185 Loss 0.010 Val Acc 58.580\n",
      "L 10 Epoch 186 Loss 0.008 Val Acc 58.340\n",
      "L 10 Epoch 187 Loss 0.011 Val Acc 58.080\n",
      "L 10 Epoch 188 Loss 0.012 Val Acc 57.940\n",
      "L 10 Epoch 189 Loss 0.013 Val Acc 58.060\n",
      "L 10 Epoch 190 Loss 0.008 Val Acc 58.060\n",
      "L 10 Epoch 191 Loss 0.010 Val Acc 57.540\n",
      "L 10 Epoch 192 Loss 0.014 Val Acc 57.900\n",
      "L 10 Epoch 193 Loss 0.013 Val Acc 57.920\n",
      "L 10 Epoch 194 Loss 0.020 Val Acc 57.960\n",
      "L 10 Epoch 195 Loss 0.011 Val Acc 58.360\n",
      "L 10 Epoch 196 Loss 0.017 Val Acc 58.060\n",
      "L 10 Epoch 197 Loss 0.014 Val Acc 58.020\n",
      "L 10 Epoch 198 Loss 0.018 Val Acc 58.200\n",
      "L 10 Epoch 199 Loss 0.014 Val Acc 57.620\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "mean = torch.tensor([0.4914, 0.4822, 0.4465]).to(device)\n",
    "std = torch.tensor([0.2023, 0.1994, 0.2010]).to(device)\n",
    "normalize = K.Normalize(mean=mean, std=std)\n",
    "# define a sequence of augmentations\n",
    "aug_list = AugmentationSequential(\n",
    "    K.RandomHorizontalFlip(p=0.5),\n",
    "    K.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.2),\n",
    "    K.RandomResizedCrop(size=(32,32), scale=(0.7, 1.0), p=0.5),\n",
    "    normalize,\n",
    "    same_on_batch=False\n",
    ").to(device)\n",
    "\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)\n",
    "stats_list = defaultdict(list)\n",
    "for N in range(0, N_TIMES):\n",
    "    for L in Ls:\n",
    "        model = ScatResNet18(L).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=T_max)\n",
    "        \n",
    "        stats = {\n",
    "            'total_training_time': 0,\n",
    "            'loss': [],\n",
    "            'time_per_epoch': [],\n",
    "            'total_time_per_epoch': [],\n",
    "            'val_accuracy': [],\n",
    "            'max_val_accuracy': 0,\n",
    "            'allocated_memory': [], # Memory currently used by Tensors\n",
    "            'reserved_memory': [], # Memory held by the PyTorch caching allocator\n",
    "        }\n",
    "        \n",
    "        start_time = time.time()\n",
    "        for epoch in range(n_epochs):\n",
    "            model.train()\n",
    "            iteration_losses = []\n",
    "            epoch_start_time = time.time()\n",
    "            for inputs, targets in trainloader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "        \n",
    "                inputs = aug_list(inputs)\n",
    "        \n",
    "                outputs = model(inputs)\n",
    "        \n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                iteration_losses.append(loss.item())\n",
    "        \n",
    "            scheduler.step()\n",
    "            epoch_end_time = time.time()\n",
    "        \n",
    "            model.eval()\n",
    "            val_accuracy = calculate_accuracy(model, valloader, device)\n",
    "        \n",
    "            # Track stats\n",
    "            if (epoch % 1) == 0:\n",
    "                stats['loss'].append(\n",
    "                    np.mean(iteration_losses)\n",
    "                )\n",
    "                stats['val_accuracy'].append(\n",
    "                    val_accuracy\n",
    "                )\n",
    "                stats['allocated_memory'].append(torch.cuda.memory_allocated())\n",
    "                stats['reserved_memory'].append(torch.cuda.memory_reserved())\n",
    "                stats['time_per_epoch'].append(epoch_end_time - epoch_start_time)\n",
    "                stats['total_time_per_epoch'].append(time.time() - start_time)\n",
    "        \n",
    "            # Store best model\n",
    "            if (val_accuracy > stats['max_val_accuracy']):\n",
    "                if (val_accuracy > val_accuracy_storing_threshold):\n",
    "                    stats['max_val_accuracy'] = val_accuracy\n",
    "                    print('==> Saving model ...')\n",
    "                    state = {\n",
    "                        'net': model.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'acc':val_accuracy\n",
    "                    }\n",
    "                    save_path = checkpoint_dir / f\"{MODEL_NAME}_L{L}_N{N}_max_acc.pth\"\n",
    "                    torch.save(state, save_path)\n",
    "        \n",
    "            if DEBUG:\n",
    "                print('==> Saving model ... DEBUG')\n",
    "                state = {\n",
    "                    'net': model.state_dict(),\n",
    "                    'epoch': epoch,\n",
    "                    'acc':val_accuracy\n",
    "                }\n",
    "                save_path = checkpoint_dir / f\"{MODEL_NAME}_L{L}_N{N}_max_acc.pth\"\n",
    "                torch.save(state, save_path)\n",
    "                \n",
    "            # Print progress\n",
    "            if (epoch % print_progress_every) == 0:\n",
    "                print(f\"L {L} Epoch {epoch} Loss {stats['loss'][-1]:.3f} Val Acc {stats['val_accuracy'][-1]:.3f}\")\n",
    "        \n",
    "        stats_list[L].append(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f237f53e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T23:15:35.187381Z",
     "iopub.status.busy": "2025-12-30T23:15:35.187101Z",
     "iopub.status.idle": "2025-12-30T23:17:43.465212Z",
     "shell.execute_reply": "2025-12-30T23:17:43.464439Z"
    },
    "papermill": {
     "duration": 128.610733,
     "end_time": "2025-12-30T23:17:43.633289",
     "exception": false,
     "start_time": "2025-12-30T23:15:35.022556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test accuracy  for L 4 N 0 is: 55.280\n",
      "Final val accuracy   for L 4 N 0 is: 56.680\n",
      "Final train accuracy for L 4 N 0 is: 100.000\n",
      "Final test accuracy  for L 4 N 1 is: 55.090\n",
      "Final val accuracy   for L 4 N 1 is: 54.940\n",
      "Final train accuracy for L 4 N 1 is: 100.000\n",
      "Final test accuracy  for L 4 N 2 is: 55.270\n",
      "Final val accuracy   for L 4 N 2 is: 54.760\n",
      "Final train accuracy for L 4 N 2 is: 100.000\n",
      "Final test accuracy  for L 4 N 3 is: 56.070\n",
      "Final val accuracy   for L 4 N 3 is: 55.900\n",
      "Final train accuracy for L 4 N 3 is: 100.000\n",
      "Final test accuracy  for L 4 N 4 is: 56.930\n",
      "Final val accuracy   for L 4 N 4 is: 57.320\n",
      "Final train accuracy for L 4 N 4 is: 100.000\n",
      "Final test accuracy  for L 4 N 5 is: 55.270\n",
      "Final val accuracy   for L 4 N 5 is: 54.460\n",
      "Final train accuracy for L 4 N 5 is: 100.000\n",
      "Final test accuracy  for L 6 N 0 is: 59.230\n",
      "Final val accuracy   for L 6 N 0 is: 58.980\n",
      "Final train accuracy for L 6 N 0 is: 100.000\n",
      "Final test accuracy  for L 6 N 1 is: 53.890\n",
      "Final val accuracy   for L 6 N 1 is: 54.100\n",
      "Final train accuracy for L 6 N 1 is: 100.000\n",
      "Final test accuracy  for L 6 N 2 is: 53.190\n",
      "Final val accuracy   for L 6 N 2 is: 52.500\n",
      "Final train accuracy for L 6 N 2 is: 100.000\n",
      "Final test accuracy  for L 6 N 3 is: 55.250\n",
      "Final val accuracy   for L 6 N 3 is: 55.320\n",
      "Final train accuracy for L 6 N 3 is: 100.000\n",
      "Final test accuracy  for L 6 N 4 is: 56.860\n",
      "Final val accuracy   for L 6 N 4 is: 56.420\n",
      "Final train accuracy for L 6 N 4 is: 100.000\n",
      "Final test accuracy  for L 6 N 5 is: 58.120\n",
      "Final val accuracy   for L 6 N 5 is: 58.140\n",
      "Final train accuracy for L 6 N 5 is: 100.000\n",
      "Final test accuracy  for L 8 N 0 is: 57.550\n",
      "Final val accuracy   for L 8 N 0 is: 57.460\n",
      "Final train accuracy for L 8 N 0 is: 100.000\n",
      "Final test accuracy  for L 8 N 1 is: 58.660\n",
      "Final val accuracy   for L 8 N 1 is: 59.780\n",
      "Final train accuracy for L 8 N 1 is: 100.000\n",
      "Final test accuracy  for L 8 N 2 is: 58.540\n",
      "Final val accuracy   for L 8 N 2 is: 58.120\n",
      "Final train accuracy for L 8 N 2 is: 100.000\n",
      "Final test accuracy  for L 8 N 3 is: 57.970\n",
      "Final val accuracy   for L 8 N 3 is: 57.900\n",
      "Final train accuracy for L 8 N 3 is: 100.000\n",
      "Final test accuracy  for L 8 N 4 is: 59.750\n",
      "Final val accuracy   for L 8 N 4 is: 59.540\n",
      "Final train accuracy for L 8 N 4 is: 100.000\n",
      "Final test accuracy  for L 8 N 5 is: 56.580\n",
      "Final val accuracy   for L 8 N 5 is: 56.080\n",
      "Final train accuracy for L 8 N 5 is: 100.000\n",
      "Final test accuracy  for L 10 N 0 is: 58.730\n",
      "Final val accuracy   for L 10 N 0 is: 58.240\n",
      "Final train accuracy for L 10 N 0 is: 100.000\n",
      "Final test accuracy  for L 10 N 1 is: 55.710\n",
      "Final val accuracy   for L 10 N 1 is: 55.340\n",
      "Final train accuracy for L 10 N 1 is: 100.000\n",
      "Final test accuracy  for L 10 N 2 is: 58.160\n",
      "Final val accuracy   for L 10 N 2 is: 57.520\n",
      "Final train accuracy for L 10 N 2 is: 100.000\n",
      "Final test accuracy  for L 10 N 3 is: 59.880\n",
      "Final val accuracy   for L 10 N 3 is: 59.300\n",
      "Final train accuracy for L 10 N 3 is: 100.000\n",
      "Final test accuracy  for L 10 N 4 is: 56.340\n",
      "Final val accuracy   for L 10 N 4 is: 56.840\n",
      "Final train accuracy for L 10 N 4 is: 100.000\n",
      "Final test accuracy  for L 10 N 5 is: 58.850\n",
      "Final val accuracy   for L 10 N 5 is: 58.580\n",
      "Final train accuracy for L 10 N 5 is: 100.000\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "L_test_accs = defaultdict(list)\n",
    "for L in Ls:\n",
    "    for N in range(0, N_TIMES):\n",
    "        model = ScatResNet18(L).to(device)\n",
    "        checkpoint = torch.load(checkpoint_dir / f\"{MODEL_NAME}_L{L}_N{N}_max_acc.pth\", map_location=device)\n",
    "        model.load_state_dict(checkpoint['net'])\n",
    "        model.eval()\n",
    "\n",
    "        stats = stats_list[L][N]\n",
    "        total_params, model_size_mb = get_model_summary(ScatResNet18(L))\n",
    "        stats['total_params'] = total_params\n",
    "        stats['model_size_mb'] = model_size_mb\n",
    "        stats['train_acc'] = calculate_accuracy(model, trainloader, device)\n",
    "        stats['val_acc'] = calculate_accuracy(model, valloader, device)\n",
    "        stats['test_acc'] = calculate_accuracy(model, testloader, device)\n",
    "        \n",
    "        print(f'Final test accuracy  for L {L} N {N} is: {stats['test_acc']:.3f}')\n",
    "        print(f'Final val accuracy   for L {L} N {N} is: {stats['val_acc']:.3f}')\n",
    "        print(f'Final train accuracy for L {L} N {N} is: {stats['train_acc']:.3f}')\n",
    "        \n",
    "        with open(training_stats_dir / f'{MODEL_NAME}_L{L}_N{N}_stats.pkl', 'wb') as file:\n",
    "            pickle.dump(stats, file)\n",
    "        L_test_accs[L].append(stats['test_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0960cf63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T23:17:43.965261Z",
     "iopub.status.busy": "2025-12-30T23:17:43.964577Z",
     "iopub.status.idle": "2025-12-30T23:17:44.349000Z",
     "shell.execute_reply": "2025-12-30T23:17:44.348267Z"
    },
    "papermill": {
     "duration": 0.55343,
     "end_time": "2025-12-30T23:17:44.350420",
     "exception": false,
     "start_time": "2025-12-30T23:17:43.796990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 800x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(ncols=1, nrows=len(Ls), figsize=(8,10))\n",
    "for i, L in enumerate(Ls):\n",
    "    ax[i].plot(L_test_accs[L])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2eb7b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-30T23:17:44.687750Z",
     "iopub.status.busy": "2025-12-30T23:17:44.686934Z",
     "iopub.status.idle": "2025-12-30T23:17:44.692162Z",
     "shell.execute_reply": "2025-12-30T23:17:44.691427Z"
    },
    "papermill": {
     "duration": 0.173275,
     "end_time": "2025-12-30T23:17:44.693633",
     "exception": false,
     "start_time": "2025-12-30T23:17:44.520358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean     for L 4: 55.652\n",
      "Variance for L 4: 0.426\n",
      "Mean     for L 6: 56.090\n",
      "Variance for L 6: 4.755\n",
      "Mean     for L 8: 58.175\n",
      "Variance for L 8: 0.971\n",
      "Mean     for L 10: 57.945\n",
      "Variance for L 10: 2.133\n"
     ]
    }
   ],
   "source": [
    "for L in Ls:\n",
    "    print(f\"Mean     for L {L}: {np.mean(L_test_accs[L]):.3f}\")\n",
    "    print(f\"Variance for L {L}: {np.var(L_test_accs[L]):.3f}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOlKRvREyFsuJ9trrs14c5Y",
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11403.420541,
   "end_time": "2025-12-30T23:17:48.019670",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-30T20:07:44.599129",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
